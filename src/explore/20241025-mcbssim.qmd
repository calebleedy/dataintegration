---
title: "Real Data Analysis: MCBS Data"
author: "Caleb Leedy"
date: "October 25, 2024"
from: markdown+emoji
format: 
  html:
    embed-resources: true
    code-fold: true
    grid:
      margin-width: 450px
bibliography: references.bib
reference-location: margin
comments:
  hypothesis: true
---

# Summary

This report 

1. Understands the MCBS data,
2. Understands the ACS data,
3. Applies our data integration method to these surveys.

# Data

```{r}
#| label: setup

library(dplyr)
library(readr)
library(haven)
library(stringr)
library(purrr)
library(nleqslv)
library(doParallel)

```

```{r}
#| label: load data

# # mcbs_df <- read_delim("data/MCBS/MCBSPUF_2021_1_fall.txt") # Did not work
# mcbs_fall_df <- read_csv("data/MCBS/puf2021_1_fall.csv")
# mcbs_winter_df <- read_csv("data/MCBS/puf2021_2_winter.csv")
# mcbs_summer_df <- read_csv("data/MCBS/puf2021_3_summer.csv")

```

The summer supplement contains information about whether an individual took a
COVID vaccine. The variable is `CVS_ONEDOSE`. Demographic information is found
in the fall supplement. Individuals between data sets can be matched using the
`PUF_ID`. The corresponding `MCBSPUF_2021_<NUM>_<SEASON>.txt` file contains a
description of data responses.

I also downloaded CPS data to get information about people below the poverty
line: Table A-5 from 
[here](https://www.census.gov/data/tables/2022/demo/income-poverty/p60-277.html).
This provides a mean for the percentage of older Americans are below the poverty
line in 2021. (Note: The ACS also has the variable `POVPIP` which is the
income-to-poverty percentage.)

Futhermore, we have data from the American Community Survey to get additional
demographic information.

```{r}
#| label: acs data

# acs_a <- read_csv("data/ACS/psam_pusa.csv")
# acs_b <- read_csv("data/ACS/psam_pusb.csv")
# acs_df <- bind_rows(acs_a, mutate(acs_b, ST = as.character(ST)))

```

We can see the variable codes in the `data/ACS/PUMS_Data_Dictionary_2021.pdf`.
We have the following common variables between the MCBS and ACS:

| Variable | MCBS Code | ACS Code |
|----------|-----------|----------|
| Sex      | DEM_SEX   | SEX      |
| Education | DEM_EDU  | SCHL     |
| Marital Status | DEM_MARSTA | MAR (MSP?) |
| Race / Ethnicity | DEM_RACE | RAC1P |
| On Medicare | ADM_H_MEDSTA  | HINS3 | 
| On Medicaid | ADM_DUAL_FLAG_YR | FHINS4C |

Since ACS contains information about age, we can reweight the data to only
include people 65 years or older. We also have data from the 
~~CPS containing information about sex, education, and marital status.~~
CMS containing information about the total number of people enrolled in Medicare
in 2021.

## Cleaning the Data

We need to ensure that the data from MCBS and ACS are encoded in the same
way.

```{r}
#| label: understanding acs data

# acs_df %>%
#   group_by(SERIALNO) %>%
#   summarize(Count = n()) %>%
#   arrange(desc(Count))
# 
# # What is the unique identifier for the ACS data?
# # RT - record type
# # SERIALNO - HU or GQ serial number
# # DIVISION - division based on Census. Ex. New England, Middle Atlantic
# # SPORDER - person number (1 - 20)
# # PUMA - public use microdata area code
# #
# # I believe SERIALNO + SPORDER will be unique
# 
# acs_df %>%
#   mutate(tmp_id = str_c(SERIALNO, SPORDER)) %>%
#   group_by(tmp_id) %>%
#   summarize(Count = n()) %>%
#   arrange(desc(Count))
# 
# # It is unique!

```

```{r}
#| label: cleaning sex category

# unique(acs_df$SEX)            # Male: 1, Female: 2
# unique(mcbs_fall_df$DEM_SEX)  # Male: 1, Female: 2

```


Cleaning the school category

MCBS: `DEM_EDU` contains the following categories:

| Category | Response |
|----------|----------|
| Less than high school | 1 |
| High school or vocational | 2 |
| More than high school | 3 |
| Refused | R |
| Don't know | D |

The ACS needs to be transformed into this scale:

| Category | Response |
|----------|----------|
| Less than high school | bb and 01 - 15 | 
| High school or vocational | 16 - 19 | 
| More than high school | 20 - 24 |


Cleaning Marital Status

MCBS: `DEM_MARSTA` contains the following categories:

| Category | Response |
|----------|----------|
| Married | 1 |
| Widowed | 2 |
| Divorced / Separated | 3 |
| Never married | 4 |
| Refused | R |

The ACS needs to be transformed into this scale:

| Category | ACS Response |
|----------|----------|
| Married | 1 |
| Widowed | 2 |
| Divorced / Separated | 3, 4 |
| Never married | 5 |

The actual cleaning of the data is found in `../build/clean_acs_data.R` with the
associated saved data set.

# Getting Uncertainty Estimates

According to the PUMS Handbook (see `references/` folder) the way to get
uncertainty estimates in the ACS is to compute the estimate with the weight
`PWGTP`. Call the $X$. Then compute the estimate with the weights `PWGTP1`
through `PWGTP80`. Call these $X_1, \dots, X_{80}$. Then an estimate for the
variance is 

$$\hat V = \frac{4}{80} \sum_{i = 1}^{80} (X_i - X)^2.$$

We need to compute the uncertainty for the following variables: `DEM_EDU`,
`DEM_MARSTA`, and `DEM_RACE`.

```{r}
#| label: acs uncertainty

acs_cl <- read_csv("data/cleaned_acs_data.csv")
acs_med <- filter(acs_cl, HINS3 == 1)

# Transform categorical to logical vectors
acs_med <- acs_med %>%
  mutate(edulths = (DEM_EDU == "1"),
         edueqhs = (DEM_EDU == "2"),
         edumths = (DEM_EDU == "3")) %>%
  mutate(marmar = (DEM_MARSTA == "1"),
         marwid = (DEM_MARSTA == "2"),
         mardiv = (DEM_MARSTA == "3"),
         marsin = (DEM_MARSTA == "4")) %>%
  mutate(sexm = (DEM_SEX == "1"),
         sexf = (DEM_SEX == "2")) %>%
  mutate(racew = (DEM_RACE == "1"),
         raceb = (DEM_RACE == "2"),
         raceh = (DEM_RACE == "3"))

get_est_uncertainty <- function(acs_med, varname, def_w = "PWGTP") {

  # Estimate Mean
  # est <- sum(acs_med[[varname]] * acs_med[[def_w]]) / sum(acs_med[[def_w]])
  est <- sum(acs_med[[varname]] * acs_med[[def_w]])
  
  # Estimate Variance
  alt_est <- rep(NA, 80)
  for (i in 1:80) {
    w_str <- str_c("PWGTP", i)
    # alt_est[i] <- sum(acs_med[[varname]] * acs_med[[w_str]]) / sum(acs_med[[w_str]])
    alt_est[i] <- sum(acs_med[[varname]] * acs_med[[w_str]])
  }

  v_est <- 4 / 80 * sum((est - alt_est)^2)
    
  return(list(est = est, var = v_est))

}

param_df <- 
  tibble(varname = c("HINS3", "edulths", "edueqhs", "edumths",
                     "marmar", "marwid", "mardiv", "marsin",
                     "sexm", "sexf",
                     "racew", "raceb", "raceh"),
         type = c("TOT", "EDU", "EDU", "EDU", "MARSTA", "MARSTA", "MARSTA", "MARSTA",
                  "SEX", "SEX", "RACE", "RACE", "RACE")
  )

```

## Clean MCBS Data

```{r}
#| label: clean MCBS

mcbs_fall_df <- read_csv("data/MCBS/puf2021_1_fall.csv")
mcbs_summer_df <- read_csv("data/MCBS/puf2021_3_summer.csv")

# The fall segment contains demographic information while the summer segment
# contains the covid questions

mcbs_all <- 
  left_join(select(mcbs_summer_df, PUF_ID, starts_with("CVS_"), starts_with("PUFS")),
            select(mcbs_fall_df, PUF_ID, starts_with("DEM_")),
            by = join_by(PUF_ID))

mcbs_all <- mcbs_all %>%
  mutate(edulths = (DEM_EDU == "1"),
         edueqhs = (DEM_EDU == "2"),
         edumths = (DEM_EDU == "3")) %>%
  mutate(marmar = (DEM_MARSTA == "1"),
         marwid = (DEM_MARSTA == "2"),
         mardiv = (DEM_MARSTA == "3"),
         marsin = (DEM_MARSTA == "4")) %>%
  mutate(sexm = (DEM_SEX == "1"),
         sexf = (DEM_SEX == "2")) %>%
  mutate(racew = (DEM_RACE == "1"),
         raceb = (DEM_RACE == "2"),
         raceh = (DEM_RACE == "3"))

mcbs_all <- mcbs_all %>%
  mutate(CVS_ONEDOSE = case_when(
    CVS_ONEDOSE == "1" ~ 1,
    CVS_ONEDOSE == "2" ~ 2,
    is.na(CVS_ONEDOSE) & (CVS_TWODOSE %in% c("1", "2")) ~ as.numeric(CVS_TWODOSE),
    CVS_ONEDOSE %in% c("R", "D") & !is.na(CVS_VCNUMS) ~ 1,
    TRUE ~ 2
  )) 

```

# Data Integration

The CMS data is considered a population level data. There is no uncertainty in
this result.

```{r}
#| label: add CMS data

cov_ests <- 
  # bind_rows(
  purrr::map_dfr(param_df$varname, function(vn) {
    out <- get_est_uncertainty(acs_med, vn)
    return(tibble(Data = "ACS", Name = vn, Est = out$est, Var = out$var))
  })#,
  # purrr::map_dfr(param_df$varname[-1], function(vn) {
  #   out <- get_est_uncertainty(mcbs_all, vn, def_w = "PUFSWGT")
  #   return(tibble(Data = "MCBS", Name = vn, Est = out$est, Var = out$var))
  # }))

mcbs_tot_est <- 
  get_est_uncertainty(mutate(mcbs_all, ones = 1), "ones", def_w = "PUFSWGT")
  
cov_ests <- cov_ests %>% 
  add_row(Data = "CMS", Name = "tot", Est = 63892626, Var = 1) %>%
  add_row(Data = "CMS", Name = "sexm", Est = 29159084, Var = 1) %>%
  add_row(Data = "CMS", Name = "sexf", Est = 34733542, Var = 1) %>%
  mutate(Name = ifelse(Name %in% c("HINS3", "ones"), "tot", Name)) 

# %>%
#  add_row(Data = "MCBS", Name = "tot", Est = mcbs_tot_est$est, Var = mcbs_tot_est$var)
  
```

```{r}
#| label: data integration

#' This  function does the debiased calibration
#'
#' @param zmat - A n x p matrix
#' @param w1i - A vector of length n
#' @param d2i - A vector of length n
#' @param T1 - A vector of length p
#' @param qi - A scaler or a vector of length n
#' @param entropy - One of "EL", "ET"
solveGE <- function(zmat, w1i, d2i, T1, qi, entropy) {

  f <- function(lam, zmat, w1i, d2i, T1, qi, entropy, returnw = FALSE) {

    if (entropy == "EL") {
      w <- (-d2i) / drop(zmat %*% lam)
    } else if (entropy == "ET") {
      w <- d2i * exp(drop(zmat %*% lam))
    } else {
      stop("We only accept entropy of EL or ET.")
    }

    if (returnw) {
      return(w)
    } else {
      return(T1 - drop((w * w1i * qi) %*% zmat))
    }
  }

  j <- function(lam, zmat, w1i, d2i, T1, qi, entropy) {
    if (entropy == "EL") {
      res <- (-1) * t(zmat) %*% diag((w1i * qi * d2i) / drop(zmat %*% lam)^2) %*% zmat
    } else if (entropy == "ET") {
      res <- (-1) * t(zmat) %*% 
        diag((w1i * qi * d2i) * exp(drop(zmat %*% lam))) %*% zmat
    }

    return(res)
  }

  init <- c(rep(0, ncol(zmat) - 1), 1)
  res <- 
  nleqslv(init, f, jac = j, zmat = zmat, w1i = w1i, d2i = d2i,
          T1 = T1, qi = qi, entropy = entropy,
          method = "Newton", control = list(maxit = 1e5, allowSingular = TRUE))

  resw <- f(res$x, zmat, w1i, d2i, T1, qi, entropy, returnw = TRUE)

  if (!(res$termcd %in% c(1, 2))) {
    return(NA)
  }

  return(resw)

}

# The solveGE_alpha function estimates alpha and the weights.
# This is useful for when we do not have design weights for the entire
# population.
solveGE_alpha <- function(alpha, zmat, w1i, d2i, T1, qi, entropy, retw = FALSE) {

  N <- T1[1]
  T1_adj <- T1
  T1_adj[length(T1_adj)] <- alpha * N
  dc_w <- solveGE(zmat, w1i, d2i, T1_adj, qi, entropy)

  if (retw) {
    return(dc_w)
  }

  Ka_type <- "alpha"
  if (Ka_type == "alpha") {
    # HACK: G(x) = -log(x) for EL only
    G <- function(x) {-log(x)}
    return(sum(G(dc_w)) - N * alpha)
  }
}

data_int_mcbsacs <- function(mcbs_all, cov_ests, weight_v = "PUFSWGT") {

  qi = 1
  entropy = "EL"
  N_pop <- filter(cov_ests, Data == "CMS", Name == "tot") %>% pull(Est)

  # Default: p0_df -> mcbs_all
  # d_vec <- 1 / p0_df$pi0
  d_vec <- mcbs_all[[weight_v]]
  if (entropy == "EL") {
    gdi <- -1 / d_vec
    gpinv <- d_vec^2
  } else if (entropy == "ET") {
    gdi <- log(d_vec)
    gpinv <- d_vec
  } else {
    stop("This type of entropy has not yet been implemented.")
  }

  samp_X <- model.matrix(~edulths + edueqhs + marmar + marwid + mardiv + sexm,
                        data = mcbs_all)
  zmat <- cbind(samp_X, gdi)
  w1i <- 1

  c_ests <- 
    filter(cov_ests, !(Name %in% c("edumths", "sexf", "marsin"))) %>%
    filter(!(Name %in% c("racew", "raceb", "raceh")))

  # We want the mean for X not the total
  xhat <- c_ests$Est / N_pop
  # Columns: tot, edulths, edueqhs, marmar, marwid, mardiv, sexm,
  #          racew, raceb, raceh
  # Rows: same dimension as length(xhat)
  x_mat <- matrix(c(as.numeric(c_ests$Name == "tot"),
                    as.numeric(c_ests$Name == "edulths"),
                    as.numeric(c_ests$Name == "edueqhs"),
                    as.numeric(c_ests$Name == "marmar"),
                    as.numeric(c_ests$Name == "marwid"),
                    as.numeric(c_ests$Name == "mardiv"),
                    as.numeric(c_ests$Name == "sexm")),
                  ncol = length(unique(c_ests$Name)))

  # We want the variance of the mean not the variance of the total
  vmat <- diag(c_ests$Var / N_pop^2)
  vinv <- diag(N_pop^2 / c_ests$Var)
  xgls <- drop(solve(t(x_mat) %*% vinv %*% x_mat) %*% 
              (t(x_mat) %*% vinv %*% xhat))
  
  # We want T1 to be the totals
  T1 <- c(xgls * N_pop, NA)
 
  # Estimating Alpha
  alpha_init <- (-1) * nrow(mcbs_all) / N_pop
  a_res <- 
    optim(par = alpha_init, fn = solveGE_alpha, 
        zmat = zmat, w1i = w1i, d2i = d_vec, T1 = T1, qi = qi, entropy = entropy)

  if (a_res$convergence != 0) {
    warning(paste0("a_res has convergence", a_res$convergence))
  }

  dc_w <- 
    solveGE_alpha(a_res$par, zmat, w1i, d_vec, T1, qi, entropy, retw = TRUE)

  # Mean Estimation
  Y_vec <- as.numeric(mcbs_all$CVS_ONEDOSE == 1)
  theta <- sum(Y_vec * dc_w * w1i) / N_pop

  # FIXME: Do we want samp_X or zmat?
  gam_dc <- 
    solve(t(samp_X) %*% diag(d_vec * gpinv * qi) %*% samp_X,
          t(samp_X) %*% diag(d_vec * gpinv) %*% Y_vec)
  coefs <- as.numeric(gam_dc)
  eps <- (Y_vec - drop(samp_X %*% coefs))

  # Variance Estimation
  var_x <- solve(t(x_mat) %*% vinv %*% x_mat)

  v1 <- matrix(coefs, nrow = 1) %*% var_x %*% matrix(coefs, ncol = 1)
  v2 <- sum((1 - (1 / d_vec)) / (1 / d_vec)^2 * eps^2) / (N_pop)^2
  v_hat <- v1 + v2 

  return(list(theta = theta, v_hat = as.numeric(v_hat)))
}

reg_est <- function(mcbs_all, cov_ests, weight_v = "PUFSWGT") {

  N_pop <- filter(cov_ests, Data == "CMS", Name == "tot") %>% pull(Est)
  Y_vec <- as.numeric(mcbs_all$CVS_ONEDOSE == 1)
  mod_df <- mcbs_all %>%
    mutate(Y = (CVS_ONEDOSE == 1))

  mod <- lm(Y ~ sexm, data = mod_df, weights = mod_df[[weight_v]])
  yhat <- predict(mod, mod_df) 

  # TEST:
  # mod_t <- lm(Y ~ edulths + edueqhs + marmar + marwid + mardiv + sexm,
  #   data = mod_df, weights = mod_df[[weight_v]])
  # yhat <- predict(mod_t, mod_df) 
  # END TEST:
  
  # Regression Estimation
  cms_tot <- filter(cov_ests, Data == "CMS", Name == "tot") %>% pull(Est)
  cms_sexm <- filter(cov_ests, Data == "CMS", Name == "sexm") %>% pull(Est)
  cms_mean <- cms_tot / N_pop
  cms_meansexm <- cms_sexm / N_pop

  err_term <- sum((mod_df$Y - yhat) * mod_df[[weight_v]]) / N_pop

  theta <- 
    cms_mean * mod$coefficients[1] + cms_meansexm * mod$coefficients[2] + err_term

  # Variance Estimation
  pihat <- 1 / mod_df[[weight_v]]
  v_hat <- sum((1 - pihat) * (mod_df$Y - yhat)^2 * (1 / pihat)^2) / N_pop^2
  
  return(list(theta = theta, v_hat = v_hat))
}

ht_est <- function(mcbs_all, cov_ests, weight_v = "PUFSWGT") {

  N_pop <- filter(cov_ests, Data == "CMS", Name == "tot") %>% pull(Est)
  Y_vec <- as.numeric(mcbs_all$CVS_ONEDOSE == 1)
  theta <- sum(Y_vec * mcbs_all[[weight_v]]) / N_pop
  pihat <- 1 / mod_df[[weight_v]]
  v_hat <- sum((1 - pihat) * (Y_vec  / pihat)^2) / N_pop^2

  return(list(theta = theta, v_hat = v_hat))
}


```

# Assessing the Variance 

```{r}
#| label: variance analysis of mcbs

res <- data_int_mcbsacs(mcbs_all, cov_ests, weight_v = "PUFSWGT") 
ms <- reg_est(mcbs_all, cov_ests)
ht <- ht_est(mcbs_all, cov_ests)

cl <- makeCluster(min(detectCores() - 1, 100), outfile = "")
registerDoParallel(cl)

packs <- c("dplyr", "stringr", "purrr", "nleqslv")

mc_res <- 
  foreach(b = 1:100, .packages = packs,
          .errorhandling = "remove", .inorder = FALSE) %dopar% {

  if ((b %% 10) == 0) {
    print(paste0("Iteration: ", b))
  }

  weight_v <- paste0("PUFS", str_sub(paste0("00", b), -3))
  ms <- data_int_mcbsacs(mcbs_all, cov_ests, weight_v = weight_v) 
  # ms <- reg_est(mcbs_all, cov_ests, weight_v = weight_v)

  return(
      tibble(Est = c("MSEst"),
             Theta = c(ms$theta),
             Var = c(ms$v_hat), 
             Iter = b)
  )

  } %>% bind_rows()

stopCluster(cl)

# Weighted variance estimate
var(mc_res$Theta)

# Estimated variance estimate
res$v_hat

# Relative Bias
(res$v_hat - var(mc_res$Theta)) / var(mc_res$Theta)

```

Next steps:
* Use data integration model.
* Check variance assessment using other weight models (PUFS001, etc.).
