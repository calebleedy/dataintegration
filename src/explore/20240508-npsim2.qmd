---
title: "Debiased Nonparametric Regression Estimation"
author: "Caleb Leedy"
from: markdown+emoji
format: 
  html:
    embed-resources: true
    code-fold: true
    grid:
      margin-width: 450px
bibliography: references.bib
reference-location: margin
comments:
  hypothesis: true
---

# Summary

This report implements the simulation study in @wang2023statistical
in which we assess the bias of a
nonparametric regression estimator. We compare an oracle regression estimate,
naive two-phase nonparametric regression, and our proposed method--debiased
nonparametric regression estimation. This is very similar to
`20240506-npregest.qmd` except with a different setup.

# Simulation Setup

```{r}
#| label: setup

B_sims <- 1000

N_obs <- 10000
n1_obs <- 500

library(dplyr)
library(doParallel)
library(doRNG)
library(randomForest)
library(mgcv)

```

We generate a finite population of size $N =$`{r} N_obs` from a superpopulation
model of:

$$\begin{aligned}
X_{1i} &\stackrel{ind}{\sim} U(1, 3) \\
X_{2i} &\stackrel{ind}{\sim} U(1, 3) \\
X_{3i} &\stackrel{ind}{\sim} U(1, 3) \\
X_{4i} &\stackrel{ind}{\sim} U(1, 3) \\
\varepsilon_i &\stackrel{ind}{\sim} N(0, 1),
\end{aligned}
$$

The construction of $Y$ depends on the chosen model,

| Model | $Y$ |
|:------|----:|
| A     | $Y_i = 3 + 2.5 x_{1i} + 2.75 x_{2i} + 2.5 x_{3i} + 2.25 x_{4i} + \sqrt{3} \varepsilon_i$|
| B     | $Y_i = 3 + (1 / 35) x_{1i}^2 x_{2i}^3 x_{3i} + 0.1 x_{4i} + \sqrt{3} \varepsilon_i $|
| C     | $Y_i = 3 + (1 / 180) x_{1i}^2 x_{2i}^3 x_{3i} x_{4i}^2 + \sqrt{3} \varepsilon_i$ |

The Phase 1 sample is a simple random sample (SRS) of size $n_1 =$`{r} n1_obs`
from the finite population and the Phase 2 sample is a Poisson sample with the
probability of selection into the Phase 2 sample from the Phase 1 sample being
$\text{logit}(\bf x_i^T \bm \beta + 2.5)$ where $\bm \beta = (-1.1, 0.5, -0.25,
-0.1)^T$ or $\text{logit}(-0.3 + 0.7 x_{1i}^2 - 0.5 x_{2i} - 0.25 x_{3i} -0.25
x_{4i})$.

```{r}
#| label: generate data and samples

gen_pop <- function(N_obs, n1_obs, model, response) {

  x1 <- runif(N_obs, 1, 3)
  x2 <- runif(N_obs, 1, 3)
  x3 <- runif(N_obs, 1, 3)
  x4 <- runif(N_obs, 1, 3)
  eps <- rnorm(N_obs)

  if (model == "A") {
    y <- 3 + 2.5 * x1 + 2.75 * x2 + 2.5 * x3 + 2.25 * x4 + sqrt(3) * eps
  } else if (model == "B") {
    y <- 3 + (1 / 35) * x1^2 * x2^3 * x3 + 0.1 * x4 + sqrt(3) * eps
  } else if (model == "C") {
    y <- 3 + (1 / 180) * x1^2 * x2^3 * x3 * x4^2 + sqrt(3) * eps
  } else {
    stop("We must have model be A, B, or C.")
  }

  pi1 <- n1_obs / N_obs

  if (response == 1) {
    beta <- c(-1.1, 0.5, -0.25, -0.1)
    xbeta <- 2.5 + x1 * beta[1] + x2 * beta[2] + x3 * beta[3] + x4 * beta[4]
  } else if (response == 2) {
    beta <- c(0.7, -0.5, -0.25, -0.25)
    xbeta <- -0.3 + x1^2 * beta[1] + x2 * beta[2] + x3 * beta[3] + x4 * beta[4]
  } else {
    stop("response must be either 1 or 2.")
  }

  expit <- function(x) {1 / (1 + exp(-x))}
  pi2 <- expit(xbeta)

  return(tibble(X1 = x1, X2 = x2, X3 = x3, X4 = x4, Y = y, pi1 = pi1, pi2 = pi2))

}

gen_samps <- function(pop_df, p1_samp = "srs", p2_samp = "poisson") {

  if (p1_samp == "srs") {
    ind <- sample(1:nrow(pop_df), size = pop_df$pi1[1] * nrow(pop_df))
    p1_df <- filter(pop_df, row_number() %in% ind) %>%
      mutate(del1 = 1)
  } else {
    stop("p1_samp must be srs.")
  }

  if (p2_samp == "poisson") {
    del2 <- rbinom(nrow(p1_df), 1, p1_df$pi2)
    p1_df <- mutate(p1_df, del2 = del2)
    p2_df <- filter(p1_df, del2 == 1)
  } else {
    stop("p2_samp must be poisson")
  }

  return(list(p1_df, p2_df))
}

```

```{r}
#| labels: estimators

oracle_est <- function(p1_df, p2_df, N_obs, model) {

  # Steps:
  # 1. Get mhat.
  # 2. Predict missing values
  # 3. Estimate variance via linearization

  # 1. Get mhat.
  if (model == "A") {
    mx <- 3 + 2.5 * p1_df$X1 + 2.75 * p1_df$X2 + 2.5 * p1_df$X3 + 2.25 * p1_df$X4 
    mx2 <- 3 + 2.5 * p2_df$X1 + 2.75 * p2_df$X2 + 2.5 * p2_df$X3 + 2.25 * p2_df$X4 
  } else if (model == "B") {
    mx <- 3 + (1 / 35) * p1_df$X1^2 * p1_df$X2^3 * p1_df$X3 + 0.1 * p1_df$X4
    mx2 <- 3 + (1 / 35) * p2_df$X1^2 * p2_df$X2^3 * p2_df$X3 + 0.1 * p2_df$X4
  } else if (model == "C") {
    mx <- 3 + (1 / 180) * p1_df$X1^2 * p1_df$X2^3 * p1_df$X3 * p1_df$X4^2
    mx2 <- 3 + (1 / 180) * p2_df$X1^2 * p2_df$X2^3 * p2_df$X3 * p2_df$X4^2
  } else {
    stop("We must have model be A, B, or C.")
  }

  # 2. Predict missing values
  eta <- mx + p1_df$del2 / p1_df$pi2 * (p1_df$Y - mx)
  theta <- sum(eta / p1_df$pi1) / N_obs

  # 3. Estimate variance via linearization
  v1 <- (1 / nrow(p1_df) - 1 / N_obs) * var(eta)
  v2 <- sum(1 / (p2_df$pi1 * p2_df$pi2^2) * 
           (1 - p2_df$pi2) * (p2_df$Y - mx2)^2) / N_obs^2

  vhat <- (v1 + v2)


  return(list(theta, vhat))
}

naive_npreg_est <- function(p1_df, p2_df, N_obs, type = "loess") {

  # Steps:
  # 1. Estimate mhat.
  # 2. Predict missing values
  # 3. Estimate variance via linearization

  # 1. Estimate mhat.
  if (type == "loess") {
    mod <- loess(Y ~ X1 + X2 + X3 + X4, p2_df,
                 control = loess.control(surface="direct"))
  } else if (type == "randomforest") {
    mod <- randomForest(Y ~ X1 + X2 + X3 + X4, p2_df)
  } else if (type == "spline") {
    # mod <- gam(Y ~ s(X1, X2, X3, X4), data = p2_df)
    mod <- gam(Y ~ s(X1, k = 15, bs = "cr") +
                   s(X2, k = 15, bs = "cr") +
                   s(X3, k = 15, bs = "cr") +
                   s(X4, k = 15, bs = "cr"), data = p2_df)
  }

  mx <- predict(mod, p1_df)
  mx2 <- predict(mod, p2_df)

  # 2. Predict missing values
  eta <- mx + p1_df$del2 / p1_df$pi2 * (p1_df$Y - mx)
  theta <- sum(eta / p1_df$pi1) / N_obs

  # 3. Estimate variance via linearization
  v1 <- (1 / nrow(p1_df) - 1 / N_obs) * var(eta)
  v2 <- sum(1 / (p2_df$pi1 * p2_df$pi2^2) * 
           (1 - p2_df$pi2) * (p2_df$Y - mx2)^2) / N_obs^2

  vhat <- (v1 + v2)

  return(list(theta, vhat))
}

ss_npreg_est <- function(p1_df, p2_df, N_obs, type = "loess") {

  # Steps:
  # 0. Split A_2
  # 1. Estimate mhat in both splits
  # 2. Predict missing values
  # 3. Estimate variance via linearization

  # 0. Split A_2
  inda <- sample(1:nrow(p2_df), size = round(nrow(p2_df) / 2))
  p2a_df <- filter(p2_df, row_number() %in% inda)
  p2b_df <- filter(p2_df, !(row_number() %in% inda))
  p2_df <- p2_df %>%
    mutate(del2a = row_number() %in% inda) %>%
    mutate(del2b = !(row_number() %in% inda)) 

  p1_df <- p1_df %>%
    left_join(p2_df, by = join_by(X1, X2, X3, X4, Y, pi1, pi2, del1, del2)) %>%
    mutate(del2a = ifelse(is.na(del2a), 0, del2a)) %>%
    mutate(del2b = ifelse(is.na(del2b), 0, del2b))

  # 1. Estimate mhat in both splits
  if (type == "loess") {
    moda <- loess(Y ~ X1 + X2 + X3 + X4, p2a_df,
                  control = loess.control(surface="direct"))
    modb <- loess(Y ~ X1 + X2 + X3 + X4, p2b_df,
                  control = loess.control(surface="direct"))
  } else if (type == "randomforest") {
    moda <- randomForest(Y ~ X1 + X2 + X3 + X4, p2a_df)
    modb <- randomForest(Y ~ X1 + X2 + X3 + X4, p2b_df)
  } else if (type == "spline") {
    # moda <- gam(Y ~ s(X1, X2, X3, X4), data = p2a_df)
    # modb <- gam(Y ~ s(X1, X2, X3, X4), data = p2b_df)
    moda <- gam(Y ~ s(X1, k = 15, bs = "cr") +
                   s(X2, k = 15, bs = "cr") +
                   s(X3, k = 15, bs = "cr") +
                   s(X4, k = 15, bs = "cr"), data = p2a_df)
    modb <- gam(Y ~ s(X1, k = 15, bs = "cr") +
                   s(X2, k = 15, bs = "cr") +
                   s(X3, k = 15, bs = "cr") +
                   s(X4, k = 15, bs = "cr"), data = p2b_df)
  }

  ma_2 <- predict(moda, p2_df)
  mb_2 <- predict(modb, p2_df)
  ma_1 <- predict(moda, p1_df)
  mb_1 <- predict(modb, p1_df)
  mx <- (ma_1 + mb_1) / 2

  # 2. Predict missing values
  eta <- 
    mx + 
    p1_df$del2a / p1_df$pi2 * (p1_df$Y - mb_1) + 
    p1_df$del2b / p1_df$pi2 * (p1_df$Y - ma_1)

  # plot(p1_df$Y, eta)

  theta <- sum(eta / p1_df$pi1) / N_obs

  # 3. Estimate variance via linearization
  vhat <- var(eta) / nrow(p1_df)

  return(list(theta, vhat))
}


```

```{r}
#| label: MCMC

set.seed(1)
model <- "C"
response <- 1
pop_df <- gen_pop(N_obs, n1_obs, model = model, response = response)
true_theta <- mean(pop_df$Y)

# png("tables/popplot.png")
# plot(pop_df$X, pop_df$Y)
# dev.off()

clust <- makeCluster(min(detectCores() - 2, 100), outfile = "")
registerDoParallel(clust)

mc_res <- 
  foreach(iter=1:B_sims,
          .packages = c("dplyr", "randomForest", "mgcv")) %dopar% {

  if (iter %% 100 == 0) {
    print(paste0("Iter: ", iter))
  }

  samps <- gen_samps(pop_df)
  p1_df <- samps[[1]]
  p2_df <- samps[[2]]

  # png("tables/p2plot.png")
  # plot(p2_df$X, p2_df$Y)
  # dev.off()

  oest <- oracle_est(p1_df, p2_df, N_obs, model = model)
  nest_l <- naive_npreg_est(p1_df, p2_df, N_obs, type = "loess")
  pest_l <- ss_npreg_est(p1_df, p2_df, N_obs, type = "loess")
  nest_rf <- naive_npreg_est(p1_df, p2_df, N_obs, type = "randomforest")
  pest_rf <- ss_npreg_est(p1_df, p2_df, N_obs, type = "randomforest")
  nest_sp <- naive_npreg_est(p1_df, p2_df, N_obs, type = "spline")
  pest_sp <- ss_npreg_est(p1_df, p2_df, N_obs, type = "spline")

  return(tibble(
      Est = c("Oracle",
              "NaiveLoess", "PropLoess",
              "NaiveRF", "PropRF",
              "NaiveSpline", "PropSpline"),
      Theta = c(oest[[1]],
                nest_l[[1]], pest_l[[1]],
                nest_rf[[1]], pest_rf[[1]],
                nest_sp[[1]], pest_sp[[1]]),
      Var = c(oest[[2]],
              nest_l[[2]], pest_l[[2]],
              nest_rf[[2]], pest_rf[[2]],
              nest_sp[[2]], pest_sp[[2]]),
      Iter = iter)
  )

} %>% bind_rows()

stopCluster(clust)


```


```{r}
#| label: Analyse MCMC

# Find any failed estimates
which(is.na(mc_res$Theta))
which(is.na(mc_res$Var))

# Analyze Mean
mc_res %>%
  mutate(err = Theta - true_theta) %>%
  mutate(CI = abs(err) < qnorm(0.975) * sqrt(Var)) %>%
  group_by(Est) %>%
  summarize(Bias = mean(err, na.rm = TRUE),
            RMSE = sqrt(mean(err^2, na.rm = TRUE)),
            EmpCI = mean(CI, na.rm = TRUE),
            sdest = sd(Theta, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(Ttest = abs(Bias) / sqrt(sdest^2 / B_sims)) %>%
  select(-sdest) %>%
  mutate(Ind = case_when(
    Est == "NaiveLoess" ~ 2,
    Est == "NaiveRF" ~ 4,
    Est == "NaiveSpline" ~ 6,
    Est == "PropLoess" ~ 3,
    Est == "PropRF" ~ 5,
    Est == "PropSpline" ~ 7,
    Est == "Oracle" ~ 1
  )) %>%
  arrange(Ind) %>%
  select(-Ind) 

%>%
  knitr::kable(digits = 3, "latex", booktabs = TRUE, linesep = "") %>%
  cat(file = paste0("tables/npsim2", model, response, "_mean.tex"))

# Analyze Variance
mc_res %>%
  group_by(Est) %>%
  summarize(MCVar = var(Theta, na.rm = TRUE),
            EstVar = mean(Var, na.rm = TRUE),
            VarVar = var(Var, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(Ttest = abs(MCVar - EstVar) / sqrt(VarVar / B_sims)) %>%
  mutate(Ind = case_when(
    Est == "NaiveLoess" ~ 2,
    Est == "NaiveRF" ~ 4,
    Est == "NaiveSpline" ~ 6,
    Est == "PropLoess" ~ 3,
    Est == "PropRF" ~ 5,
    Est == "PropSpline" ~ 7,
    Est == "Oracle" ~ 1
  )) %>%
  arrange(Ind) %>%
  select(-Ind) 

%>%
  knitr::kable(digits = c(1, 4, 4, 7, 1), "latex", booktabs = TRUE, linesep = "") %>%
  cat(file = paste0("tables/npsim2", model, response, "_var.tex"))


```


