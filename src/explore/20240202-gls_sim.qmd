---
title: "Non-Monotone GLS Simulation: Simple"
author:
  - name: Caleb Leedy
date: today
date-format: "D MMMM YYYY"
format:
  pdf:
    include-in-header: 
      - "latex_header.tex"
    keep-tex: true
---

```{r}
#| label: global variables
#| echo: false
B <- 1000

obs_seg <- 250
true_mu <- 5
cov_e1e2 <- 0.5

```

# Introduction

The goal of this report is to conduct a simulation study to show the validity of
using a GLS estimator when intermediate models do not also estimate the
parameter in question. This setup is different from the previous setup is a
couple of ways:

1. We do not use $E[g \mid G_r(Z)]$ as the $g$-functions. Instead, we have $g = (g_1,
   g_2, g_3)' = (X, Y_1, Y_2)'$.
2. We have fewer comparison estimators. Since we changed the $g$-functions it
   makes less sense to compare then to other estimators that are using different
   intermediate estimators.
3. We use a simple random sample (SRS) instead of a Poisson sample. For this
   setup, we have each segment being totally independent of each other. Each
   segment also has a fixed sample size of `r obs_seg` instead of having
   segments with random sample sizes with a total observation count of 1000.
4. The GLS estimator is now only estimating $\theta = E[Y_2] = E[g_3]$.

# Notation and Setup

Let $Z = (X, Y_1, Y_2)'$. We want to estimate the parameter $\theta = E[Y_2]$
where we may not always observe $Y_1$ and
$Y_2$. Define segments that contain observations in which the same variables are
observed as in @tbl-vars1.

| Segment  | Variables Observed |
| -------  | ------------------ |
| $A_{00}$ | $X$                |
| $A_{10}$ | $X, Y_1$           |
| $A_{01}$ | $X, Y_2$           |
| $A_{11}$ | $X, Y_1, Y_2$      | 
: This table identifies which variables are observed in each segment. Since $X$
is always observed, the subscript for each segment identifies which of variables
$Y_1$ and $Y_2$ are in the segment based on the position of a 1. {#tbl-vars1}

Let $\delta_{i, j_1, j_2}$ be the sample inclusion indicator for observation $i$
in segment $A_{j_1, j_2}$, and let $\pi_{j_1, j_2}$ be the probability of
selecting an element into $A_{j_1, j_2}$.

We consider the vector $g(Z) = (g_1(X), g_2(Y_1), g_3(Y_2))'$ and 
*for this simulation setup*, let $g_1$, $g_2$, and $g_3$ all be the identity
function $I()$. This means that $g(Z) = (X, Y_1, Y_2)'$. Notice, that we
have $\theta = E[Y_2] = E[g_3]$. In each segment $A_{j_1, j_2}$ we can obtain
estimators of some of the $g$ elements. We have the following:

\begin{align*}
g_1^{(11)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} g_1(x_i) \\
g_2^{(11)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} g_2(y_{1i}) \\
g_3^{(11)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} g_3(y_{2i}) \\
g_1^{(10)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{10}}{\pi_{10}} g_1(x_i) \\
g_2^{(10)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{10}}{\pi_{10}} g_2(y_{1i}) \\
g_1^{(01)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{01}}{\pi_{01}} g_1(x_i) \\
g_3^{(01)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{01}}{\pi_{01}} g_2(y_{2i}) \\
g_1^{(00)} &= n^{-1} \sum_{i = 1}^n \frac{\delta_{00}}{\pi_{00}} g_1(x_i) \\
\end{align*}

This yields the following linear estimator, 

$$ \hat g = Zg + e $$

where 

$$\hat g = 
\begin{bmatrix}
g_1^{(11)} \\
g_2^{(11)} \\
g_3^{(11)} \\
g_1^{(10)} \\
g_2^{(10)} \\
g_1^{(01)} \\
g_3^{(01)} \\
g_1^{(00)} \\
\end{bmatrix},
Z = 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
\end{bmatrix},
E[e] = 0,
\text{ and } 
\Var(e) = n^{-1}
\begin{bmatrix}
V_{11} & 0 & 0 & 0 \\
0 & V_{10} & 0 & 0 \\
0 & 0 & V_{01} & 0 \\
0 & 0 & 0 & V_{00} \\
\end{bmatrix}.
$$

Here, we also have

$$
V_{11} = 
\begin{bmatrix}
\frac{1}{\pi_{11}} E[g_1^2] - E[g_1]^2 & \frac{1}{\pi_{11}} E[g_1g_2] -
E[g_1]E[g_2] & \frac{1}{\pi_{11}} E[g_1g_3] - E[g_1]E[g_3] \\
\frac{1}{\pi_{11}} E[g_1g_2] - E[g_1]E[g_2] & \frac{1}{\pi_{11}} E[g_2^2] -
E[g_2]^2 & \frac{1}{\pi_{11}} E[g_2g_3] - E[g_2]E[g_3] \\
\frac{1}{\pi_{11}} E[g_1g_3] - E[g_1]E[g_3] & \frac{1}{\pi_{11}} E[g_2g_3] -
E[g_2]E[g_3] & \frac{1}{\pi_{11}} E[g_3^2] - E[g_3]^2 \\
\end{bmatrix},
$$
$$
V_{10} = 
\begin{bmatrix}
\frac{1}{\pi_{10}}E[g_1^2] - E[g_1]^2 & \frac{1}{\pi_{10}}E[g_1g_2] - E[g_1]E[g_2]\\
\frac{1}{\pi_{10}}E[g_1g_2] - E[g_1]E[g_2] & \frac{1}{\pi_{10}}E[g_2^2] - E[g_2]^2 \\
\end{bmatrix},
$$
$$
V_{01} = 
\begin{bmatrix}
\frac{1}{\pi_{01}}E[g_1^2] - E[g_1]^2 & \frac{1}{\pi_{01}}E[g_1g_3] - E[g_1]E[g_3]\\
\frac{1}{\pi_{01}}E[g_1g_3] - E[g_1]E[g_3] & \frac{1}{\pi_{01}}E[g_3^2] - E[g_3]^2 \\
\end{bmatrix}, \text{ and }
V_{00} = 
\begin{bmatrix}
\frac{1}{\pi_{00}}E[g_1^2] - E[g_1]^2
\end{bmatrix}.
$$

{{< pagebreak >}}

# Simulation

We use the following simulation setup

\begin{align*}
  \begin{bmatrix} x \\ e_1 \\ e_2 \end{bmatrix} 
  &\stackrel{ind}{\sim} N\left(\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, 
  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & \rho \\ 0 & \rho & 1 \end{bmatrix}\right) \\
  y_1 &= x + e_1 \\
  y_2 &= \mu + x + e_2 \\
\end{align*}

This yields outcome variables $Y_1$ and $Y_2$ that are correlated both with $X$
and additionally with each other. To generate the missingness pattern, we select
`r obs_seg` observations into the four segments independently.




```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false

library(dplyr)

library(doParallel)
library(doRNG)
library(parallelly)

```

```{r}
#| label: simulation function
#| echo: false

gen_simple_data <- function(n_obs_sec, mu, rho) {

  # A_11
  x <- rnorm(n_obs_sec)
  e_mat <- MASS::mvrnorm(n_obs_sec, c(0, 0), matrix(c(1, rho, rho, 1), nrow = 2))
  e1 <- e_mat[, 1]
  e2 <- e_mat[, 2]
  y1 <- x + e1
  y2 <- mu + x + e2

  df_11 <- 
    tibble(X = x, Y1 = y1, Y2 = y2,
           delta_00 = 0, delta_10 = 0, delta_01 = 0, delta_11 = 1,
           pi_00 = 0.25, pi_10 = 0.25, pi_01 = 0.25, pi_11 = 0.25)
  # A_10
  x <- rnorm(n_obs_sec)
  e_mat <- MASS::mvrnorm(n_obs_sec, c(0, 0), matrix(c(1, rho, rho, 1), nrow = 2))
  e1 <- e_mat[, 1]
  e2 <- e_mat[, 2]
  y1 <- x + e1
  y2 <- mu + x + e2

  df_10 <- 
    tibble(X = x, Y1 = y1, Y2 = y2,
           delta_00 = 0, delta_10 = 1, delta_01 = 0, delta_11 = 0,
           pi_00 = 0.25, pi_10 = 0.25, pi_01 = 0.25, pi_11 = 0.25)
  # A_01
  x <- rnorm(n_obs_sec)
  e_mat <- MASS::mvrnorm(n_obs_sec, c(0, 0), matrix(c(1, rho, rho, 1), nrow = 2))
  e1 <- e_mat[, 1]
  e2 <- e_mat[, 2]
  y1 <- x + e1
  y2 <- mu + x + e2

  df_01 <- 
    tibble(X = x, Y1 = y1, Y2 = y2,
           delta_00 = 0, delta_10 = 0, delta_01 = 1, delta_11 = 0,
           pi_00 = 0.25, pi_10 = 0.25, pi_01 = 0.25, pi_11 = 0.25)
  # A_00
  x <- rnorm(n_obs_sec)
  e_mat <- MASS::mvrnorm(n_obs_sec, c(0, 0), matrix(c(1, rho, rho, 1), nrow = 2))
  e1 <- e_mat[, 1]
  e2 <- e_mat[, 2]
  y1 <- x + e1
  y2 <- mu + x + e2

  df_00 <- 
    tibble(X = x, Y1 = y1, Y2 = y2,
           delta_00 = 1, delta_10 = 0, delta_01 = 0, delta_11 = 0,
           pi_00 = 0.25, pi_10 = 0.25, pi_01 = 0.25, pi_11 = 0.25)

  df <- bind_rows(df_11, df_10, df_01, df_00)

  # Return
  # X, Y_1, Y_2,
  # delta_00, delta_10, delta_01, delta_11,
  # pi_00, pi_10, pi_01, pi_11
  return(df)
  
}

```



```{r}
#| label: gls functions
#| echo: false

gls_est <- function(df, gfun, class = "basic", mu, rho) {

  if (class == "basic") {
    z_mat <- matrix(c(1, 0, 0,
                      0, 1, 0,
                      0, 0, 1,
                      1, 0, 0,
                      0, 1, 0,
                      1, 0, 0,
                      0, 0, 1,
                      1, 0, 0), ncol = 3, byrow = TRUE)

    g_hat <- 
      c(mutate(df, tmp = delta_11 / pi_11 * X) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_11 / pi_11 * Y1) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_11 / pi_11 * Y2) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_10 / pi_10 * X) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_10 / pi_10 * Y1) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_01 / pi_01 * X) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_01 / pi_01 * Y2) |> pull(tmp) |> mean(),
        mutate(df, tmp = delta_00 / pi_00 * X) |> pull(tmp) |> mean())

    v_11 <- 1 / df$pi_11[1] *
      matrix(c(1, 1, 1,
               1, 2, 1 + rho,
               1, 1 + rho, 2), nrow = 3)
    v_10 <- 1 / df$pi_10[1] * matrix(c(1, 1, 1, 2), nrow = 2)
    v_01 <- 1 / df$pi_01[1] * matrix(c(1, 1, 1, 2), nrow = 2)
    v_00 <- 1 / df$pi_00[1]

    v_mat <- matrix(rep(0, 64), ncol = 8)
    v_mat[1:3, 1:3] <- v_11
    v_mat[4:5, 4:5] <- v_10
    v_mat[6:7, 6:7] <- v_01
    v_mat[8, 8] <- v_00

    v_mat <- v_mat / nrow(df)

    ghat <- MASS::ginv(t(z_mat) %*% MASS::ginv(v_mat) %*% z_mat) %*% 
      t(z_mat) %*% MASS::ginv(v_mat) %*% g_hat

    return(as.numeric(t(c(0, 0, 1) %*% ghat)))

  } else {
    stop("We have only 1 class: 'basic'.")
  }
}
``` 


```{r}
#| label: MCMC
#| echo: false
#| tbl-cap: |
#|   Results from simulations study with independent equally sized segments
#|   $A_{11}$, $A_{10}$, $A_{01}$, and $A_{00}$ all of size $n = 250$. In this
#|   simulation we have the true mean of $Y_2$ equal to $\mu = 5$ and the covariance
#|   between $e_1$ and $e_2$ is $\rho = 0.5$. The goal is to estimate 
#|   $E[Y_2] = \mu$. For the GLS estimation, we use the
#|   true known covariance matrix (using the true values of $\mu$ and $\rho$),
#|   and we use g-functions $g_1 = X$, $g_2 = Y_1$ and $g_3 = Y_2$.

clust <- parallel::makeCluster(min(parallelly::availableCores() - 2, 100))
registerDoParallel(clust)

mc_theta <-
  foreach(iter = 1:B,
          .options.RNG = 1,
          .packages = c("dplyr", "stringr")) %dorng% {

    # Generate Data
    df <- gen_simple_data(obs_seg, mu = true_mu, rho = cov_e1e2)

    # Estimators
    oracle <- mean(df$Y2)
    cc <- df |>
      filter(delta_11 == 1) |>
      pull(Y2) |>
      mean()
    ipw <- df |>
      mutate(tmp = delta_11 / pi_11 * Y2) |>
      pull(tmp) |>
      mean()

    gls <- gls_est(df, gfun = "Y2", mu = true_mu, rho = cov_e1e2)

    return(tibble(Oracle = oracle,
                  CC = cc,
                  IPW = ipw,
                  GLS = gls
    ))

  } |>
  bind_rows()

stopCluster(clust)

# Analyze Results
mc_theta |>
  summarize(
    across(everything(),
    list(Bias = ~mean(.) - true_mu,
         SD = ~sd(.),
         Tstat = ~(mean(.) - true_mu) / sqrt(var(.) / B)),
    .names = "{.fn}_{.col}")) |> 
  tidyr::pivot_longer(cols = everything(),
                      names_to = c(".value", "Algorithm"),
                      names_pattern = "(.*)_(.*)") |>
  mutate(Pval = pt(-abs(Tstat), df = B)) |>
  knitr::kable(digits = 3)

```

The theoretical variance of $\hat g_{GLS}$ is 

```{r}
#| echo: false

z_mat <- matrix(c(1, 0, 0,
                  0, 1, 0,
                  0, 0, 1,
                  1, 0, 0,
                  0, 1, 0,
                  1, 0, 0,
                  0, 0, 1,
                  1, 0, 0), ncol = 3, byrow = TRUE)

v_11 <- 1 / 0.25 *
  matrix(c(1, 1, 1,
           1, 2, 1 + cov_e1e2,
           1, 1 + cov_e1e2, 2), nrow = 3)
v_10 <- 1 / 0.25 * matrix(c(1, 1, 1, 2), nrow = 2)
v_01 <- 1 / 0.25 * matrix(c(1, 1, 1, 2), nrow = 2)
v_00 <- 1 / 0.25

v_mat <- matrix(rep(0, 64), ncol = 8)
v_mat[1:3, 1:3] <- v_11
v_mat[4:5, 4:5] <- v_10
v_mat[6:7, 6:7] <- v_01
v_mat[8, 8] <- v_00

v_mat <- v_mat / (4 * obs_seg)
ones_v <- t(c(0, 0, 1))

theo_var <- 
  ones_v %*% MASS::ginv(t(z_mat) %*% MASS::ginv(v_mat) %*% z_mat) %*% t(ones_v)

```

$$ V(\hat g_{GLS}) = [0, 0, 1] (Z'V^{-1}Z)^{-1} [0, 0, 1]'.$$ 

This is equal to `{r} theo_var` which means that the standard deviation 
is `{r} sqrt(theo_var)`.

