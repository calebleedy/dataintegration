\documentclass{beamer} %
\usetheme{CambridgeUS}
\usepackage[latin1]{inputenc}
\usefonttheme{professionalfonts}
\usepackage{times}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{biblatex}

\usetikzlibrary{shapes, positioning, arrows}
\addbibresource{references.bib}

\renewcommand{\L}{\ensuremath{\mathcal{L}}}
\providecommand{\bY}{\ensuremath{\mathbf{Y}}}
\providecommand{\bX}{\ensuremath{\mathbf{X}}}
\providecommand{\bV}{\ensuremath{\mathbf{V}}}
\providecommand{\bK}{\ensuremath{\mathbf{K}}}
\providecommand{\bmu}{\ensuremath{\mathbf{\mu}}}
\providecommand{\bSigma}{\ensuremath{\mathbf{\Sigma}}}
\providecommand{\bbeta}{\ensuremath{\boldsymbol{\beta}}}
\providecommand{\beps}{\ensuremath{\boldsymbol{\varepsilon}}}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}
%\setbeamersize{text margin left=0.3mm,text margin right=0.1mm} 

\author{Caleb Leedy}
\title[Data Integration]{Previous Work on Non-montone Missingness}

\begin{document}

\everymath{\displaystyle}
\setbeamertemplate{title page}[default][colsep=-4bp,rounded=true]
\setbeamertemplate{itemize items}[circle]
\setbeamercolor{block title}{bg=red!50,fg=black}
\frame{\titlepage}

\begin{frame}{Outline}

  \begin{itemize}
    \item Notation
    \item Simulation Studies
  \end{itemize}
\end{frame}

\begin{frame}{Notation}

  \begin{tabular}{cc}
    Case 1: Monotone missingness &
  Case 2: Non-monotone missingness \\
  \begin{tabular}{lrrrrr}
    \toprule
    & $X$ & $Y_1$ & $Y_2$ & $R_1$ & $R_2$ \\
    \midrule
    $A_{11}$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 1 & 1 \\
    $A_{10}$ & $\checkmark$ & $\checkmark$ &          & 1 & 0 \\
    $A_{00}$ & $\checkmark$ &          &          & 0 & 0 \\
    \bottomrule
  \end{tabular} &
  \begin{tabular}{lrrrrr}
    \toprule
    & $X$ & $Y_1$ & $Y_2$ & $R_1$ & $R_2$ \\
    \midrule
    $A_{11}$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 1 & 1 \\
    $A_{10}$ & $\checkmark$ & $\checkmark$ &          & 1 & 0 \\
    $A_{01}$ & $\checkmark$ &          & $\checkmark$ & 0 & 1 \\
    $A_{00}$ & $\checkmark$ &          &          & 0 & 0 \\
    \bottomrule
  \end{tabular}
  \end{tabular}

  The goal is to estimate $\theta = E[g(X, Y_1, Y_2)]$.

\end{frame}

\begin{frame}{Proposed Estimator: Monotone}
  For, $b(X_i, Y_{1i}) = E[g_i \mid X_i, Y_{1i}]$ and $i = \{1, \dots, n\}$,

  \begin{align*}
\hat \theta_{\text{eff}} &= n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] \\ 
&+ n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}(b(X_i, Y_{1i}) - E[g_i \mid X_i]) \\ 
&+ n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)}(g_i - b_i)
  \end{align*}
  
\end{frame}

\begin{frame}{Proposed Estimator: Non-monotone}
  
  Define,
\begin{align*}
b(X_i, Y_{1i}) &= E[g_i \mid X_i, Y_{1i}], \\ 
a(X_i, Y_{2i}) &= E[g_i \mid X_i, Y_{2i}], \text{ and } \\
i &= \{1, \dots, n\},
\end{align*}

\end{frame}

\begin{frame}{Proposed Estimator: Non-monotone}

\begin{align*}
\hat \theta_{\text{eff}} &= n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] \\ 
&+ n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}(b(X_i, Y_{1i}) - E[g_i \mid X_i]) \\ 
&+ n^{-1} \sum_{i = 1}^n \frac{R_{2i}}{\pi_{2+}(X_i)}(a(X_i, Y_{2i}) - E[g_i \mid X_i]) \\ 
&+ n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)}(g_i - b_i - a_i + E[g_i \mid X_i])
\end{align*}

\end{frame}

\begin{frame}{Simulation Outline: Monotone MAR}

  For this simulation we use the following approach:
  \begin{enumerate}
      \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
      \item Using the covariate $X$, determine the probability $p_1$ of $Y_1$
      being observed for each element $i$.
      \item Based on $p_1$, determine if $R_1 = 1$.
      \item If $R_1 = 0$, then $R_2 = 0$. Otherwise, using variables $X$ and
        $Y_1$, determine the probability $p_{12}$.
      \item Based on $p_{12}$ determine if $R_2 = 1$.
  \end{enumerate}
\end{frame}
  
\begin{frame}{Simulation Outline: Non-monotone MAR}

    Following the approach of \cite{robins1997non},
    the algorithm to generate the data is the following:
    \begin{enumerate}
        \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
        \item Using the covariate $X_i$, generate probabilities for each element
          $i$ $p_0$, $p_1$, and $p_2$ such that $p_0 + p_1 + p_2 = 1$. 
        \item Select one option based on the three probabilities for each
          element $i$. If 0 is selected: $R_1 = 0$ and $R_2 = 0$; if 1 is
          selected $R_1 = 1$; if 2 is selected, $R_2 = 1$.
        \item We take the next step in multiple cases. If 0 was selected, we are
          done. If 1 was selected, we generate probabilities $p_{12}$ based on
          $X$ and $Y_1$. Then based on this probability, we determine if $R_2 =
          1$. In the same manner, if 2 was selected in the previous step, we
          generate probabilities $p_{21}$ based on $X$ and $Y_2$. Then based on
          this probability, we determine if $R_2 = 1$.
    \end{enumerate}
\end{frame}

\begin{frame}{Simulation 1: Monotone MAR}

    
    We generate data from the following distributions:
    \begin{align*}
        X_i &\stackrel{iid}{\sim} N(0, 1) \\
        Y_{1i} &\stackrel{iid}{\sim} N(0, 1)\\
        Y_{2i} &\stackrel{iid}{\sim} N(\theta, 1)
    \end{align*}

    Then, we create the probabilities $p_1 = \logistic(x_i)$ and 
    $p_{12} = \logistic(y_{1i})$.
    Since, both $x_i$ and $y_1$ are standard normal distributions, each of these
    probabilities is approximately $0.5$ in expectation.

    The goal of this simulation is to estimate $\theta$. Alternatively, we can
    express this as solving the estimating equation:

    \[g(\theta) \equiv Y_2 - \theta = 0.\]

\end{frame}

\begin{frame}{Simulation 1: Monotone MAR}

    We estimate $\theta$ using the following procedures:

    \begin{itemize}
        \item Oracle: This computes $\bar Y_2$ using \textit{both} the observed
          and missing data.
        \item IPW-Oracle: This is an IPW estimator using only the observed
          values of $Y_2$. The weights (inverse probabilities) use the actual
          probabilities.
        \item IPW-Est: This is an IPW estimator using the probabilities that
          have been estimated by a logistic model.
        \item Semi: The monotone efficient estimator.
    \end{itemize}

    \begin{itemize}
      \item Sample size ($n$): 2000
      \item Monte Carlo replications: 2000
    \end{itemize}

\end{frame}

\begin{frame}

    \input{Tables/monomar_t-5.tex}

\end{frame}

\begin{frame}

    \input{Tables/monomar_t0.tex}

\end{frame}

\begin{frame}

    \input{Tables/monomar_t5.tex}

\end{frame}

\begin{frame}{Simulation 1: Non-monotone MAR}

    We generate variables $(X, Y_1, Y_2)$ using the following setup:

    \[\begin{bmatrix}
    X_i \\ \varepsilon_{1i} \\ \varepsilon_{2i}
    \end{bmatrix} \stackrel{iid}{\sim}
    N\left(
    \begin{bmatrix}
        0 \\ 0 \\ \theta
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & \sigma_{yy}\\
        0 & \sigma_{yy} & 1
    \end{bmatrix}
    \right).\]

    Then, 

    \[y_{1i} = x_i + \varepsilon_{1i} \text{ and } 
    y_{2i} = x_i + \varepsilon_{2i}.\]

\end{frame}

\begin{frame}

    Since we have nonmonotone data, our ``Stage 1'' probabilities are
    different. We compute the true Stage 1 probabilities being proportional to
    the following values:
    
    \begin{align*}
        p_0 &= 0.2 \\
        p_1 &= 0.4 \\
        p_2 &= 0.4 \\
    \end{align*}
    
   However, we keep the same structure for the Stage 2 probabilities with:
   $p_{12} = \logistic(y_1)$ and $p_{21} = \logistic(y_2)$. 

\end{frame}

\begin{frame}

  \input{Tables/nonmonosim1m-5.tex}

\end{frame}

\begin{frame}

  \input{Tables/nonmonosim1m0.tex}

\end{frame}

\begin{frame}

  \input{Tables/nonmonosim1m5.tex}

\end{frame}

\begin{frame}{Simulation 2: Non-monotone MAR}

    For this simulation, we
    focus on $\Cov(Y_1, Y_2)$. The data generating process now has $\sigma_{yy}
    \neq 0$. We are still interested in $\bar Y_2$ and we still run 2000
    simulation with 2000 observations. In all the next simulations the true
    value of $\theta = 0$. The results are the following:

\end{frame}

\begin{frame}

    \input{Tables/nonmonosim2c0.1.tex}

\end{frame}

\begin{frame}

    \input{Tables/nonmonosim2c0.5.tex}

\end{frame}

\begin{frame}

    \input{Tables/nonmonosim2c0.9.tex}

\end{frame}

\begin{frame}{Comparing with a Calibration Estimator}

  The efficient monotone estimator should be very similar to the
    following calibration estimator, for $\sum_{i = 1}^n w_i y_{2i}$,

    \begin{align*}
      \argmin_w \sum_{i = 1}^n w_i^2& \text{ such that }\\
      \sum_{i = 1}^n x_i &= \sum_{i = 1}^n R_{1i} w_{1i} x_i \\
      \sum_{i = 1}^n w_{1i} (x_i, y_{1i}) &= \sum_{i = 1}^n R_{1i} R_{2i} w_{2i}
      (x_i, y_{1i}) \\
    \end{align*}

  The reason that these should be the same is because they are similar in
    relationship to a calibration and regression estimator which are equivalent.
  
\end{frame}

\begin{frame}{Calibration Comparison: Monotone}
  To test the idea that the monotone regression estimator is similar to
    the calibration estimator we run several simulation studies. In the monotone
    case data is generating in the following steps:

    \begin{enumerate}
      \item The variables $X$, $Y_1$, and $Y_2$ are simulated from the following
        distributions:
        \begin{align*}
          X_i &\stackrel{iid}{\sim} N(0, 1) \\
          Y_{1i} &\stackrel{iid}{\sim} N(0, 1) \\
          Y_{2i} &\stackrel{iid}{\sim} N(\theta, 1).
        \end{align*}

      \item After the variables have been simulated, we see which variables are
        observed. We always observe $X_i$. We observed $Y_1$ with
        probability $p_{1i} \propto \logistic(x_i)$. If $Y_{1i}$ is observed,
        then we observe $Y_{2i}$ with probability $p_{2i} \propto
        \logistic(y_{1i})$. If $Y_{1i}$ is not observed, we do not observe
        $Y_{2i}$.
    \end{enumerate}

\end{frame}


\begin{frame}{Additional Estimators}
  \begin{itemize}
\item HT estimator of $\theta=E(Y_2)$: 
$$ \hat{\theta}_{\rm HT} = \frac{1}{n} \sum_{i=1}^n \frac{R_{1i} R_{2i}}{ \pi_{11}(X_i) } y_{2i}  $$
\item The three-phase regression estimator of $\theta$:
\begin{eqnarray*} 
\hat{\theta}_{\rm reg} &=& \frac{1}{n} \sum_{i \in A_2}\frac{1}{ \pi_{2i}  }\left\{ y_{i}  - \hat{E} ( Y \mid x_i, z_{i} ) \right\} \\
&+& \frac{1}{n} \sum_{i \in A_1}\frac{1}{ \pi_{1i}  }\left\{ \hat{E} ( Y \mid x_i, z_{i} ) - \hat{E} ( Y \mid x_i ) \right\} + \frac{1}{n} \sum_{i \in U} \hat{E} ( Y \mid x_i ) \\
&=& \bar{x}_0' \hat{\beta} + \left( \bar{x}_1' \hat{\gamma}_x + \bar{z}_1' \hat{\gamma}_z - \bar{x}_1' \hat{\beta} \right) + \{ \bar{y}_2- (\bar{x}_2' \hat{\gamma}_x + \bar{z}_2' \hat{\gamma}_z ) \} \\
&=& \bar{y}_2 + \{ \bar{x}_1' \hat{\gamma}_x + \bar{z}_1' \hat{\gamma}_z  -  (\bar{x}_2' \hat{\gamma}_x + \bar{z}_2' \hat{\gamma}_z ) \} +\left(  \bar{x}_0' \hat{\beta} - \bar{x}_1' \hat{\beta} \right) \end{eqnarray*} 
\item We can view the above three-phase regression estimator as a projection estimator of \cite{kim2012combining}.
\end{itemize}
\end{frame}

\begin{frame}

    \input{Tables/calimono_t-5.tex}

\end{frame}

\begin{frame}

    \input{Tables/calimono_t0.tex}

\end{frame}

\begin{frame}

    \input{Tables/calimono_t5.tex}

\end{frame}

\begin{frame}{Non-monotone Calibration}

    For the non-monotone case,
    we believe that we have the following calibration equations:

    \begin{align*}
      \sum_{i = 1}^n E[g_i \mid X_i] &= \sum_{i = 1}^n R_{1i} w_{1i} E[g_i \mid
      X_i]\\
      \sum_{i = 1}^n E[g_i \mid X_i] &= \sum_{i = 1}^n R_{2i} w_{2i} E[g_i \mid
      X_i]\\
      \sum_{i = 1}^n R_{1i} w_{1i} E[g_i \mid X_i, Y_{1i}] &= \sum_{i = 1}^n
      R_{1i} R_{2i} w_{ci} E[g_i \mid X_i, Y_{1i}]\\
      \sum_{i = 1}^n R_{2i} w_{2i} E[g_i \mid X_i, Y_{1i}] &= \sum_{i = 1}^n
      R_{1i} R_{2i} w_{ci} E[g_i \mid X_i, Y_{2i}]\\
      \sum_{i = 1}^n E[g_i \mid X_i] &= \sum_{i = 1}^n R_{1i} R_{2i} w_{ci}
      E[g_i \mid X_i].
    \end{align*}
  
\end{frame}

\begin{frame}{Non-monotone Calibration}
  We still have the same goal of the simulation study: estimate $\theta =
    E[Y_2]$.

    \begin{enumerate}
      \item[1.] Generate $X_i$, $\varepsilon_{1i}$, and $\varepsilon_{2i}$ from the
        following distributions:

        \begin{align*}
          x_i &\stackrel{iid}{\sim} N(0, 1)\\
          \varepsilon_{1i} &\stackrel{iid}{\sim} N(0, 1)\\
          \varepsilon_{2i} &\stackrel{iid}{\sim} N(\theta, 1)\\
        \end{align*}

        Then we have
        \[y_{1i} = x_i + \varepsilon_{1i} \text{ and } y_{2i} = x_i +
        \varepsilon_{2i}.\]

    \end{enumerate}

\end{frame}

\begin{frame}
  \begin{enumerate}
    \item[2.] Then we have to select the variables to observe. We always observe
        $X_i$. Then we choose to either observe $Y_1$ with probability $0.4$,
        $Y_2$ with probability $0.4$ or neither with probability $0.2$.

      \item[3.] If neither then $R_{1i} = 0$ and $R_{2i} = 0$. If we observe $Y_1$
        then $R_1 = 1$ and if we observe $Y_2$ then $R_2 = 1$.

      \item[4.] If we observe either $Y_1$ or $Y_2$ then with probability $p \propto
        \logistic(Y_k)$ where $Y_k$ is the observed $Y$ variable we choose to
        observe the other $Y$ variable.

      \item[5.] If the other $Y$ variable is observed then the corresponding $R_k =
        1$. Otherwise, $R_k = 0$.
      \end{enumerate}
    \end{frame}

\begin{frame}

    \input{Tables/calinonmono1m-5.tex}

\end{frame}

\begin{frame}

    \input{Tables/calinonmono1m0.tex}

\end{frame}

\begin{frame}

    \input{Tables/calinonmono1m5.tex}

\end{frame}

\begin{frame}[allowframebreaks]{References}
  \printbibliography
\end{frame}


\end{document}
