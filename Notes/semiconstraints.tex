% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Understanding Semiparametric Constraints},
  pdfauthor={Caleb Leedy},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Understanding Semiparametric Constraints}
\author{Caleb Leedy}
\date{22 January 2024}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, boxrule=0pt, frame hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, enhanced, sharp corners]}{\end{tcolorbox}}\fi

\hypertarget{sec-intro}{%
\section{Introduction}\label{sec-intro}}

The goal of this report is to understand how additional constraints
effect semiparametric models. Inspired by Dr.~Fuller's note, I set out
to see if we could assess the amount of efficiency loss due to using
semiparametric inference. First, let's define some notation.

\hypertarget{tbl-reftab}{}
\begin{longtable}[]{@{}llll@{}}
\caption{\label{tbl-reftab}This table shows the relationship between
different quantities of interest in each segment, their estimators, and
the corresponding coefficients of their estimators.}\tabularnewline
\toprule\noalign{}
Segment & Quantity of Interest & Estimator & Coefficient \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Segment & Quantity of Interest & Estimator & Coefficient \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(A_{00}\) & \(E[g \mid X]\) & \(\hat \gamma_{11}\) & \(c_{11}\) \\
\(A_{10}\) & \(E[g \mid X]\) & \(\hat \gamma_{21}\) & \(c_{21}\) \\
\(A_{10}\) & \(E[g \mid X, Y_1]\) & \(\hat \gamma_{22}\) & \(c_{22}\) \\
\(A_{01}\) & \(E[g \mid X]\) & \(\hat \gamma_{31}\) & \(c_{31}\) \\
\(A_{01}\) & \(E[g \mid X, Y_2]\) & \(\hat \gamma_{33}\) & \(c_{33}\) \\
\(A_{11}\) & \(E[g \mid X]\) & \(\hat \gamma_{41}\) & \(c_{41}\) \\
\(A_{11}\) & \(E[g \mid X, Y_1]\) & \(\hat \gamma_{42}\) & \(c_{42}\) \\
\(A_{11}\) & \(E[g \mid X, Y_2]\) & \(\hat \gamma_{43}\) & \(c_{43}\) \\
\(A_{11}\) & \(E[g \mid X, Y_1, Y_2]\) & \(\hat \gamma_{44}\) &
\(c_{44}\) \\
\end{longtable}

This is a similar framework to understanding the class of estimators
that we discussed in \verb|optest_update.pdf|, where we write the
estimator as a weighted average of different expectations with
functional coefficients.

\[\hat \theta = \frac{\delta_{11}}{\pi_{11}}g(Z) + \beta_0(\delta)E[g(Z)
\mid X] + \beta_1(\delta)E[g(Z) \mid X, Y_1] + \beta_2(\delta) E[g(Z)
\mid X, Y_2].\]

In this case, we assume that \(E[\beta_j(\delta)] = 0\) for
\(j = 0, 1, 2\).

However, now we write the estimator as

\begin{equation}\protect\hypertarget{eq-linest}{}{\hat \theta = \sum_{k,t} c_{kt}\hat \gamma_{kt}}\label{eq-linest}\end{equation}

where
\(\hat \gamma_{kt} = \frac{\delta_{ij}}{\pi_{ij}}E[g \mid G_{ij}(X, Y_1, Y_2)]\)
and \(ij\) corresponds to the segment \(A_{ij}\) associated with
\(\hat \gamma_{kt}\) in Table~\ref{tbl-reftab}. In general the estimator
has the following form,

\begin{align*}
\hat \theta = n^{-1} \sum_{i = 1}^n 
  &\left\{c_{11}\frac{\delta_{00}}{\pi_{00}} E[g \mid X]+ \right.\\
  &c_{21}\frac{\delta_{10}}{\pi_{10}} E[g \mid X]+ 
  c_{22}\frac{\delta_{10}}{\pi_{10}} E[g \mid X, Y_1]+ \\
  &c_{31}\frac{\delta_{01}}{\pi_{01}} E[g \mid X]+ 
  c_{33}\frac{\delta_{01}}{\pi_{01}} E[g \mid X, Y_2]+ \\
  &c_{41}\frac{\delta_{11}}{\pi_{11}} E[g \mid X]+ 
  c_{42}\frac{\delta_{11}}{\pi_{11}} E[g \mid X, Y_1]+
  c_{43}\frac{\delta_{11}}{\pi_{11}} E[g \mid X, Y_2]+
  \left.c_{44}\frac{\delta_{11}}{\pi_{11}} E[g \mid X, Y_1, Y_2]\right\}
\end{align*}

which we have condensed to Equation~\ref{eq-linest}.

\hypertarget{sec-methods}{%
\section{Methods}\label{sec-methods}}

As noted by Dr.~Fuller, we can understand parametric estimation as
solving the following constrained optimization problems:

\begin{align*}
  \min &\Var\left(\sum_{k, t} c_{kt} \hat \gamma_{kt}\right) \text{ such that } 
  \sum_{k,t} c_{kt} = 1 \\
       &\text{or} \\
  \min &\Var\left(\sum_{k, t} c_{kt} \hat \gamma_{kt}\right) \text{ such that } 
  \sum_{(k,t): (k,t) \neq (4, 4)} c_{kt} = 0 \text{ and } c_{44} = 1. \\
\end{align*}

To be robust to the outcome model, we can construct a similar
optimization problem with different constraints:

\begin{align*}
  \min \Var\left(\sum_{k, t} c_{kt} \hat \gamma_{kt}\right) \text{ such that }& 
  c_{11} + c_{21} + c_{31} + c_{41} = 0, c_{22} + c_{42} = 0, c_{33} + c_{43} = 0,\\
       &\text{and } c_{44} = 1.
\end{align*}

Likewise to be robust to the response model, we can have the problem:

\begin{align*}
  \min \Var\left(\sum_{k, t} c_{kt} \hat \gamma_{kt}\right) \text{ such that }& 
  c_{11} = \pi_{00}, c_{21} + c_{22} = \pi_{10}, c_{31} + c_{33} = \pi_{01}, \\
       &\text{ and } c_{41} + c_{42} + c_{43} + c_{44} = \pi_{11}.
\end{align*}

To be double robust (robust to the outcome and response model) we can
combine the last two constraints. This is summarized in
Table~\ref{tbl-mods}.

\hypertarget{tbl-mods}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7059}}@{}}
\caption{\label{tbl-mods}This table identifies the different constraints
for each model type.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Constraints
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Constraints
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parametric 1 & \(\sum_{k, t} c_{kt} = 1\) \\
Parametric 2 &
\(\sum_{k, t: (k, t) \neq (4, 4)} c_{kt} = 0, c_{44} = 1\) \\
Outcome Robust &
\(c_{11} + c_{21} + c_{31} + c_{41} = 0, c_{22} + c_{42} = 0, c_{33} + c_{43} = 0, \text{ and } c_{44} = 1.\) \\
Response Robust &
\(c_{11} = \pi_{00}, c_{21} + c_{22} = \pi_{10}, c_{31} + c_{33} = \pi_{01}, \text{ and } c_{41} + c_{42} + c_{43} + c_{44} = \pi_{11}\) \\
Double Robust &
\(c_{11} + c_{21} + c_{31} + c_{41} = 0, c_{22} + c_{42} = 0, c_{33} + c_{43} = 0, c_{44} = 1, c_{11} = \pi_{00}, c_{21} + c_{22} = \pi_{10}, c_{31} + c_{33} = \pi_{01}, \text{ and } c_{41} + c_{42} + c_{43} + c_{44} = \pi_{11}\) \\
\end{longtable}

We can justify the constraints for the Outcome Robust model by rewriting
the estimator into the following form:

\begin{align*}
E_\delta[\hat \theta] &= n^{-1} \sum_{i = 1}^n E_\delta\left[
E[g \mid X] \left(\frac{\delta_{00}}{\pi_{00}} c_{11} +
\frac{\delta_{10}}{\pi_{10}}c_{21} \frac{\delta_{01}}{\pi_{01}}c_{31} +
\frac{\delta_{11}}{\pi_{11}}c_{41}\right)\right. + \\ 
&\qquad\qquad\qquad\qquad E[g \mid X, Y_1] \left(\frac{\delta_{10}}{\pi_{10}}c_{22} + 
\frac{\delta_{11}}{\pi_{11}}c_{42}\right) +\\ 
&\qquad\qquad\qquad\qquad E[g \mid X, Y_2] \left(\frac{\delta_{01}}{\pi_{01}}c_{33} + 
\frac{\delta_{11}}{\pi_{11}}c_{43}\right) +\\ 
&\qquad\qquad\qquad\qquad \left.g \frac{\delta_{11}}{\pi_{11}}c_{44}\right] \\
&= n^{-1} \sum_{i = 1}^n \left\{E[g \mid X](c_{11} + c_{21} + c_{31} + c_{41}) +
E[g \mid X, Y_1] (c_{22} + c_{42}) +
E[g \mid X, Y_2] (c_{33} + c_{43}) + g c_{44}\right\}
\end{align*}

Thus, if the Outcome Robust constraints are satisfied,
\(E_\delta[\hat \theta] = g\) when we take the expectation with respect
to the response variable \(\delta\). These constraints make sense
because if they are satisfied \(E[\hat \theta]\) does not depend on the
correct specification of the models for \(E[g \mid G_r(X, Y_1, Y_2)]\).

Likewise, we can justify the constraints for the Response Robust model
by rewriting Equation~\ref{eq-linest} into the form

\begin{align*}
E_Z [\hat \theta] &= n^{-1} \sum_{i = 1}^n E_Z\left[
\frac{\delta_{00}}{\pi_{00}}c_{11} E[g \mid X]\right. + \\
&\qquad\qquad\qquad\qquad\frac{\delta_{10}}{\pi_{10}}\left(c_{21}E[g \mid X] + 
  c_{22}E[g \mid X, Y_1]\right) +\\
&\qquad\qquad\qquad\qquad\frac{\delta_{01}}{\pi_{01}}\left(c_{31}E[g \mid X] + 
  c_{33}E[g \mid X, Y_2]\right) +\\
&\qquad\qquad\qquad\qquad\left.\frac{\delta_{11}}{\pi_{11}}\left(c_{41}E[g \mid X] + 
  c_{42}E[g \mid X, Y_1] + c_{43}E[g \mid X, Y_2] + c_{44}g\right)\right] \\
& n^{-1} \sum_{i = 1}^n \left(\frac{\delta_{00}}{\pi_{00}}c_{11} + 
\frac{\delta_{10}}{\pi_{10}}(c_{21} + c_{22}) + 
\frac{\delta_{01}}{\pi_{01}}(c_{31} + c_{33}) + 
\frac{\delta_{11}}{\pi_{11}}(c_{41} + c_{42} + c_{43} + c_{44})\right)g
\end{align*}

Thus, if the Response Robust constraints are satisfied,
\(E_X[\hat \theta] = g\) when we take the expectation with respect to
the outcome variable \(Z = (X, Y_1, Y_2)'\). These constraints make
sense because when they are satisfied \(E[\hat \theta]\) does not depend
on the correct response probabilities \(\pi = E[\delta]\).

\hypertarget{sec-simulations}{%
\section{Simulation Study}\label{sec-simulations}}

\hypertarget{simulation-1}{%
\subsection*{Simulation 1}\label{simulation-1}}
\addcontentsline{toc}{subsection}{Simulation 1}

We use the following simulation setup

\begin{align*}
  \begin{bmatrix} x \\ e_1 \\ e_2 \end{bmatrix} 
  &\stackrel{ind}{\sim} N\left(\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, 
  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & \rho \\ 0 & \rho & 1 \end{bmatrix}\right) \\
  y_1 &= x + e_1 \\
  y_2 &= \theta + x + e_2 \\
\end{align*}

Furthermore, \(\pi_{11} = 0.2\) and \(\pi_{00} = 0.3, \pi_{10} = 0.4\),
and \(\pi_{01} = 0.1\). The goal of this simulation study is to find
\(\theta = E[Y_2]\). In other words, \(g(Z) = Y_2\). There are several
algorithms for comparison which are defined as the following:

\begin{align*}
  Oracle &= n^{-1} \sum_{i = 1}^n g(Z_i)\\
  CC &= \frac{\sum_{i = 1}^n \delta_{11} g(Z_i)}{\sum_{i = 1}^n \delta_{11}} \\
  IPW &= \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} g(Z_i)\\
\end{align*}

Furthermore, we include four existing estimators that we have already
proposed in the past: WLS, Prop, PropInd, and SemiDelta.

The estimator WLS is a a weight least square estimator derived in the
following manner. We now consider a normal model:

\[ 
  \begin{pmatrix}
    x_i \\ e_{1i} \\ e_{2i}
  \end{pmatrix} \stackrel{ind}{\sim}
  N \left(
  \begin{bmatrix}
    0 \\ 0 \\ 0
  \end{bmatrix},
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & \sigma_{11} & \sigma_{12} \\ 
    0 & \sigma_{12} & \sigma_{22}
  \end{bmatrix},
  \right)
\]

and define \(y_{1i} = \theta_1 + x_i + e_{1i}\) and
\(y_{2i} = \theta_2 + x_i + e_{2i}\). Then \(b_1 = b_2 = 1\). We define
\(\bar z_k^{(ij)}\) as the mean of \(y_k\) in segment \(A_{ij}\). This
means that we have means \(\bar z_1^{(11)}\), \(\bar z_2^{(11)}\),
\(\bar z_1^{(10)}\), and \(\bar z_2^{(01)}\). Let
\(W = [\bar z_1^{(11)}, \bar z_2^{(11)}, \bar z_1^{(10)}, \bar z_2^{(01)}]'\),
then for \(n_{ij} = |A_{ij}|\), we have

\[Z - M \mu \sim N(\vec 0, V)\]

where

\[M = 
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}
  \text{ and }
  V = 
  \begin{bmatrix}
    \frac{\sigma_{11}}{n_{11}} & \frac{\sigma_{12}}{n_{11}} & 0 & 0 \\
    \frac{\sigma_{12}}{n_{11}} & \frac{\sigma_{22}}{n_{11}} & 0 & 0 \\
    0 & 0 & \frac{\sigma_{11}}{n_{10}} & 0 \\
    0 & 0 & 0 & \frac{\sigma_{22}}{n_{01}} \\
  \end{bmatrix}.
\]

Thus, the BLUE for \(\mu = [\mu_1, \mu_2]'\) is

\[\hat \mu = (M' V^{-1} M)^{-1} M' V^{-1} W.\]

Hence, WLS is \(\mu_2\) as \(g(X, Y_1, Y_2) = Y_2\).

The remaining three estimators are drived from the following expression
where the values for \(\beta\) are provided in Table~\ref{tbl-beta}.
These are good estimators to compare to because Prop is the original
proposed estimator. PropInd has the same form as Prop except the values
for \(\beta\) is different. PropInd shares the same values of \(\beta\)
as all of the new models with constraints. SemiDelta is useful because
it is the best estimator in general so far.

\[
\hat \theta = \frac{\delta_{11}}{\pi_{11}}g(Z) + 
\beta_0(\delta, c_0)E[g(Z) \mid X] + 
\beta_1(\delta, c_1)E[g(Z) \mid X, Y_1] + 
\beta_2(\delta, c_2) E[g(Z)
\mid X, Y_2].
\]

\hypertarget{tbl-beta}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4151}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4151}}@{}}
\caption{\label{tbl-beta}This table displays the values of \(\beta\) for
different estimator types.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\beta_0(\delta, c_0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\beta_1(\delta, c_1)\)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\beta_0(\delta, c_0)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\beta_1(\delta, c_1)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Prop &
\(\left(1 - \frac{(\delta_{10} + \delta_{11})}{(\pi_{10} + \pi_{11})} - \frac{(\delta_{01} + \delta_{11})}{(\pi_{01} + \pi_{11})} + \frac{\delta_{11}}{\pi_{11}}\right)\)
&
\(\left(\frac{\delta_{10} + \delta_{11}}{\pi_{10} + \pi_{11}} - \frac{\delta_{11}}{\pi_{11}}\right)\) \\
PropInd &
\(\left(1 - \frac{(\delta_{10})}{(\pi_{10})} - \frac{(\delta_{01})}{(\pi_{01})} + \frac{\delta_{11}}{\pi_{11}}\right)\)
&
\(\left(\frac{\delta_{10}}{\pi_{10}} - \frac{\delta_{11}}{\pi_{11}}\right)\) \\
SemiDelta &
\(c_0\left(\frac{\delta_{11}}{\pi_{11}} - \frac{\delta_{00}}{\pi_{00}}\right)\)
&
\(c_1\left(\frac{\delta_{11}}{\pi_{11}} - \frac{\delta_{10}}{\pi_{10}}\right)\) \\
\end{longtable}

In the simulation results in Table~\ref{tbl-sim1}, the new results have
the same label as the value from the \texttt{Type} column in
Table~\ref{tbl-mods}.

\hypertarget{tbl-sim1}{}
\begin{longtable}[]{@{}lrrrr@{}}
\caption{\label{tbl-sim1}Results from Simulation 1. True values:
\(\theta = 5, \rho = 0.5\). The test conducted for the T-statistic and
P-value is a two sample test to see if the estimator is unbiased. This
simulation uses \(Y_1 = x + \varepsilon_1\),
\(Y_2 = \theta + x + \varepsilon_2\) where \(X \sim N(0, 1)\) and
\((\varepsilon_1, \varepsilon_2)\) come from a mean zero bivariate
normal distribution with unit variance and covariance \(\rho\). The
segments are unbalanced with: \(\pi_{11} = 0.2\), \(\pi_{10} = 0.4\),
\(\pi_{01} = 0.1\), and \(\pi_{00} = 0.3\).}\tabularnewline
\toprule\noalign{}
Algorithm & bias & sd & tstat & pval \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Algorithm & bias & sd & tstat & pval \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Oracle & -0.002 & 0.045 & -1.669 & 0.048 \\
CC & 0.001 & 0.083 & 0.263 & 0.396 \\
IPW & 0.010 & 0.357 & 0.872 & 0.192 \\
WLS & 0.000 & 0.054 & -0.065 & 0.474 \\
\(\hat \theta_{prop}\) & -0.002 & 0.063 & -0.768 & 0.221 \\
\(\hat \theta_{\delta}\) & -0.001 & 0.064 & -0.342 & 0.366 \\
Parametric 1 & -0.001 & 0.061 & -0.509 & 0.305 \\
Parametric 2 & -0.001 & 0.061 & -0.509 & 0.305 \\
Outcome Robust & -0.001 & 0.061 & -0.543 & 0.294 \\
Response Robust & -0.001 & 0.061 & -0.509 & 0.305 \\
Double Robust & -0.001 & 0.061 & -0.543 & 0.294 \\
\end{longtable}

\hypertarget{simulation-2}{%
\subsection*{Simulation 2}\label{simulation-2}}
\addcontentsline{toc}{subsection}{Simulation 2}

Next, we used the same simulation setup except that we are interested in
estimating \(\theta = E[g] = E[Y_1^2 Y_2]\). We do not use the WLS
estimator because it does not work.

\hypertarget{tbl-sim2}{}
\begin{longtable}[]{@{}lrrrr@{}}
\caption{\label{tbl-sim2}Results from Simulation 2. True values:
\(\theta = 10, \rho = 0.5\). The test conducted for the T-statistic and
P-value is a two sample test to see if the estimator is unbiased. This
simulation uses the same setup as Simulation 1 with
\(Y_1 = x + \varepsilon_1\), \(Y_2 = \theta + x + \varepsilon_2\) where
\(X \sim N(0, 1)\) and \((\varepsilon_1, \varepsilon_2)\) come from a
mean zero bivariate normal distribution with unit variance and
covariance \(\rho\). The segments are unbalanced with:
\(\pi_{11} = 0.2\), \(\pi_{10} = 0.4\), \(\pi_{01} = 0.1\), and
\(\pi_{00} = 0.3\).}\tabularnewline
\toprule\noalign{}
Algorithm & bias & sd & tstat & pval \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Algorithm & bias & sd & tstat & pval \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Oracle & -0.037 & 0.528 & -2.214 & 0.014 \\
CC & 0.007 & 1.196 & 0.179 & 0.429 \\
IPW & 0.023 & 1.405 & 0.520 & 0.302 \\
\(\hat \theta_{prop}\) & -0.044 & 0.691 & -2.027 & 0.021 \\
\(\hat \theta_{\delta}\) & -0.032 & 0.674 & -1.521 & 0.064 \\
Parametric 1 & -0.004 & 0.301 & -0.461 & 0.322 \\
Parametric 2 & -0.004 & 0.344 & -0.344 & 0.366 \\
Outcome Robust & -0.040 & 0.654 & -1.950 & 0.026 \\
Response Robust & -0.004 & 0.301 & -0.461 & 0.322 \\
Double Robust & -0.040 & 0.654 & -1.950 & 0.026 \\
\end{longtable}

\hypertarget{discussion}{%
\subsection*{Discussion}\label{discussion}}
\addcontentsline{toc}{subsection}{Discussion}

There has been concern about why the constrained results are the same in
Table~\ref{tbl-sim1}. This section aims to address these concerns.

Interestingly enough, the optimization algorithm does not have a lot of
variability in the output. The variance for each coefficient is found in
Table~\ref{tbl-estvar}. Since these values are all basically zero, we
can see that for each algorithm we are getting basically the same
coefficients each iteration. The coefficients that we see are displayed
in Table~\ref{tbl-estc}.

\hypertarget{tbl-estvar}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1562}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0938}}@{}}
\caption{\label{tbl-estvar}This table displays the variance of each
coefficient for each estimator. A number close to zero means that the
optimization algorithm chose very similar values for each Monte Carlo
simulation. A value of exactly zero indicated that the optimization
algorithm chose the exact same number.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_3\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_4\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_5\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_6\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_7\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_8\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_9\)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_3\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_4\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_5\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_6\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_7\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_8\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_9\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Para. 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
Para. 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
Outcome & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
Response & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
Double & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{longtable}

\hypertarget{tbl-estc}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1449}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0870}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1014}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0870}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1014}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0870}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1014}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1014}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1014}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0870}}@{}}
\caption{\label{tbl-estc}This table displays the estimated coefficients
for each algorithm on iteration 1. Since the variance between iterations
is small, these are reasonable for subsequent
iterations.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_3\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_4\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_5\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_6\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_7\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_8\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_9\)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_3\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_4\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_5\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_6\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_7\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_8\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(c_9\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Para. 1 & 0.3 & 0.400 & 0.000 & 0.100 & 0.000 & 0.200 & 0.000 & 0.000 &
0 \\
Para. 2 & 0.3 & 0.400 & 0.000 & 0.100 & 0.000 & 0.200 & 0.000 & -1.000 &
1 \\
Outcome & 0.3 & -0.071 & 0.471 & -0.194 & 0.294 & -0.035 & -0.471 &
-0.294 & 1 \\
Response & 0.3 & 0.400 & 0.000 & 0.100 & 0.000 & 0.200 & 0.000 & 0.000 &
0 \\
Double & 0.3 & -0.071 & 0.471 & -0.194 & 0.294 & -0.035 & -0.471 &
-0.294 & 1 \\
\end{longtable}

\hypertarget{tbl-fstat}{}
\begin{longtable}[]{@{}llrr@{}}
\caption{\label{tbl-fstat}This table computes the F-statistic and
P-value for an F-test comparing the \texttt{Full\ Model} with a nested
model in \texttt{Reduced\ Model}. The computation of the F-statistic is
found in the \texttt{get\_f\_statistic} function.}\tabularnewline
\toprule\noalign{}
Full Model & Reduced Model & Fstat & P-Value \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Full Model & Reduced Model & Fstat & P-Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parametric 1 & Parametric 2 & 0.00 & 1 \\
Parametric 1 & Outcome Robust & 799.85 & 0 \\
Parametric 1 & Response Robust & 0.00 & 1 \\
Parametric 1 & Double Robust & 342.79 & 0 \\
Parametric 2 & Outcome Robust & 1200.99 & 0 \\
Parametric 2 & Response Robust & 0.00 & 1 \\
Parametric 2 & Double Robust & 400.33 & 0 \\
Outcome Robust & Double Robust & 0.00 & 1 \\
Response Robust & Double Robust & 601.70 & 0 \\
\end{longtable}

From the analysis in Table~\ref{tbl-fstat}, we can see that there are
significant differences between several of the models. Why is this
different from the previous analysis in Table~\ref{tbl-sim1}? The answer
comes from how we compute the variance versus the sum of squares of
error. The variance is computed from each estimator created in a Monte
Carlo trial; whereas the sum of squares of error is summed from each
individuals estimate for the mean of \(Y_2\). For more details about how
the F-test is computed see Appendix A and Appendix B.

\newpage{}

\hypertarget{appendix-a-computing-the-f-test}{%
\section{Appendix A: Computing the
F-Test}\label{appendix-a-computing-the-f-test}}

To compute an F test we can use Equation~\ref{eq-fstat}.

\begin{equation}\protect\hypertarget{eq-fstat}{}{ F = \frac{(SSE_{Reduced} - SSE_{Full}) / (DFE_{Reduced} - DFE_{Full})}{
SSE_{Full} / DFE_{Full}}. }\label{eq-fstat}\end{equation}

Let's assess what this would mean to compare models
\texttt{Parametric\ 1} and \texttt{Outcome\ Robust} from
Table~\ref{tbl-mods}.

\hypertarget{tbl-mods}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7059}}@{}}
\caption{\label{tbl-mods}This table identifies the different constraints
for each model type.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Constraints
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Constraints
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parametric 1 & \(\sum_{k, t} c_{kt} = 1\) \\
Parametric 2 &
\(\sum_{k, t: (k, t) \neq (4, 4)} c_{kt} = 0, c_{44} = 1\) \\
Outcome Robust &
\(c_{11} + c_{21} + c_{31} + c_{41} = 0, c_{22} + c_{42} = 0, c_{33} + c_{43} = 0, \text{ and } c_{44} = 1.\) \\
Response Robust &
\(c_{11} = \pi_{00}, c_{21} + c_{22} = \pi_{10}, c_{31} + c_{33} = \pi_{01}, \text{ and } c_{41} + c_{42} + c_{43} + c_{44} = \pi_{11}\) \\
Double Robust &
\(c_{11} + c_{21} + c_{31} + c_{41} = 0, c_{22} + c_{42} = 0, c_{33} + c_{43} = 0, c_{44} = 1, c_{11} = \pi_{00}, c_{21} + c_{22} = \pi_{10}, c_{31} + c_{33} = \pi_{01}, \text{ and } c_{41} + c_{42} + c_{43} + c_{44} = \pi_{11}\) \\
\end{longtable}

Consider the case where we run estimate each model on a data set with
\(n = 1000\) observations, and define the following notation: let
\(\hat \theta^{(P)}\) and \(\hat \theta^{(OR)}\) be the estimated values
of \(\theta = E[Y_2]\) for the \texttt{Parametric\ 1} model and the
\texttt{Outcome\ Robust} model respectively. Define the estimated
coefficients to be \(\hat c_j^{(P)}\) and \(\hat c_j^{(OR)}\) where
\(j = 1, \dots 9\) for the \texttt{Parametric\ 1} and
\texttt{Outcome\ Robust} models respectively. The one can compute the
SSE with the following:

\[ SSE = n^{-1} \sum_{i = 1}^n (\hat y_{2i} - \theta)^2 \text{ where }
\hat y_{2i} = \sum_{j = 1}^9 \hat c_j \hat \gamma_j \]

and
\(\hat \gamma_0 := \hat \gamma_{00} = \frac{\delta_{00i}}{\pi_{00}} E[Y_2 \mid x_i]\),
\(\hat \gamma_1 := \hat \gamma_{11} = \frac{\delta_{10i}}{\pi_{10}} E[Y_2 \mid x_i, y_{1i}]\),
etc. Note that this is different from the previous estimate of the
variance which used the Monte Carlo variance (standard deviation)
defined by
\[ \frac{1}{n - 1} \sum_{b = 1}^B (\hat \theta_b - \bar \theta_B)^2 \]
where \(B\) is the number of Monte Carlo estimates and
\(\bar \theta_B = \frac{1}{B} \sum_{b = 1}^B \hat \theta_b\).

Likewise, one can compute the degrees of freedom by noticing that each
model has a degrees of freedom equal to nine minus the number of
constraints. This means that we can compute the model degrees of freedom
and error degrees of freedom for each model type, which we do in
Table~\ref{tbl-df}

\hypertarget{tbl-df}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-df}This table displays the degrees of freedom for
each model}\tabularnewline
\toprule\noalign{}
Model & Model Degrees of Freedom & Error Degrees of Freedom \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Model & Model Degrees of Freedom & Error Degrees of Freedom \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parametric 1 & \(9 - 1 = 8\) & \(n - 1 - 8 = n - 9\) \\
Parametric 2 & \(9 - 1 = 8\) & \(n - 1 - 8 = n - 9\) \\
Outcome Robust & \(9 - 4 = 5\) & \(n - 1 - 5 = n - 6\) \\
Response Robust & \(9 - 4 = 5\) & \(n - 1 - 5 = n - 6\) \\
Double Robust & \(9 - 8 = 1\) & \(n - 1 - 1 = n - 2\) \\
\end{longtable}

So continuing our first example\footnote{SSE values might differ in
  Table~\ref{tbl-fstat} slightly depending on the actual data values.},
if \(SSE^{(P)} = 993\) and \(SSE^{(OR)} = 3428\) with \(n = 1000\) then
the F statistic is

\[F = \frac{(3428 - 993) / (3)}{993 / (1000 - 9)} = 810.\]

The critical value we want to compare this with is the \(0.95\) quantile
of \(F_{3, 993}\) which is \(2.61\). So there \emph{is} a significant
difference between the fit of these two model. (The p-value is basically
zero.) The code to do this automatically is in Appendix B.

\newpage{}

\hypertarget{appendix-b-r-functions}{%
\section{Appendix B: R Functions}\label{appendix-b-r-functions}}

\hypertarget{lst-getfstat}{%
\label{lst-getfstat}}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} get\_f\_statistic is a function to compute the F{-}statistic between two of }
\CommentTok{\#\textquotesingle{} the constrained models.}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param c\_tab {-} A table with values for the estimated c coefficients}
\CommentTok{\#\textquotesingle{} @param reduced\_mod {-} A string indicating which model is the reduced model}
\CommentTok{\#\textquotesingle{} @param full\_mod {-} A string indicating which model is the full model}
\CommentTok{\#\textquotesingle{} @param df {-} A data set simulated from the same data generating process that }
\CommentTok{\#\textquotesingle{}    the c\_tab coefficients were estimated from.}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @note We have previously shown that the variance of the estimated c\_tab }
\CommentTok{\#\textquotesingle{} variables is close to or exactly zero. Hence, conducting a new simulation }
\CommentTok{\#\textquotesingle{} should not effect the results of the model much.}
\NormalTok{get\_f\_statistic }\OtherTok{\textless{}{-}} 
  \ControlFlowTok{function}\NormalTok{(c\_tab, reduced\_mod, full\_mod, df, }\AttributeTok{theta =}\NormalTok{ true\_theta, cov\_e1e2) \{}

\NormalTok{  vars\_lst }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(reduced\_mod, full\_mod)}

  \CommentTok{\# SSE reduced }
\NormalTok{  red\_cvec }\OtherTok{\textless{}{-}}
\NormalTok{    c\_tab }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(iter }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(.data[[}\StringTok{"estimator"}\NormalTok{]] }\SpecialCharTok{==}\NormalTok{ vars\_lst[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"c\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{as.numeric}\NormalTok{()}

  \CommentTok{\# SSE full}
\NormalTok{  full\_cvec }\OtherTok{\textless{}{-}}
\NormalTok{    c\_tab }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(iter }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(.data[[}\StringTok{"estimator"}\NormalTok{]] }\SpecialCharTok{==}\NormalTok{ vars\_lst[[}\DecValTok{2}\NormalTok{]]) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"c\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{as.numeric}\NormalTok{()}

  \CommentTok{\# Constructing gam\_vec}
\NormalTok{  Eg }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  Egx }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{X }
\NormalTok{  Egxy1 }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{+}\NormalTok{ cov\_e1e2 }\SpecialCharTok{*}\NormalTok{ (df}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{X)}
\NormalTok{  Egxy2 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{Y2}
\NormalTok{  EEg2 }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{2}
\NormalTok{  EEgx2 }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}
\NormalTok{  EEgxy12 }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ cov\_e1e2}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  EEgxy22 }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{2}
\NormalTok{  EEgxy1Egxy2 }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ cov\_e1e2}\SpecialCharTok{\^{}}\DecValTok{2}

\NormalTok{  g\_11 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_00 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_00 }\SpecialCharTok{*}\NormalTok{ (Egx)}
\NormalTok{  g\_21 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_10 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_10 }\SpecialCharTok{*}\NormalTok{ (Egx)}
\NormalTok{  g\_22 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_10 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_10 }\SpecialCharTok{*}\NormalTok{ (Egxy1)}
\NormalTok{  g\_31 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_01 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_01 }\SpecialCharTok{*}\NormalTok{ (Egx)}
\NormalTok{  g\_33 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_01 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_01 }\SpecialCharTok{*}\NormalTok{ (Egxy2)}
\NormalTok{  g\_41 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_11 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_11 }\SpecialCharTok{*}\NormalTok{ (Egx)}
\NormalTok{  g\_42 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_11 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_11 }\SpecialCharTok{*}\NormalTok{ (Egxy1)}
\NormalTok{  g\_43 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_11 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_11 }\SpecialCharTok{*}\NormalTok{ (Egxy2)}
\NormalTok{  g\_44 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{delta\_11 }\SpecialCharTok{/}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{prob\_11 }\SpecialCharTok{*}\NormalTok{ (df}\SpecialCharTok{$}\NormalTok{Y2)}

\NormalTok{  gam\_mat }\OtherTok{\textless{}{-}}\NormalTok{ base}\SpecialCharTok{::}\FunctionTok{cbind}\NormalTok{(g\_11, g\_21, g\_22, g\_31, g\_33, g\_41, g\_42, g\_43, g\_44)}

  \CommentTok{\# SSE}
\NormalTok{  sse\_red }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((}\FunctionTok{as.numeric}\NormalTok{(gam\_mat }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(}\FunctionTok{t}\NormalTok{(red\_cvec))) }\SpecialCharTok{{-}}\NormalTok{ theta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  sse\_full }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((}\FunctionTok{as.numeric}\NormalTok{(gam\_mat }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(}\FunctionTok{t}\NormalTok{(full\_cvec))) }\SpecialCharTok{{-}}\NormalTok{ theta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

  \CommentTok{\# Error Degrees of Freedom}
\NormalTok{  df\_vec }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(df) }\SpecialCharTok{{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{9} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\DecValTok{9} \SpecialCharTok{{-}} \DecValTok{2}\NormalTok{, }\DecValTok{9} \SpecialCharTok{{-}} \DecValTok{4}\NormalTok{, }\DecValTok{9} \SpecialCharTok{{-}} \DecValTok{4}\NormalTok{, }\DecValTok{9} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}
  \FunctionTok{names}\NormalTok{(df\_vec) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"parac"}\NormalTok{, }\StringTok{"zeroc"}\NormalTok{, }\StringTok{"outc"}\NormalTok{, }\StringTok{"respc"}\NormalTok{, }\StringTok{"doubc"}\NormalTok{)}

\NormalTok{  df\_red }\OtherTok{\textless{}{-}}\NormalTok{ df\_vec[reduced\_mod]}
\NormalTok{  df\_full }\OtherTok{\textless{}{-}}\NormalTok{ df\_vec[full\_mod]}

\NormalTok{  fstat }\OtherTok{\textless{}{-}}\NormalTok{ ((sse\_red }\SpecialCharTok{{-}}\NormalTok{ sse\_full) }\SpecialCharTok{/}\NormalTok{ (df\_red }\SpecialCharTok{{-}}\NormalTok{ df\_full)) }\SpecialCharTok{/}\NormalTok{ (sse\_full }\SpecialCharTok{/}\NormalTok{ df\_full)}

\NormalTok{  pval }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pf}\NormalTok{(fstat, df\_red }\SpecialCharTok{{-}}\NormalTok{ df\_full, df\_full)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{f =}\NormalTok{ fstat, }\AttributeTok{p =}\NormalTok{ pval))}

\NormalTok{\}}
\end{Highlighting}
\end{Shaded}




\end{document}
