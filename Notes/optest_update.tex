\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
\usepackage{biblatex}
\usepackage{booktabs}
\usepackage{float}

\addbibresource{references.bib}

\hypersetup{colorlinks = true, urlcolor = black}

\headheight     15pt
\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\renewcommand{\arraystretch}{1.2}

\begin{document}

\lhead{Caleb Leedy}
\chead{Non-monotone Missingness}
%\chead{STAT 615 - Advanced Bayesian Methods}
%\rhead{Page \thepage\ of \pageref{LastPage}}
\rhead{\today}

\section*{Setup}

Consider the following setup. Let $(X, Y_1, Y_2, \delta)  \stackrel{ind}{\sim}
F$ for some distribution $F$ that is unknown. We define $Z = (X, Y_1, Y_2$ and
we observe the following:

\begin{table}[!ht]
  \centering
  \caption{This table shows some of our notation and some of the corresponding
  notation from \cite{tsiatis2006semiparametric}.}
\begin{tabular}{cccccccc}
  \toprule
  Segments & $X$ & $Y_1$ & $Y_2$ & Prob. Element in Segment & $\delta$ & $C$ & $G_C(Z)$ \\
  \midrule
  $A_{00}$ & $\checkmark$ &               &         & $\pi_{00}$ & $\delta_{00}$ & 1 & $\{X\}$ \\
  $A_{10}$ & $\checkmark$ & $\checkmark$  &         & $\pi_{10}$ & $\delta_{10}$ & 2 & $\{X, Y_1\}$ \\
  $A_{01}$ & $\checkmark$ &               & $\checkmark$ & $\pi_{01}$ & $\delta_{01}$ & 3 & $\{X, Y_2\}$ \\
  $A_{11}$ & $\checkmark$ & $\checkmark$  & $\checkmark$ & $\pi_{11}$ & $\delta_{11}$ & $\infty$ & $\{X, Y_1, Y_2\}$ \\
  \bottomrule
\end{tabular}
\end{table}

Define $\varpi(r, Z) = \Pr(C = r \mid Z)$. For now, we assume that $\varpi(r,
Z)$ is known. Notice that $\varpi(\infty, Z) = \pi_11$, $\varpi(3, Z) = \pi_{01}$,
$\varpi(2, Z) = \pi_{10}$, and $\varpi(1, Z) = \pi_{00}$. Since $\pi_{00} +
\pi_{10} + \pi_{01} + \pi_{11} = 1$, we only need to define three inclusion 
probabilities.\\

Suppose that we want to estimate $\theta = E[g(X, Y_1, Y_2)]$ for a known
function $g$. The proposed estimator is 

\begin{align}\label{eq:prop}
  &\hat \theta_{prop} =\\ \nonumber
  &n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] + n^{-1} \sum_{i = 1}^n
  \frac{\delta_{10}}{\pi_{10}} (E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i]) 
  + n^{-1} \sum_{i = 1}^n \frac{\delta_{01}}{\pi_{01}} (E[g_i \mid X_i, Y_{2i}] - E[g_i \mid X_i]) \\ \nonumber
  &+ n^{-1} \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} (g_i - E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i, Y_{2i}] + E[g_i \mid X_i]).
\end{align}

The goal is the show that over all functions $b_1(X, Y_1)$ and $b_2(X, Y_2)$,
$\hat \theta_{prop}$ is the optimal estimator in the form:

\begin{align}
  &\hat \theta =\\ \nonumber
  &n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] + n^{-1} \sum_{i = 1}^n
  \frac{\delta_{10}}{\pi_{10}} (b_1(X_i, Y_{1i}) - E[g_i \mid X_i]) 
  + n^{-1} \sum_{i = 1}^n \frac{\delta_{01}}{\pi_{01}} (b_2(X_i, Y_{2i}) - E[g_i \mid X_i]) \\ \nonumber
  &+ n^{-1} \sum_{i = 1}^n \frac{\delta_{11}}{\pi_{11}} (g_i - b_1(X_i, Y_{1i}) - b_2(X_i, Y_{2i}) + E[g_i \mid X_i])
\end{align}

\section*{Semiparametric Inference}

We know from Theorem 7.2 of \cite{tsiatis2006semiparametric} that if $\Pr(C =
\infty \mid Z) = \pi_{11} > 0$ 
then the semiparametric influence function has the form (see page 20 of notes):

\[ \frac{I(C = \infty) g(Z)}{\varpi(\infty, Z)} + \frac{I(C =
  \infty)}{\varpi(\infty, Z)} \left(\sum_{r \neq \infty} \varpi(r, G_r(Z))
  L_{2r}(G_r(Z))\right) - \sum_{r \neq \infty} I(C = \infty)L_{2r}(G_r(Z))\]

where $L_{2r}(G_r(Z))$ is an arbitrary function of $G_r(Z)$. Notice, that this
form does not identify \textit{the} optimal estimator but a class of 
semiparametric functions. A reasonable choice of an estimator for $L_{2r}(G_r(Z))$ is

\[L_{2r}(G_r(Z)) = E[g(Z) \mid G_r(Z)].\]

It turns out that $\hat \theta_{prop}$ from Equation~\ref{eq:prop} is contained within 
the previous class of estimators with 

\[L_{2r}(G_r(Z)) = \varpi(r, G_r(Z)) E[g(Z) \mid G_r(Z)].\]

(See pages 81-82 of notes for proof.) \\

\subsection*{Linear Expectations}

To simplify this problem, we consider the following estimator\footnote{This
estimator has slightly different coefficients compared to the initial
estimator.}:

\begin{align}
  &\hat \theta_c \\ \nonumber
  &= \frac{\delta_{11}}{\pi_{11}} g(Z) + \left(1 -
    \left(\frac{\delta_{10} + \delta_{11}}{\pi_{10} + \pi_{11}}\right) -
    \left(\frac{\delta_{01} + \delta_{11}}{\pi_{01} + \pi_{11}}\right) + 
  \frac{\delta_{11}}{\pi_{11}}\right) c_0 E[g \mid X] \\ \nonumber
  &+
  \left(\left(\frac{\delta_{10} + \delta_{11}}{\pi_{10} + \pi_{11}}\right) - 
  \frac{\delta_{11}}{\pi_{11}}\right) c_1 E[g \mid X, Y_1] +
  \left(\left(\frac{\delta_{01} + \delta_{11}}{\pi_{01} + \pi_{11}}\right) - 
  \frac{\delta_{11}}{\pi_{11}}\right) c_2 E[g \mid X, Y_2]
\end{align}

\subsection*{Projection onto Nuisance Tangent Space}

The goal is now to find the coefficients $c_0, c_1$, and $c_2$ such that 
$\langle \hat \theta_c, L_2\rangle \equiv E[\hat \theta_c L_2] = 0$ for 
all $L_2 \in \Lambda_2$ (see \cite{tsiatis2006semiparametric} for definition 
of $\Lambda_2$). If we can find such coefficients that the estimator $\hat
\theta_c$ will be orthogonal to $\Lambda_2$ and hence by Theorem 10.1 of 
\cite{tsiatis2006semiparametric} semiparametrically optimal. The good news 
is that we know (CITEME) that any element $L_2 \in \Lambda_2$ has a form:

\begin{align}
  L_2 &= 
  \left(\frac{\delta_{11}}{\pi_{11}}\pi_{00} - \delta_{00}\right) L_{20}(X) +
  \left(\frac{\delta_{11}}{\pi_{11}}\pi_{10} - \delta_{10}\right) L_{21}(X, Y_1) +
  \left(\frac{\delta_{11}}{\pi_{11}}\pi_{01} - \delta_{01}\right) L_{22}(X, Y_2).
\end{align}

Then expanding and solving $E[\hat \theta_c L_2] = 0$ yields:

{\tiny
\begin{align*}
  0 &= E[\hat \theta_c L_2] \\ 
    &E\left[\left(\frac{\pi_{00}}{\pi_{11}} + \left(\frac{\pi_{10}}{\pi_{10} + \pi_{11}}\right)\frac{\pi_{00}c_1}{\pi_{11}} +
      \left(\frac{\pi_{01}}{\pi_{01} + \pi_{11}}\right) \frac{\pi_{00}c_2}{\pi_{11}} + \frac{\pi_{00}(\pi_{10}\pi_{01} -
  \pi_{11}^2)c_0}{(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})\pi_{11}}\right) E[g \mid X] L_{20}(X)\right. \\ 
    &+ \frac{\pi_{10}}{\pi_{11}} \left(E[g(Z) L_{21}(X, Y_1) \mid X] - c_1 E[E[g(Z) \mid X, Y_1] L_{21}(X, Y_1) \mid X] +
      \frac{\pi_{01}c_2}{\pi_{10} + \pi_{11}} E[E[g \mid X, Y_2] L_{21}(X, Y_1) \mid X] +
      \frac{\pi_{10} \pi_{01}}{\pi_{11}(\pi_{01} + \pi_{11})} E[g \mid X]
      E[L_{21}(X, Y_1) \mid X]c_0\right)\\
    &+ \left.\frac{\pi_{01}}{\pi_{11}} \left(E[g(Z) L_{22}(X, Y_2) \mid X] + \frac{\pi_{10} c_1}{\pi_{10} + \pi_{11}} E[E[g(Z) \mid X, Y_1] L_{22}(X, Y_2) \mid X] -
      c_2 E[E[g \mid X, Y_2] L_{22}(X, Y_2) \mid X] +
      \frac{\pi_{10}}{(\pi_{01} + \pi_{11})} E[g \mid X]
      E[L_{22}(X, Y_2) \mid X]c_0\right)\right]
\end{align*}
}

To solve for $c_0, c_1$, and $c_2$ we need the following to hold for any
$L_{21}(X, Y_1)$ and $L_{22}(X, Y_2)$:

{\small
\begin{align*}
  &1 + c_0 \frac{\pi_{01} \pi_{10} - \pi_{11}^2}{(\pi_{10} + \pi_{11})(\pi_{01}
  + \pi_{11})} + c_1 \frac{\pi_{10}}{\pi_{01} + \pi_{11}} + c_2
  \frac{\pi_{01}}{\pi_{01} + \pi_{11}} = 0\\ 
  &E\left[\left(g(Z) + c_0 \frac{\pi_{01}}{\pi_{01} + \pi_{11}}E[g(Z) \mid X] - c_1 E[g(Z) \mid X, Y_1] + c_2 \frac{\pi_{01}}{\pi_{10} + \pi_{11}}E[g(Z) \mid X, Y_2]\right)L_{21}(X, Y_1) \mid X\right] = 0 \\ 
  &E\left[\left(g(Z) + c_0 \frac{\pi_{10}}{\pi_{10} + \pi_{11}}E[g(Z) \mid X] + c_1 \frac{\pi_{10}}{\pi_{10} + \pi_{11}} E[g(Z) \mid X, Y_1] - c_2 E[g(Z) \mid X, Y_2]\right)L_{22}(X, Y_2) \mid X\right] = 0
\end{align*}
}

\subsection*{Optimal Normal Model}

An alternative optimization problem is to find coefficients $c_0$, $c_1$, and
$c_2$, that minimize the variance of $\hat \theta_c$. After expanding the 
variance and differentiating by each $c_i$, the optimal parameters are 

\begin{align*}
  \begin{bmatrix} \hat c_0 \\ \hat c_1 \\ \hat c_2 \end{bmatrix} 
  &= -
  \begin{bmatrix} 
    M_{11} & M_{12} & M_{13} \\
    M_{21} & M_{22} & M_{23} \\
    M_{31} & M_{32} & M_{33} \\
  \end{bmatrix}^{-1}
  \times 
  \begin{bmatrix}
    E[E[g \mid X]^2] \left(1 + \frac{\pi_{10}\pi_{01} - \pi_{11}^2}{\pi_{11}(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})}\right) \\
    E[E[g \mid X, Y_1]^2] \left(\frac{-\pi_{10}}{\pi_{11}(\pi_{10} + \pi_{11})}\right) \\
    E[E[g \mid X, Y_2]^2] \left(\frac{-\pi_{01}}{\pi_{11}(\pi_{01} + \pi_{11})}\right) \\
  \end{bmatrix}
\end{align*}

where 

\begin{align*}
  M_{11} &= E[E[g \mid X]^2] \left(\frac{\pi_{11}^2 + \pi_{10}\pi_{01}}{\pi_{11}(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})} - 1\right) \\
  M_{12} &= E[E[g \mid X]^2] \left(\frac{-\pi_{10}\pi_{01}}{\pi_{11}(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})}\right) \\
  M_{13} &= E[E[g \mid X]^2] \left(\frac{-\pi_{10}\pi_{01}}{\pi_{11}(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})}\right) \\
  M_{22} &= E[V(E[g \mid X, Y_1] \mid X)] \left(\frac{\pi_{10}}{\pi_{11}(\pi_{10} + \pi_{11})}\right) \\
  M_{23} &= E[E[g \mid X, Y_1]E[g \mid X, Y_2]] \left(\frac{\pi_{10}\pi_{01}}{\pi_{11}(\pi_{10} + \pi_{11})(\pi_{01} + \pi_{11})}\right) \\
  M_{33} &= E[V(E[g \mid X, Y_2] \mid X)] \left(\frac{\pi_{01}}{\pi_{11}(\pi_{01} + \pi_{11})}\right)
\end{align*}

\newpage 

\printbibliography

\end{document}
