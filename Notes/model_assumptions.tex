\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
\usepackage{biblatex}
\usepackage{booktabs}
\usepackage{float}

\addbibresource{references.bib}

\hypersetup{colorlinks = true, urlcolor = black}

\headheight     15pt
\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\renewcommand{\arraystretch}{1.2}

\begin{document}

\lhead{Caleb Leedy}
\chead{Non-monotone Missingness}
%\chead{STAT 615 - Advanced Bayesian Methods}
%\rhead{Page \thepage\ of \pageref{LastPage}}
\rhead{\today}

\section*{Introduction}

This document reports the optimality of the proposed non-monotone estimator.
It turns out that the proposed estimator is not optimal. We show a better
estimator and present a simulation study.

\section*{Setup}

\begin{itemize}
  \item Let $U = \{1, \dots N\}$ be a finite population.
  \item Consider the random variables $(X_i, Y_{1i}, Y_{2i}, \pi_i) 
    \stackrel{\text{ind}}{\sim} F$ for some unknown $F$ for all $i \in U$.
  \item Let $I_i \stackrel{\text{ind}}{\sim} \pi_i$ for $i \in U$ be the 
    sample inclusion indicator.
  \item We assume that $x_i$ is observed throughout the finite population.
  \item However, $(y_{1i}, y_{2i}, \pi_i)$ is only observed if $I_i = 1$.
  \item The goal is to estimate $\theta = E[g(X, Y_1, Y_2)]$ in the population.
  \item The proposed estimator is

  \begin{align}\label{eq:prop}
    \hat \theta_{\text{eff}} 
    &= n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] \nonumber \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}
      (E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i])\nonumber  \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{2i}}{\pi_{2+}(X_i)}
      (E[g_i \mid X_i, Y_{2i}] - E[g_i \mid X_i])\nonumber  \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)}
      (g_i - E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i, Y_{2i}] + E[g_i \mid X_i])
  \end{align}
  
  \item The goal is to show that this estimator is optimal within the class
    of estimators:

  \begin{align*}
    \hat \theta_{\text{eff}} 
    &= n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}
      (b(X_i, Y_{1i}) - E[g_i \mid X_i]) \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{2i}}{\pi_{2+}(X_i)}
      (a(X_i, Y_{2i}) - E[g_i \mid X_i]) \\ 
    &+ n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)}
      (g_i - b_i - a_i + E[g_i \mid X_i]).
  \end{align*}

\end{itemize}

\section*{Results}

I have done a lot to explore this area but I have several points of confusion.
The first part recaps what Dr. Fuller and I discussed this past week.

\begin{itemize}
  \item Let $g = E[Y_2]$, and suppose that we know the distribution of $X$ and 
    the covariance structure of $[X, Y_1, Y_2, \pi]$. Then instead of modeling
    the relationship between $X$ and $Y_k$, we can use difference estimators:
    $W_1 \equiv Y_1 - b_1 \tilde X$ and $W_2 = \equiv Y_2 - b_2 \tilde X$, where
    $\tilde X = X - E[X]$.
    To minimize the variance of $W_k$ we can choose the optimal value of $b_k$,
     which is 

     \[b_k = \frac{\Cov(Y_k, X)}{\Var(X)}.\]

    Since these values are known, $b_k$ is known. This means that we now have
    the following table:

    \begin{figure}[!ht]
      \centering
      \begin{tabular}{lrr}
        \toprule
         & $W_1$ & $W_2$ \\
         \midrule
        $A_{11}$ & \checkmark & \checkmark \\
        $A_{10}$ & \checkmark & \\
        $A_{01}$ & & \checkmark \\
        $A_{00}$ & & \\
        \bottomrule
      \end{tabular}
    \end{figure}

    Because we assumed that the distribution of $X$ is known, the section
    $A_{00}$ contains no additional information about $Y_1$ or $Y_2$. Let
    $\mu_k = E[Y_k]$.

  \item We now consider a normal model:

    \[ 
      \begin{pmatrix}
        x_i \\ e_{1i} \\ e_{2i}
      \end{pmatrix} \stackrel{ind}{\sim}
      N \left(
      \begin{bmatrix}
        0 \\ 0 \\ 0
      \end{bmatrix},
      \begin{bmatrix}
        1 & 0 & 0 \\
        0 & \sigma_{11} & 0 \\ 
        0 & 0 & \sigma_{22}
      \end{bmatrix},
      \right)
    \]

    and define $y_{1i} = x_i + e_{1i}$ and $y_{2i} = x_i + e_{2i}$. Then $b_1 =
    b_2 = 1$. We define $\bar w_k^{(ij)}$ as the mean of $y_k$ in segment
    $A_{ij}$. This means that we have means $\bar w_1^{(11)}$, $\bar
    w_2^{(11)}$, $\bar w_1^{(10)}$, and $\bar w_2^{(01)}$. Let 
    $W = [\bar w_1^{(11)}, \bar w_2^{(11)}, \bar w_1^{(10)}, \bar w_2^{(01)}]'$,
    then for $n_{ij} = |A_{ij}|$, we have

    \[W - M \mu \sim N(\vec 0, V)\]

    where 

    \[M = 
      \begin{bmatrix}
        1 & 0 \\
        0 & 1 \\
        1 & 0 \\
        0 & 1 \\
      \end{bmatrix}
      \text{ and }
      V = 
      \begin{bmatrix}
        \frac{\sigma_{11}}{n_{11}} & 0 & 0 & 0 \\
        0 & \frac{\sigma_{22}}{n_{11}} & 0 & 0 \\
        0 & 0 & \frac{\sigma_{11}}{n_{10}} & 0 \\
        0 & 0 & 0 & \frac{\sigma_{22}}{n_{01}} \\
      \end{bmatrix}.
    \]

    Thus, the BLUE for $\mu = [\mu_1, \mu_2]'$ is 

    \[ \hat \mu = (M' V^{-1} M)^{-1} M' V^{-1} W. \]

  \item If we do the matrix algebra, $V^{-1} = 
    \text{diag}\left(\frac{n_{11}}{\sigma_{11}}, \frac{n_{11}}{\sigma_{22}},
    \frac{n_{10}}{\sigma_{11}}, \frac{n_{01}}{\sigma_{22}}\right)$ and hence,

    {\renewcommand{\arraystretch}{2}
    \begin{equation}\label{eq:wls}
    \hat \mu = 
      \begin{bmatrix}
        \frac{n_{11}\bar w_1^{(11)} + n_{10}\bar w_1^{(10)}}{n_{11} + n_{10}} \\
        \frac{n_{11}\bar w_2^{(11)} + n_{01}\bar w_2^{(01)}}{n_{11} + n_{01}} \\
      \end{bmatrix}
    \end{equation}
    }

  \item Since this is the BLUE, we would expect it to be at least as good as
    the proposed estimator. And if the proposed estimator is optimal it should
    be equivalent to the BLUE in the case that $X$, $Y_1$ and $Y_2$ are normal.
    So I ran a simulation study to test this. For $i = \{1, \dots, n = 1000\}$,

    \[
      \begin{pmatrix}
        x_i \\ e_{1i} \\ e_{2i}
      \end{pmatrix} \stackrel{ind}{\sim}
      N \left(
      \begin{bmatrix}
        0 \\ 0 \\ 0
      \end{bmatrix},
      \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\ 
        0 & 0 & 1 
      \end{bmatrix},
      \right) \text{ and }
      y_{1i} = x_i + e_{1i}, y_{2i} = x_i + e_{2i}.
    \]

    Each observation $i$ was then assigned a segment $A_{11}$, $A_{10}$,
    $A_{01}$ or $A_{00}$ independently with each draw having probability
    $p_{11} = 0.4$, $p_{10} = 0.2$, $p_{01}= 0.2$ and $p_{00} = 0.2$
    respectively. This means that $\pi_{11} = 0.4$ and 
    $\pi_{1+} = 0.6 = \pi_{2+}$. We let $g = E[Y_2]$ and we test the following
    estimators:

    \begin{itemize}
      \item Oracle: This computes the average value of $Y_2$ in all 
        observations (even when $Y_2$ is not supposed to be observed).
        \[\hat \theta = n^{-1} \sum_{i = 1}^n y_{2i}.\]

      \item CC: This computes the average value of $Y_2$ in all observations 
        in which $Y_2$ is observed.
        \[\hat \theta = \frac{\sum_{i = 1}^n \delta_{2i} y_{2i}}
          {\sum_{i = 1}^n \delta_{2i}}.\]

      \item IPW: This computes the survey weighted average of $Y_2$ when both
        $Y_1$ and $Y_2$ are observed.
        \[\hat \theta = \frac{\sum_{i = 1}^n \delta_{1i}\delta_{2i} y_{2i}}
        {\sum_{i = 1}^n \delta_{1i}\delta_{2i}}.\]
      \item Proposed: This is the proposed estimator from Equation~\ref{eq:prop}.
      \item WLS: This is the weighted linear estimator from Equation~\ref{eq:wls}.
        (Note, since $g = E[Y_2]$ this only contains the second element from
        Equation~\ref{eq:wls}.)
    \end{itemize}

    The results are shown in the table below. This simulation was run with the 
    number of observations $n = 1000$ and the Monte Carlo sample size of 
    $B = 1000$.

    \begin{table}[ht!]
      \centering
      \caption{This table shows the estimators of $\theta = E[Y_2]$. The true 
      value of $\theta$ is $0$. The bias column shows the average bias of the 
      estimator and the actual value of $\theta = 0$ across the $B = 1000$ 
      simulations. The SD column shows the average standard deviation across the 
      $B = 1000$ simulations for each algorithm. The Tstat column displays the 
      t-statistic of a t-test comparing the estimator to the actual value. 
      The value of this column is computed via $\frac{\bar{\hat{\theta}} - 
      \theta}{\sqrt{\Var{\hat \theta}/B}}$. The Pval column displays the 
      p-value of the t-statistic.}
      \begin{tabular}[t]{lrrrr}
        \toprule
        Algorithm & Bias & SD & Tstat & Pval\\
        \midrule
        Oracle & -0.001 & 0.045 & -0.953 & 0.170\\
        CC & 0.000 & 0.056 & -0.059 & 0.476\\
        IPW & 0.000 & 0.071 & -0.183 & 0.428\\
        Prop & 0.000 & 0.051 & -0.131 & 0.448\\
        WLS & -0.001 & 0.042 & -0.470 & 0.319\\
        \bottomrule
      \end{tabular}
      \label{tab:res}
    \end{table}

  \item There are a couple things of note from Table~\ref{tab:res}. First, the 
    WLS estimator has a smaller standard error than the Oracle estimator. This 
    seems incorrect. (Please see the next section for questions.) Second, the 
    proposed estimator did not have a similar standard error as the WLS estimator.
    This suggests that it is not optimal.
\end{itemize}

\section*{Questions for Dr. Fuller}

\begin{enumerate}
  \item For the linear estimator I don't think that I am incorporating sampling
    weights. I am a bit confused about how to do this. Should I compute the 
    means $\bar w_{k}^{(ij)}$ using weights according to the sample weights? 
  \item Is it ok for the linear estimator to outperform the oracle estimator?
    This seems odd to me because the oracle estimator seems to contain more 
    information.
  \item On Tuesday, you asked me to compute the linear estimator in terms of 
    means of $Y$ and estimators of $0$. I think that the WLS estimator is the 
    estimator in terms of estimators of $0$ because $y_k - b_k x$ is an 
    estimator of $0$. To transform the estimator into an estimator in terms of
    $Y$, should I write the form in terms of $Y$ instead of $W$?
\end{enumerate}

\end{document}
