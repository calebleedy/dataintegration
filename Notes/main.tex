\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
\usepackage{biblatex}
\usepackage{booktabs}
\usepackage{float}

\addbibresource{references.bib}

\hypersetup{colorlinks = true, urlcolor = black}

\headheight     15pt
\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\begin{document}

%\lhead{Caleb Leedy}
\chead{Nonmonotone Missingness}
%\chead{STAT 615 - Advanced Bayesian Methods}
%\rhead{Page \thepage\ of \pageref{LastPage}}
\rhead{April 4, 2023}

\section*{Overview:}

The goal of this project is to outperform existing techniques in the literature
related to nonmonotone missing data.

\section*{Initial Simulations:}
% Tables generated from Simulations/nonmonotone.R

\begin{itemize}
  \item \textbf{Implemented simulation of monotone MAR data}: 
    This is correspondingly easier than the subsequent nonmonotone MAR
    simulation. For this simulation we use the following approach:
  \begin{enumerate}
      \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
      \item Using the covariate $X$, determine the probability $p_1$ of $Y_1$
      being observed for each element $i$.
      \item Based on $p_1$, determine if $R_1 = 1$.
      \item If $R_1 = 0$, then $R_2 = 0$. Otherwise, using variables $X$ and
        $Y_1$, determine the probability $p_{12}$.
      \item Based on $p_{12}$ determine if $R_2 = 1$.
  \end{enumerate}
    At the end of the algorithm, we have determined the values of binary
    variables $R_1$ and $R_2$ for each $i$ and if either of them are equal to 1,
    the corresponding level of $Y_{k}$. As is common in this literature, the
    values of $R_1$ and $R_2$ determine if the corresponding variable $Y_1$ or
    $Y_2$ is missing or observed with $R = 1$ indicating $Y$ being observed.
  
  \item \textbf{Implemented simulation of nonmonotone MAR data}: 
    Following the approach of \cite{robins1997non}, I construct a nonmonotone
    MAR simulation with two response variables $Y_1$ and $Y_2$ and one covariate
    $X$. The algorithm to generate the data is the following:
    \begin{enumerate}
        \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
        \item Using the covariate $X_i$, generate probabilities for each element
          $i$ $p_0$, $p_1$, and $p_2$ such that $p_0 + p_1 + p_2 = 1$. 
        \item Select one option based on the three probabilities for each
          element $i$. If 0 is selected: $R_1 = 0$ and $R_2 = 0$; if 1 is
          selected $R_1 = 1$; if 2 is selected, $R_2 = 1$.
        \item We take the next step in multiple cases. If 0 was selected, we are
          done. If 1 was selected, we generate probabilities $p_{12}$ based on
          $X$ and $Y_1$. Then based on this probability, we determine if $R_2 =
          1$. In the same manner, if 2 was selected in the previous step, we
          generate probabilities $p_{21}$ based on $X$ and $Y_2$. Then based on
          this probability, we determine if $R_2 = 1$.
    \end{enumerate}
    Like the monotone MAR simulation this algorithm produces similar final
    results with the determination of binary variables $R_1$ and $R_2$ and
    variables $X$, $Y_1$, and $Y_2$. Unlike the monotone MAR case, the
    nonmonotone MAR includes observations with $Y_2$ observed and $Y_1$ missing.
    
  \item \textbf{Simulation 1 with Monotone MAR}:
    Following the algorithm described in the monotone MAR simulation bullet, 
    we first generate data from the following distributions:
    \begin{align*}
        X_i &\stackrel{iid}{\sim} N(0, 1) \\
        Y_{1i} &\stackrel{iid}{\sim} N(0, 1)\\
        Y_{2i} &\stackrel{iid}{\sim} N(\theta, 1)
    \end{align*}

    Then, we create the probabilities $p_1 = \logistic(x_i)$ and 
    $p_{12} = \logistic(y_{1i})$.
    Since, both $x_i$ and $y_1$ are standard normal distributions, each of these
    probabilities is approximately $0.5$ in expectation.

    The goal of this simulation is to estimate $\theta$. Alternatively, we can
    express this as solving the estimating equation:

    \[g(\theta) \equiv Y_2 - \theta = 0.\]

    We estimate $\theta$ using the following procedures:

    \begin{itemize}
        \item Oracle: This computes $\bar Y$ using \textit{both} the observed
          and missing data.
        \item IPW-Oracle: This is an IPW estimator using only the observed
          values of $Y_2$. The weights (inverse probabilities) use the actual
          probabilities.
        \item IPW-Est: This is an IPW estimator using the probabilities that
          have been estimated by a logistic model.
        \item Semi: This is the monotone semiparametric efficient estimator from
          Slide 11 (Equation 2) of Dr. Kim's Nonmonotone Missingness
          presentation.
    \end{itemize}

    We run this simulation with different values of $\theta$, sample size of
    2000, and 2000 Monte Carlo replications. Each algorithm for each replication
    generates $\hat \theta$. In the subsequent tables, we compute the bias,
    standard deviation (sd), t-statistic (where we test for a significant
    difference between the Monte Carlo mean $\hat \theta$ and the true $\theta$)
    and the p-value of the t-statistic.
    
    \newpage 

    \input{Tables/monomar_t-5.tex}
    \input{Tables/monomar_t0.tex}
    \input{Tables/monomar_t5.tex}

    Overall, these results are mostly what I would have expected. All the
    algorithms estimate the true value of $\theta$ correctly in each case, with
    the oracle estimate having the smallest variance followed by the
    semiparametric algorithm. If there is anything surprising it is that the IPW
    estimator has better performance with the estimated weights compared to the
    true weights. However, I think that this is a known phenomenon.

    \newpage 
    
    \item \textbf{Simulation 1 with Nonmonotone MAR}:

    We generate variables $(X, Y_1, Y_2)$ using the following setup:

    \[\begin{bmatrix}
    X_i \\ \varepsilon_{1i} \\ \varepsilon_{2i}
    \end{bmatrix} \stackrel{iid}{\sim}
    N\left(
    \begin{bmatrix}
        0 \\ 0 \\ \theta
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & \sigma_{yy}\\
        0 & \sigma_{yy} & 1
    \end{bmatrix}
    \right).\]

    Then, 

    \[y_{1i} = x_i + \varepsilon_{1i} \text{ and } 
    y_{2i} = x_i + \varepsilon_{2i}.\]

    Since we have nonmonotone data, our ``Stage 1'' probabilities are
    different. We compute the true Stage 1 probabilities being proportional to
    the following values:
    
    \begin{align*}
        p_0 &= 0.2 \\
        p_1 &= 0.4 \\
        p_2 &= 0.4 \\
    \end{align*}
    
   However, we keep the same structure for the Stage 2 probabilities with:
   $p_{12} = \logistic(y_1)$ and $p_{21} = \logistic(y_2)$. The goal remains to
   estimate $\theta$. We continue to use the Oracle algorithm and the IPW-Oracle
   algorithm. Since we have nonmonotone MAR data, we use the ``Proposed''
   algorithm that is described on Slide 25 (Equation 12) of Dr. Kim's
   presentation. The outcome models were estimated using
   logistic regression and OLS and correctly specified. The response model used
   the oracle estimates of the probabilities. This yields the
   following results:
   
    \input{Tables/nonmonosim1m-5.tex}
    \input{Tables/nonmonosim1m0.tex}
    \input{Tables/nonmonosim1m5.tex}

    \newpage
    
    \item \textbf{Simulation 2 with Nonmonotone MAR}:
    We also want to simulate data that is correlated. For this simulation, we
    focus on $\Cov(Y_1, Y_2)$. The data generating process now has $\sigma_{yy}
    \neq 0$. We are still interested in $\bar Y_2$ and we still run 2000
    simulation with 2000 observations. In all the next simulations the true
    value of $\theta = 0$. The results are the following:

    \input{Tables/nonmonosim2c0.1.tex}
    \input{Tables/nonmonosim2c0.5.tex}
    \input{Tables/nonmonosim2c0.9.tex}

    \newpage

  \item \textbf{Simulation 3 with Nonmonotone MAR}:
    This simulation aims to see if the proposed algorithm is doubly robust.
    First, we check with a misspecified outcome model. In this case the data
    generating procedure is the following:

    \[\begin{bmatrix}
    X_i \\ \varepsilon_{1i} \\ \varepsilon_{2i}
    \end{bmatrix} \stackrel{iid}{\sim}
    N\left(
    \begin{bmatrix}
        0 \\ 0 \\ \theta
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & \sigma_{yy}\\
        0 & \sigma_{yy} & 1
    \end{bmatrix}
    \right).\]

    Then, the true outcome model is,

    \[y_{1i} = x_i + x_i^2 \varepsilon_{1i} \text{ and } 
    y_{2i} = -x_i + x_i^3 + \varepsilon_{2i}.\]

    This procedure causes $X$ to influence both $Y_1$ and $Y_2$ and we still
    have correlation in the error terms of $Y_1$ and $Y_2$. However, since
    neither $Y_1$ nor $Y_2$ are linear in $X$, the model will be misspecified.
    The response mechanisms are first generated MCAR with a probability of
    either $Y_1$ or $Y_2$ being the first variable observed to be $0.4$. (There
    is a $0.2$ probability neither is observed.) Then the probability of the
    other variable being observed is proportional to $\logistic(y_k)$ where
    $y_k$ is the $y$ that has been observed. To ensure that the proposed method
    has the correct propensity score we use the oracle probabilities instead of
    estimating them. This yields the following:

    \input{Tables/nonmonosim3c0.tex}
    \input{Tables/nonmonosim3c0.1.tex}
    \input{Tables/nonmonosim3c0.5.tex}

    \newpage

    Thus, the proposed method is unbiased with a misspecified outcome model.
    We now show a simulation where the outcome model is correctly specified, but
    the response model is not.

  \item \textbf{Simulation 4 with Nonmonotone MAR}:
    Continuing to test if the proposed algorithm is doubly robust, this
    simulation checks a misspecified response model. Instead of using oracle
    weights as in Simulation 3, we estimate the weights for the proposed method.
    However, unlike the true probabilities of being proportional to
    $\logistic(y_k)$, this simulation has the true probabilities being
    proportional to $\logistic(x_i)$. Thus, the true response model is the
    following:

    \begin{enumerate}
      \item (Stage 1) Choose a variable observe. We choose $Y_1$ with
        probability $0.4$, $Y_2$ with probability $0.4$ and neither with
        probability $0.2$. If neither, $R_1 = 0$ and $R_2 = 0$. Otherwise,
        continue to Step 2.
      \item (Stage 2) With probability $p_i \propto \logistic(x_i)$, choose to
        observe the other $Y$ variable. 
    \end{enumerate}

    This sequence generates the missingness indicators $R_1$ and $R_2$. Since,
    the Stage 1 probabilities are fixed and known and the Stage 2 probabilities
    only depend on $x_i$, the missingness is MAR and only a function of $x_i$.
    The algorithms to which we compare still use the oracle weights.

    \input{Tables/nonmonosim4c0.tex}
    \input{Tables/nonmonosim4c0.1.tex}
    \input{Tables/nonmonosim4c0.5.tex}

    The previous version of this simulation used the Stage 2 probability $p_i
    \propto \logistic(y_i)$ where $y_i$ was the observed $Y$ value in Stage 1.
    However, under this setup, our method is biased. This is because

    \[ E\left[E\left[\frac{R_1 R_2}{\pi_{11}(X, Y_1)} Y_1 \mid X\right]\right] 
    \neq
    E\left[\frac{E[Y_1 \mid X]}{\pi_{11}(X, Y_1)} E[R_1 R_2 \mid X]\right] \]

    but

    \[ E\left[E\left[\frac{R_1 R_2}{\pi_{11}(X)} Y_1 \mid X\right]\right] 
    =
    E\left[\frac{E[Y_1 \mid X]}{\pi_{11}(X)} E[R_1 R_2 \mid X]\right].\]

    Thus, there is strong evidence that the proposed method is 
    doubly robust because it is robust to both misspecification in the outcome
    and response model.
\end{itemize}

\newpage

\section*{Missingness Mechanism}

\begin{itemize}
  \item First I am going to reproduce the proof of double robustness that we
    talked about during our last meeting. I think it is insightful for future
    comments:

    \begin{align*}
      E[\hat \theta_{eff} - \theta_n] 
      &= E\left[n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] - g_i\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}
      (b_2(X_i, Y_{1i}) - E[g_i \mid X_i])\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{2i}}{\pi_{2+}(X_i)}
      (a_2(X_i, Y_{2i}) - E[g_i \mid X_i])\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)} 
      (g_i - a_2(X_i, Y_{2i}) - b_2(X_i, Y_{1i}) + E[g_i \mid X_i])\right]\\
      &= n^{-1} \sum_{i = 1}^n (E[E[g_i \mid X_i]] - E[g_i])\\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{1i}}{\pi_{1+}(X_i)} 
      (b_2(X_i, Y_{1i}) - E[g_i \mid X_i]) \mid X_i\right]\right] \\ 
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{2i}}{\pi_{2+}(X_i)} 
      (a_2(X_i, Y_{2i}) - E[g_i \mid X_i]) \mid X_i\right]\right] \\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{1i}
      R_{2i}}{\pi_{11}(X_i)} (g_i - a_2(X_i, Y_{2i}) - b_2(X_i, Y_{1i}) + E[g_i
      \mid X_i]) \mid X_i \right]\right]\\
      \intertext{Since $R_{1i} \perp Y_{1i} \mid X_i$, $R_{2i} \perp Y_{2i} \mid
      X_i$, $(R_{1i}, R_{2i}) \perp (Y_{1i}, Y_{2i}) \mid X_i$ and $\pi_{1+},
      \pi_{2+}$ and $\pi_{11}$ are all free of $Y_{1i}$ and $Y_{2i}$.}
      &= n^{-1} \sum_{i = 1}^n E\left[\frac{R_{1i}}{\pi_{1+}(X_i)} E\left[
        (E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i]) \mid X_i\right]\right] \\ 
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[\frac{R_{2i}}{\pi_{2+}(X_i)} 
      E\left[E[g_i \mid X_i, Y_{2i}] - E[g_i \mid X_i] \mid X_i\right]\right] \\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[\frac{R_{1i}
      R_{2i}}{\pi_{11}(X_i)} E\left[(g_i - E[g_i \mid X_i, Y_{2i}] - 
      E[g_i \mid X_i, Y_{1i}]+ E[g_i \mid X_i]) \mid X_i \right]\right]\\
      \intertext{Since $E[E[g_i \mid X_i, Y_{ki}] \mid X_i] = E[g_i \mid X_i] =
      0$,}
      &= 0.
    \end{align*}

    Thus, if the outcome models are correctly specified $\hat \theta_{eff}$ is
    unbiased. If the response models are correctly specified it is easy to see
    that $\hat \theta_{eff}$ is also unbiased. This means that $\hat
    \theta_{eff}$ is doubly robust.

  \item However, one of the key steps is that \textit{all} the response
    models are free of $Y$. In a previous iteration of Simulation 4, we had
    adopted the framework of \cite{robins1997non} where we first to observe the
    first variable and see if we observe the second variable. In this case, the
    second step can depend on the result of the first step and this is what we
    did. However, this makes it the case that $\pi_11$ is a function of $X_i$
    and $Y_1$ and $Y_2$. In this case $\hat \theta_{eff}$ is not unbiased if the
    response model is misspecified (or even just estimated).

  \item If we modify Simulation 4, such that the second step of observed the
    second variable is proportional to $\logistic(y_k)$ then we get the same
    result as before:

    \input{Tables/nonmonosim5c0.tex}
    \input{Tables/nonmonosim5c0.1.tex}
    \input{Tables/nonmonosim5c0.5.tex}
  
\end{itemize}

\newpage

%\input{estimating_response_mod.tex}

%\input{prop_full_nonmonoest.tex}

\input{minimizing_variance.tex}

\input{calibration_comparison.tex}

\input{efficiency_twophase.tex}

\input{multiphase_estimators.tex}

\printbibliography

\end{document}

