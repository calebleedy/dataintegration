\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
\usepackage{biblatex}
\usepackage{booktabs}
\usepackage{float}

\addbibresource{references.bib}

\hypersetup{colorlinks = true, urlcolor = black}

\headheight     15pt
\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\begin{document}

%\lhead{Caleb Leedy}
\chead{Nonmonotone Missingness}
%\chead{STAT 615 - Advanced Bayesian Methods}
%\rhead{Page \thepage\ of \pageref{LastPage}}
\rhead{April 4, 2023}

\section*{Overview:}

The goal of this project is to outperform existing techniques in the literature
related to nonmonotone missing data.

\section*{Initial Simulations:}
% Tables generated from Simulations/nonmonotone.R

\begin{itemize}
  \item \textbf{Implemented simulation of monotone MAR data}: This is 
  correspondingly easier
  than the subsequent nonmonotone MAR simulation. For this simulation we use the 
  following approach:
  \begin{enumerate}
      \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
      \item Using the covariate $X$, determine the probability $p_1$ of $Y_1$
      being observed for each element $i$.
      \item Based on $p_1$, determine if $R_1 = 1$.
      \item If $R_1 = 0$, then $R_2 = 0$. Otherwise, using variables $X$ and
        $Y_1$, determine the probability $p_{12}$.
      \item Based on $p_{12}$ determine if $R_2 = 1$.
  \end{enumerate}
  At the end of the algorithm, we have determined the values of binary variables
    $R_1$ and $R_2$ for each $i$ and if either of them are equal to 1, the
    corresponding level of $Y_{k}$. As is common in this literature, the values
    of $R_1$ and $R_2$ determine if the corresponding variable $Y_1$ or $Y_2$ is
    missing or observed with $R = 1$ indicating $Y$ being observed.
  
  \item \textbf{Implemented simulation of nonmonotone MAR data}: 
  Following the approach
    of \cite{robins1997non}, I construct a nonmonotone MAR simulation with two 
    response variables $Y_1$ and $Y_2$ and one covariate $X$. The algorithm to
    generate the data is the following:
    \begin{enumerate}
        \item Generate $X$, $Y_1$, and $Y_2$ for elements $i = 1, \dots, n$.
        \item Using the covariate $X_i$, generate probabilities for each element
          $i$ $p_0$, $p_1$, and $p_2$ such that $p_0 + p_1 + p_2 = 1$. 
        \item Select one option based on the three probabilities for each
          element $i$. If 0 is selected: $R_1 = 0$ and $R_2 = 0$. If 1 is
          selected $R_1 = 1$. If 2 is selected, $R_2 = 1$.
        \item We take the next step in multiple cases. If 0 was selected, we are
          done. If 1 was selected, we generate probabilities $p_{12}$ based on
          $X$ and $Y_1$. Then based on this probability, we determine if $R_2 =
          1$. In the same manner, if 2 was selected in the previous step, we
          generate probabilities $p_{21}$ based on $X$ and $Y_2$. Then based on
          this probability, we determine if $R_2 = 1$.
    \end{enumerate}
    Like the monotone MAR simulation this algorithm produces similar final
    results with the determination of binary variables $R_1$ and $R_2$ and
    variables $X$, $Y_1$, and $Y_2$. Unlike the monotone MAR case, the
    nonmonotone MAR includes observations with $Y_2$ observed and $Y_1$ missing.
    
  \item \textbf{Simulation 1 with Monotone MAR}:
    Following the algorithm described in the monotone MAR simulation bullet, 
    we first generate data from the following distributions:
    \begin{align*}
        X_i &\stackrel{iid}{\sim} N(0, 1) \\
        Y_{1i} &\stackrel{iid}{\sim} N(0, 1)\\
        Y_{2i} &\stackrel{iid}{\sim} N(\theta, 1)
    \end{align*}

    Then, we create the probabilities $p_1 = \logistic(x_i)$ and 
    $p_{12} = \logistic(y_{1i})$.
    Since, both $x_i$ and $y_1$ are standard normal distributions, each of these
    probabilities is approximately $0.5$ in expectation.

    The goal of this simulation is to estimate $\theta$. Alternatively, we can
    express this as solving the estimating equation:

    \[g(\theta) \equiv Y_2 - \theta = 0.\]

    We estimate $\theta$ using the following procedures:

    \begin{itemize}
        \item Oracle: This computes $\bar Y$ using \textit{both} the observed
          and missing data.
        \item IPW-Oracle: This is an IPW estimator using only the observed
          values of $Y_2$. The weights (inverse probabilities) use the actual
          probabilities.
        \item IPW-Est: This is an IPW estimator using the probabilities that
          have been estimated by a logistic model.
        \item Semi: This is the monotone semiparametric efficient estimator from
          Slide 11 (Equation 2) of Dr. Kim's Nonmonotone Missingness
          presentation.
    \end{itemize}

    We run this simulation with different values of $\theta$, sample size of
    2000, and 2000 Monte Carlo replications. Each algorithm for each replication
    generates $\hat \theta$. In the subsequent tables, we compute the bias,
    standard deviation (sd), t-statistic (where we test for a significant
    difference between the Monte Carlo mean $\hat \theta$ and the true $\theta$)
    and the p-value of the t-statistic.
    
    \newpage 

    \input{Tables/monomar_t-5.tex}
    \input{Tables/monomar_t0.tex}
    \input{Tables/monomar_t5.tex}

    Overall, these results are mostly what I would have expected. All of the
    algorithms estimate the true value of $\theta$ correctly in each case, with
    the oracle estimate having the smallest variance followed by the
    semiparametric algorithm. If there is anything surprising it is that the IPW
    estimator has better performance with the estimated weights compared to the
    true weights. However, I think that this is a known phenomenon.

    \newpage 
    
    \item \textbf{Simulation 1 with Nonmonotone MAR}:

    We generate variables $(X, Y_1, Y_2)$ using the following setup:

    \[\begin{bmatrix}
    X_i \\ \varepsilon_{1i} \\ \varepsilon_{2i}
    \end{bmatrix} \stackrel{iid}{\sim}
    N\left(
    \begin{bmatrix}
        0 \\ 0 \\ \theta
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & \sigma_{yy}\\
        0 & \sigma_{yy} & 1
    \end{bmatrix}
    \right).\]

    Then, 

    \[y_{1i} = x_i + \varepsilon_{1i} \text{ and } 
    y_{2i} = x_i + \varepsilon_{2i}.\]

    Since we have nonmonotone data, our ``Stage 1'' probabilities are
    different. We compute the true Stage 1 probabilities being proportional to
    the following values:
    
    \begin{align*}
        p_0 &= 0.2 \\
        p_1 &= 0.4 \\
        p_2 &= 0.4 \\
    \end{align*}
    
   However, we keep the same structure for the Stage 2 probabilities with:
   $p_{12} = \logistic(y_1)$ and $p_{21} = \logistic(y_2)$. The goal remains to
   estimate $\theta$. We continue to use the Oracle algorithm and the IPW-Oracle
   algorithm. Since we have nonmonotone MAR data, we use the ``Proposed''
   algorithm that is described on Slide 25 (Equation 12) of Dr. Kim's
   presentation. The outcome models were estimated using
   logistic regression and OLS and correctly specified. The response model used
   the oracle estimates of the probabilities. This yields the
   following results:
   
    \input{Tables/nonmonosim1m-5.tex}
    \input{Tables/nonmonosim1m0.tex}
    \input{Tables/nonmonosim1m5.tex}

    \newpage
    
    \item \textbf{Simulation 2 with Nonmonotone MAR}:
    We also want to simulate data that is correlated. For this simulation, we
    focus on $\Cov(Y_1, Y_2)$. The data generating process now has $\sigma_{yy}
    \neq 0$. We are still interested in $\bar Y_2$ and we still run 2000
    simulation with 2000 observations. In all of the next simulations the true
    value of $\theta = 0$. The results are the following:

    \input{Tables/nonmonosim2c0.1.tex}
    \input{Tables/nonmonosim2c0.5.tex}
    \input{Tables/nonmonosim2c0.9.tex}

    \newpage

  \item \textbf{Simulation 3 with Nonmonotone MAR}:
    This simulation aims to see if the proposed algorithm is doubly robust.
    First, we check with a misspecified outcome model. In this case the data
    generating procedure is the following:

    \[\begin{bmatrix}
    X_i \\ \varepsilon_{1i} \\ \varepsilon_{2i}
    \end{bmatrix} \stackrel{iid}{\sim}
    N\left(
    \begin{bmatrix}
        0 \\ 0 \\ \theta
    \end{bmatrix},
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & \sigma_{yy}\\
        0 & \sigma_{yy} & 1
    \end{bmatrix}
    \right).\]

    Then, 

    \[y_{1i} = x_i + x_i^2 \varepsilon_{1i} \text{ and } 
    y_{2i} = -x_i + x_i^3 + \varepsilon_{2i}.\]

    This procedure causes $X$ to influence both $Y_1$ and $Y_2$ and we still
    have correlation in the error terms of $Y_1$ and $Y_2$. However, since
    neither $Y_1$ nor $Y_2$ are linear in $X$, the model will be misspecified.
    The response mechanisms are first generated MCAR with a probability of
    either $Y_1$ or $Y_2$ being the first variable observed to be $0.4$. (There
    is a $0.2$ probability neither is observed.) Then the probability of the
    other variable being observed is proportional to $\logistic(y_k)$ where
    $y_k$ is the $y$ that has been observed. To ensure that the proposed method
    has the correct propensity score we use the oracle probabilities instead of
    estimating them. This yields the following:

    \input{Tables/nonmonosim3c0.tex}
    \input{Tables/nonmonosim3c0.1.tex}
    \input{Tables/nonmonosim3c0.5.tex}

    \newpage

    Thus, the proposed method is unbiased with a misspecified outcome model.
    We now show a simulation where the outcome model is correctly specified but
    the response model is not.

  \item \textbf{Simulation 4 with Nonmonotone MAR}:
    Continuing to test if the proposed algorithm is doubly robust, this
    simulation checks a misspecified response model. Instead of using oracle
    weights as in Simulation 3, we estimate the weights for the proposed method.
    However, unlike the true probabilities of being proportional to
    $\logistic(y_k)$, this simulation has the true probabilities being
    proportional to $\logistic(x_i)$.
    The algorithms to which we compare still use the oracle weights.

    \input{Tables/nonmonosim4c0.tex}
    \input{Tables/nonmonosim4c0.1.tex}
    \input{Tables/nonmonosim4c0.5.tex}

    Thus, there is strong evidence that the proposed method is 
    doubly robust. Previously, we did not get this result and the reason is 
    the way that the selection probabilities work. When they were proportional
    to $\logistic(y_k)$, the estimator is biased (see the next section);
    however, when the probabilities are proportional to $\logistic(x_i)$ then it
    works.

\end{itemize}

\newpage

\section*{Missingness Mechanism}

\begin{itemize}
  \item First I am going to reproduce the proof of double robustness that we
    talked about during our last meeting. I think it is insightful for future
    comments:

    \begin{align*}
      E[\hat \theta_{eff} - \theta_n] 
      &= E\left[n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] - g_i\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{1i}}{\pi_{1+}(X_i)}
      (b_2(X_i, Y_{1i}) - E[g_i \mid X_i])\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{2i}}{\pi_{2+}(X_i)}
      (a_2(X_i, Y_{2i}) - E[g_i \mid X_i])\right]\\
      &\quad+ E\left[n^{-1} \sum_{i = 1}^n \frac{R_{1i} R_{2i}}{\pi_{11}(X_i)} 
      (g_i - a_2(X_i, Y_{2i}) - b_2(X_i, Y_{1i}) + E[g_i \mid X_i])\right]\\
      &= n^{-1} \sum_{i = 1}^n (E[E[g_i \mid X_i]] - E[g_i])\\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{1i}}{\pi_{1+}(X_i)} 
      (b_2(X_i, Y_{1i}) - E[g_i \mid X_i]) \mid X_i\right]\right] \\ 
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{2i}}{\pi_{2+}(X_i)} 
      (a_2(X_i, Y_{2i}) - E[g_i \mid X_i]) \mid X_i\right]\right] \\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[E\left[\frac{R_{1i}
      R_{2i}}{\pi_{11}(X_i)} (g_i - a_2(X_i, Y_{2i}) - b_2(X_i, Y_{1i}) + E[g_i
      \mid X_i]) \mid X_i \right]\right]\\
      \intertext{Since $R_{1i} \perp Y_{1i} \mid X_i$, $R_{2i} \perp Y_{2i} \mid
      X_i$, $(R_{1i}, R_{2i}) \perp (Y_{1i}, Y_{2i}) \mid X_i$ and $\pi_{1+},
      \pi_{2+}$ and $\pi_{11}$ are all free of $Y_{1i}$ and $Y_{2i}$.}
      &= n^{-1} \sum_{i = 1}^n E\left[\frac{R_{1i}}{\pi_{1+}(X_i)} E\left[
        (E[g_i \mid X_i, Y_{1i}] - E[g_i \mid X_i]) \mid X_i\right]\right] \\ 
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[\frac{R_{2i}}{\pi_{2+}(X_i)} 
      E\left[E[g_i \mid X_i, Y_{2i}] - E[g_i \mid X_i] \mid X_i\right]\right] \\
      &\quad+ n^{-1} \sum_{i = 1}^n E\left[\frac{R_{1i}
      R_{2i}}{\pi_{11}(X_i)} E\left[(g_i - E[g_i \mid X_i, Y_{2i}] - 
      E[g_i \mid X_i, Y_{1i}]+ E[g_i \mid X_i]) \mid X_i \right]\right]\\
      \intertext{Since $E[E[g_i \mid X_i, Y_{ki}] \mid X_i] = E[g_i \mid X_i] =
      0$,}
      &= 0.
    \end{align*}

    Thus, if the outcome models are correctly specified $\hat \theta_{eff}$ is
    unbiased. If the response models are correctly specified it is easy to see
    that $\hat \theta_{eff}$ is also unbiased. This means that $\hat
    \theta_{eff}$ is doubly robust.

  \item However, one of the key steps is that \textit{all} of the response
    models are free of $Y$. In a previous iteration of Simulation 4, we had
    adopted the framework of \cite{robins1997non} where we first to observe the
    first variable and see if we observe the second variable. In this case, the
    second step can depend on the result of the first step and this is what we
    did. However, this makes it the case that $\pi_11$ is a function of $X_i$
    and $Y_1$ and $Y_2$. In this case $\hat \theta_{eff}$ is not unbiased if the
    response model is misspecified (or even just estimated).

  \item If we modify Simulation 4, such that the second step of observed the
    second variable is proportional to $\logistic(y_k)$ then we get the same
    result as before:

    \input{Tables/nonmonosim5c0.tex}
    \input{Tables/nonmonosim5c0.1.tex}
    \input{Tables/nonmonosim5c0.5.tex}
  
\end{itemize}

\newpage

\section*{Estimating the Response Model}

\begin{itemize}
  \item In most (critically \textit{not} Simulation 4) of the simulations of the
    previous section, we used the oracle weights when estimating our proposed
    method. The reasoning for this was straightforward{--}we wanted to ensure
    that the response model was correctly specified and the best case scenario
    is to use the oracle weights which we did.
    
  \item This section focuses more on estimating these response weights. Instead
    of focusing on the proposed method, we will actually be working on
    estimation of the complete case IPW estimator. This model is less complex
    and will thus make it easier to understand where we are making mistakes.

  \item We use the following simulation study:
    \begin{align*}
      x_i &\stackrel{iid}{\sim} N(0, 1)\\
      \varepsilon_{1i} &\stackrel{iid}{\sim} N(0, 1)\\
      \varepsilon_{2i} &\stackrel{iid}{\sim} N(0, 1)\\
      y_{1i} &= x_i + \varepsilon_{1i} \\
      y_{2i} &= x_i + \varepsilon_{2i} \\
    \end{align*}

    To select a missingness pattern for each $i$, we have sequence: first, we
    select the first variable to observe (or neither), then we either select the
    second variable or we do not. In the first step, we select $R_{1i} = 1$ with
    probability $0.4$, $R_{2i} = 1$ with probability $0.4$ and neither variable
    with probability $0.2$. For the second step, we have the probability of
    observing the other variable be $\logistic(x_i)$. This yields the following:

    \input{Tables/ipwsim.tex}
    % TODO: estimate IPW with \pi_{2|1} and \pi_{1|2}.

    The problem with this is that our estimate is biased. For one particular
    realization of the simulation, here is the distribution of the difference
    between the estimated and true probabilities of $\pi_{11}$:

    \includegraphics[width = 0.7\linewidth]{diffhist.png}
  
\end{itemize}

\newpage

\section*{Minimizing the Variance}

\begin{itemize}
  \item The goal of this section is to find optimal values of $b_2(X, Y_1)$ and
    $a_2(X, Y_2)$ such that the variance of $\hat \theta_{eff}$ is minimized.

  \item Recall:

    \begin{align*}
      \hat \theta_{eff} - \hat \theta_n 
      &= n^{-1} \sum_{i = 1}^n E[g_i \mid X_i] \left(1 - \frac{R_{1i}}{\pi_{1+}}
      - \frac{R_{2i}}{\pi_{2+}} + \frac{R_{1i}R_{2i}}{\pi_{11}}\right) \\
      &+ n^{-1} \sum_{i = 1}^n b_2(X_i, Y_{1i}) \left(\frac{R_{1i}}{\pi_{1+}} -
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right)\\
      &+ n^{-1} \sum_{i = 1}^n a_2(X_i, Y_{2i}) \left(\frac{R_{2i}}{\pi_{2+}} -
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right)\\
      &+ n^{-1} \sum_{i = 1}^n g_i \left(\frac{R_{1i}R_{2i}}{\pi_{11}} -
      1\right) \\
      &\equiv A + B + C + D.
    \end{align*}

  \item Notice that we will suppress the fact that response models are functions
    of $X$ (i.e. we write $\pi_{11}$ instead of $\pi_{11}(X)$).
  
  \item To compute the variance, we first solve for each covariance combination.
    Basically, all these computations rely of the following ideas. First, we
    assume that the response model is correctly specified. Consequently,
    $E[A] = E[B] = E[C] = E[D] = 0$ and things work out better. This helps when
    we take the covariance conditional on $X$ because the inner expectations are
    zero. The second key insight it to notice that $E[R_{j}^k] = E[R_j]$ for $j
    \in \{1, 2\}$ and $k \in \mathbb{N}$. This is because $R$ is a binary
    variable. Third, since we assume that the response models are correctly
    specified, we have $E[R_1 \mid X] = \pi_{1+}$, $E[R_2 \mid X] = \pi_{2+}$,
    and $E[R_1, R_2 \mid X] = \pi_{11}$.

    The overall approach to each of this computations is the following:
    (1) take conditional expectations with respect to $X$ (the $\Cov(E[\cdot])$
    term is zero), (2) expand the covariance to $E[XY] - E[X]E[Y]$ (the second
    term is also zero), (3) by the MAR assumption $g, a_2, b_2$ are independent
    of $R_1$ and $R_2$ and we can take the latter out of the expectation, (4)
    evaluate and simplify expressions involving $E[R]$.
  
    \begin{align*}
      \Cov(A, B) &= n^{-2} \sum_{i = 1}^n E\left[\Cov\left(E[g_i \mid X] \left(1
      - \frac{R_{1i}}{\pi_{1+}} - \frac{R_{2i}}{\pi_{2+}} +
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right) \mid X_i,\right. \right.\\
      &\qquad \qquad \qquad \qquad b_2(X_i, Y_{1i}) \left. \left.
      \left(\frac{R_{1i}}{\pi_{1+}} - \frac{R_{1i}R_{2i}}{\pi_{11}}\right) \mid
      X_i\right)\right]\\
      &= n^{-1} E\left[E\left[E[g \mid X] \left(1 - \frac{R_{1i}}{\pi_{1+}} -
      \frac{R_{2i}}{\pi_{2+}} + \frac{R_{1i}R_{2i}}{\pi_{11}}\right)
      b_2(X_i, Y_{1i}) \left(\frac{R_{1i}}{\pi_{1+}} -
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right) \mid X \right]\right] \\
      &= n^{-1} E\left[E[g \mid X] E[b_2(X, Y_1) \mid X]
      \left(\frac{1}{\pi_{1+}} + \frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}} -
      \frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right)\right].
    \end{align*}

    By symmetry,
    \begin{align*}
      \Cov(A, C) 
      &= n^{-1} E\left[E[g \mid X] E[a_2(X, Y_2) \mid X]
      \left(\frac{1}{\pi_{1+}} + \frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}} -
      \frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right)\right].
    \end{align*}

    \begin{align*}
      \Cov(A, D) &= n^{-1} E\left[E\left[E[g \mid X] \left(1 -
      \frac{R_{1i}}{\pi_{1+}} - \frac{R_{2i}}{\pi_{2+}} +
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right) g \left(\frac{R_1 R_2}{\pi_{11}} -
      1\right) \mid X\right] \right]\\
      &= n^{-1} E\left[E[g \mid X]^2 \left(\frac{-1}{\pi_{1+}} -
      \frac{1}{\pi_{2+}} + 2\right)\right].
    \end{align*}

    \begin{align*}
      \Cov(B, C) &= n^{-1} E\left[E[b_2(X, Y_1) \mid X] E[a_2(X, Y_2) \mid X]
      E\left[\left(\frac{R_1}{\pi_{1+}} - \frac{R_1 R_2}{\pi_{11}}\right)
      \left(\frac{R_2}{\pi_{2+}} - \frac{R_1 R_2}{\pi_{11}}\right) \mid
      X\right]\right]\\
      &= n^{-1} E\left[E[b_2(X, Y_1) \mid X] E[a_2(X, Y_2) \mid X]
      \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} - \frac{1}{\pi_{1+}} -
      \frac{1}{\pi_{2+}} + \frac{1}{\pi_{11}}\right)\right].
    \end{align*}

    \begin{align*}
      \Cov(B, D) &= n^{-1} E\left[ E[b_2(X, Y_1) \mid X] E[g \mid X]
      E\left[\left(\frac{R_1}{\pi_{1+}} - \frac{R_1 R_2}{\pi_{11}}\right)
      \left(\frac{R_1 R_2}{\pi_{11}} - 1\right) \mid X\right] \right]\\
      &= n^{-1} E\left[ E[b_2(X, Y_1) \mid X] E[g \mid X]
      \left(\frac{1}{\pi_{1+}} - \frac{1}{\pi_{11}}\right)\right].
    \end{align*}

    By symmetry,
    \begin{align*}
      \Cov(C, D) 
      &= n^{-1} E\left[ E[a_2(X, Y_2) \mid X] E[g \mid X]
      \left(\frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}}\right)\right].
    \end{align*}

    We also compute the variance terms,

    \begin{align*}
      \Cov(A, A) &= n^{-1} E\left[ E\left[ E[g \mid X]^2 \left(1
      - \frac{R_{1i}}{\pi_{1+}} - \frac{R_{2i}}{\pi_{2+}} +
      \frac{R_{1i}R_{2i}}{\pi_{11}}\right)^2 \mid X\right] \right]\\
      &= n^{-1} E \left[ E[g \mid X]^2 \left(-1 + \frac{2 \pi_{11}}{\pi_{1+}
      \pi_{2+}} - \frac{1}{\pi_{1+}} - \frac{1}{\pi_{2+}} +
      \frac{1}{\pi_{11}}\right)\right].
    \end{align*}

    \begin{align*}
      \Cov(B, B) &= n^{-1} E\left[ E\left[b(X, Y_1)^2 \left(\frac{R_1}{\pi_{1+}}
      - \frac{R_1 R_2}{\pi_{11}}\right)^2 \mid X\right] \right]\\
      &= n^{-1} E\left[ E[b_2(X, Y_1)^2 \mid X] \left(\frac{-1}{\pi_{1+}} +
      \frac{1}{\pi_{11}}\right)\right].
    \end{align*}
    
    \begin{align*}
      \Cov(C, C) &= n^{-1} E\left[ E[a_2(X, Y_2)^2 \mid X]
      \left(\frac{-1}{\pi_{2+}} + \frac{1}{\pi_{11}}\right) \right].
    \end{align*}

    \begin{align*}
      \Cov(D, D) &= n^{-1} E\left[ E\left[g_i^2 \left(\frac{R_1 R_2}{\pi_{11}} -
      1\right)^2 \mid X\right] \right]\\
      &= n^{-1} E\left[ E[g^2 \mid X] \left(\frac{1}{\pi_{11}} -
      1\right)\right].
    \end{align*}

    This means that
    \begin{align*}
      \Var(\hat \theta_{eff} - \hat \theta_n) 
      &= \Cov(A, A) + 2 \Cov(A, B) + 2 \Cov(A, C) + 2 \Cov(A, D) + \Cov(B, B) \\
      &+ 2 \Cov(B, C) + 2 \Cov(B, D) + \Cov(C, C) + 2 \Cov(C, D) + \Cov(D, D) \\
      &= n^{-1} E \left[ E[g \mid X]^2 \left(-1 + \frac{2 \pi_{11}}{\pi_{1+}
      \pi_{2+}} - \frac{1}{\pi_{1+}} - \frac{1}{\pi_{2+}} +
      \frac{1}{\pi_{11}}\right)\right] \\
      &+ 2 n^{-1} E\left[E[g \mid X] E[b_2(X, Y_1) \mid X]
      \left(\frac{1}{\pi_{1+}} + \frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}} -
      \frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right)\right]\\
      &+ 2 n^{-1} E\left[E[g \mid X] E[a_2(X, Y_2) \mid X]
      \left(\frac{1}{\pi_{1+}} + \frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}} -
      \frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right)\right]\\
      &+ 2 n^{-1} E\left[E[g \mid X]^2 \left(\frac{-1}{\pi_{1+}} -
      \frac{1}{\pi_{2+}} + 2\right)\right]\\
      &+ n^{-1} E\left[ E[b_2(X, Y_1)^2 \mid X] \left(\frac{-1}{\pi_{1+}} +
      \frac{1}{\pi_{11}}\right)\right]\\
      &+ 2 n^{-1} E\left[E[b_2(X, Y_1) \mid X] E[a_2(X, Y_2) \mid X]
      \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} - \frac{1}{\pi_{1+}} -
      \frac{1}{\pi_{2+}} + \frac{1}{\pi_{11}}\right)\right]\\
      &+ 2 n^{-1} E\left[ E[b_2(X, Y_1) \mid X] E[g \mid X]
      \left(\frac{1}{\pi_{1+}} - \frac{1}{\pi_{11}}\right)\right]\\
      &+  n^{-1} E\left[ E[a_2(X, Y_2)^2 \mid X]
      \left(\frac{-1}{\pi_{2+}} + \frac{1}{\pi_{11}}\right) \right]\\
      &+ 2 n^{-1} E\left[ E[a_2(X, Y_2) \mid X] E[g \mid X]
      \left(\frac{1}{\pi_{2+}} - \frac{1}{\pi_{11}}\right)\right]\\
      &+ n^{-1} E\left[ E[g^2 \mid X] \left(\frac{1}{\pi_{11}} -
      1\right)\right].
    \end{align*}

    Differentiating yields:

    \begin{align*}
      \frac{\partial}{\partial a_2} \Var(\hat \theta_{eff} - \hat \theta_n)
      &= E\left[E[g \mid X] \left(\frac{1}{\pi_{1+}} + \frac{2}{\pi_{2+}} -
      \frac{2}{\pi_{11}} - \frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right)\right] \\
      &+ E\left[E[b_2(X, Y_1) \mid X] \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} -
      \frac{1}{\pi_{1+}} - \frac{1}{\pi_{2+}} +
      \frac{1}{\pi_{11}}\right)\right]\\
      &+ E\left[ E[a_2(X, Y_2) \mid X] \left(\frac{-1}{\pi_{2+}} +
      \frac{1}{\pi_{11}}\right)\right]\\
      &\equiv 0, \text{ and }\\
      \frac{\partial}{\partial b_2} \Var(\hat \theta_{eff} - \hat \theta_n)
      &= E\left[E[g \mid X] \left(\frac{2}{\pi_{1+}} + \frac{1}{\pi_{2+}} -
      \frac{2}{\pi_{11}} - \frac{\pi_{11}}{\pi_{1+}\pi_{2+}}\right)\right]\\
      &+ E\left[E[a_2(X, Y_2) \mid X] \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} -
      \frac{1}{\pi_{1+}} - \frac{1}{\pi_{2+}} +
      \frac{1}{\pi_{11}}\right)\right]\\
      &+ E\left[ E[b_2(X, Y_2) \mid X] \left(\frac{-1}{\pi_{1+}} +
      \frac{1}{\pi_{11}}\right)\right]\\
      &\equiv 0. 
    \end{align*}


    Substitution shows that these constraints are equivalent to:

    \begin{align*}
      E&\left[E[g \mid X] \left(\frac{-1}{\pi_{1+}} +
    \frac{1}{\pi_{2+}}\right)\right] + E\left[ E[b_2(X, Y_1) \mid X]
    \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} -\frac{1}{\pi_{2+}}\right)\right]\\
      &- E\left[E[a_2(X, Y_2) \mid X] \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} - 
    \frac{1}{\pi_{1+}}\right)\right] \equiv 0
    \end{align*}

    where is the same as,

    \begin{align*}
      E&\left[ E[b_2(X, Y_1) \mid X] \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} -
    \frac{1}{\pi_{2+}}\right) \right] + E\left[E[g \mid X]
      \left(\frac{1}{\pi_{2+}}\right)\right] \\
      &=
    E\left[ E[a_2(X, Y_2) \mid X] \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}} -
    \frac{1}{\pi_{1+}}\right) \right] + E\left[E[g \mid X]
    \left(\frac{1}{\pi_{1+}}\right)\right].
    \end{align*}

  \item These constraints can be satisfied (this is sufficient but maybe not
    necessary) if 

    \begin{align*}
      E&\left[(E[b_2(X, Y_1) \mid X] - E[a_2(X, Y_2) \mid X])
      \left(\frac{\pi_{11}}{\pi_{1+} \pi_{2+}}\right) \right] = 0\\
      E&\left[\left(\frac{1}{\pi_{1+}} - \frac{1}{\pi_{2+}}\right)(E[a_2(X, Y_2)
      \mid X] + E[b_2(X, Y_1) \mid X] - 2E[g \mid X])\right] = 0.
    \end{align*}

\end{itemize}

\textbf{Questions for Dr. Kim}
\begin{itemize}
  \item Where do we go from here?
  \item This is a functional expectation that I need to calibrate. Should I use
    some sort of basis spline to try to estimate these expectations and then set
    them equal to zero? I don't think that I have worked with this kind of
    constraint before.
\end{itemize}
%\input{prop_full_nonmonoest.tex}

\newpage

\printbibliography

\end{document}

