\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}
\usepackage{float}

\addbibresource{references.bib}

\hypersetup{colorlinks = true, urlcolor = black}

\headheight     15pt
\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}

\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\argmax}{{\text{argmax}}}
\newcommand{\argmin}{{\text{argmin}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\bf}[1]{\mathbf{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

\title{Debiased calibration for generalized two-Phase sampling}
\author{Caleb Leedy}
\maketitle 


\section{Introduction}

\begin{itemize}
\item Combining information from several sources is an important practical problem. 
\item In many cases, we do not have direct access to the other sources. We can only obtain summary statistics (and their standard errors) for the external data sources. We wish to incorporate the information from external  sources in our in-house data effectively using calibration weighting.  
\item We formulate the problem as a generalized two-phase sampling where the first phase sample can be obtained from multiple sources. The second-phase sample is our in-house data in which we want to construct the calibration weights. 
\item To achieve the goal, we first consider  the  classical two-phase sampling setup where the second-phase sample is a subset of the first-phase sample. After that, we extend the setup to more general cases such as non-nested two-phase sampling or multiple independent surveys with some common measurements. 
\item (To simplify the presentation, I wonder whether we can just use SRS in the first-phase sampling. ) 
\end{itemize}
\section{Topic 1: Classical Two-Phase Sampling}

\subsection{Background and Introduction}

Consider a finite population of size $N$ containing elements $(X_i, Y_i)$ where
an initial (Phase 1) sample of size $n$ is selected and $X_i$ is observed. Then
from the Phase 1 sample of elements, a (Phase 2) sample of size $r < n$ is
selected and $Y_i$ is observed. This is two-phase sampling (See 
\cite{fuller2009sampling}, \cite{kim2024statistics} for general references.) The
goal of two-phase sampling is to construct an estimator of $Y$ not
only using the observed information from the Phase 2 sample but also
incorporating the extra auxilary information of $X$ from the Phase 1 sample, and
the challenge is doing this efficiently.

An easy-to-implement unbiased estimator in the spirit of a Horvitz-Thompson
estimator (\cite{horvitz1952generalization}, \cite{narain1951sampling}) is the
$\pi^*$-estimator. Let $\pi_i^{(2)}$ be the response probability of element $i$
being observed in the Phase 2 sample. Then, allowing the elements in the Phase 1
sample to be represented by $A_1$ and the elements in the Phase 2 sample to be
denoted as $A_2$,

\begin{align*}
  \pi_i &= \sum_{A_2: i \in A_2} \Pr(A_2) \\ 
        &= \sum_{A_1: A_2 \subseteq A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid
        A_1) \Pr(A_1) \\
        &= \sum_{A_1: i \in A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1) \Pr(A_1).
\end{align*}

If we define $\pi_{2i | 1}^{(2)} = \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1)$ and
$\pi_i^{(1)} = \sum_{A_1: i \in A_1} \Pr(A_1)$ then,

$$ \pi_i = \pi_{2i | 1}^{(2)} \pi_i^{(1)}.$$
\textcolor{red}{(note: there is an abuse of notation. You do not need to introduce superscript $(2)$ on $\pi_{2i \mid 1}$.) }
This means that we can define the $\pi^*$-estimator as the following design
unbiased estimator:

$$ \hat Y_{\pi^*} = \sum_{i \in A_2} \frac{y_i}{\pi_{2i | 1}^{(2)} \pi_i^{(1)}}.$$

\textcolor{red}{Dr. Kim: Should I provide the proof for design unbiasedness of
$\hat Y_{\pi^*}$? } \textcolor{blue}{Not necessary. You may cite a book or a paper  for this claim. }

While unbiased, the $\pi^*$-estimator unfortunately does not account for the
additional information contained in the auxiliary Phase 1 variable $X$. The
two-phase regression estimator $\hat Y_{reg, tp}$ does incorporate \textcolor{blue}{$\hat{X}_1$ obtained from the phase one sample. That is, we can leverage the external information $\hat{X}_1$ to improve the $\pi^*$-estimator in the second-phase sample. } 
The two-phase 
regression estimator has the form,

$$ \hat Y_{reg, tp} 
= \sum_{i \in A_1} \frac{1}{\pi_i^{(1)}} x_i \hat \beta_2+ \sum_{i \in A_2}
\frac{1}{\pi_i^{(1)}\pi_{2i|1}^{(2)}} (y_i - x_i \hat \beta_2)$$
where $$\hat \beta_2 = \left(\sum_{i \in A_2} 
  \frac{x_i x_i'}{\pi_{2i|1}^{(2)}}\right)^{-1} 
  \sum_{i \in A_2} \frac{x_i y_i}{\pi_i^{(1)}\pi_{2i|1}^{(2)}}. 
  \footnote{Caleb, we do not have to use $\pi_{2i \mid 1}^{(2)}$ in computing $\hat{\beta}_2$. Please check my book. }
  $$ 


\textcolor{red}{I suggest that we use 
$$\hat \beta_2 = \left(\sum_{i \in A_2} 
  \frac{x_i x_i'}{\pi_{i}^{(1)} q_i }\right)^{-1} 
  \sum_{i \in A_2} \frac{x_i y_i}{\pi_i^{(1)} q_i }
$$ 
where $q_i=q(\bx_i)$ is a function of $\bx_i$. This will have some connection with model-optimal regression estimator (under superpopulation model).  
}

  
  The regression
estimator is the minumum variance design consistent linear estimator which is
easily shown to be the case because $\hat Y_{reg, tp} = \sum_{i \in A_2} \hat
w_{2i} y_i / \pi_i^{(1)}$ where 

$$\hat w_i = \argmin_{w} \sum_{i \in A_2} (w_i - \pi_{2i|1}^{-1})^2 \text{ such
that } \sum_{i \in A_2} w_i x_i / \pi_i^{(1)} = \sum_{i \in A_1} x_i /
\pi_i^{(1)}.$$

\textcolor{red}{Using $q_i$, we can construct 
$$\hat w_{2i} = \argmin_{w} \sum_{i \in A_2} (w_{2i} - \pi_{2i|1}^{-1})^2  q_i \text{ such
that } \sum_{i \in A_2} w_{2i}  x_i / \pi_i^{(1)} = \sum_{i \in A_1} x_i /
\pi_i^{(1)}$$
as a way to implement the two-phase regression estimator indirectly using calibration weighting. 
}


This means that $\hat Y_{reg, tp}$ is also a calibration estimator. The idea
that regression estimation is a form of calibration was extended by 
\cite{deville1992calibration} to consider loss functions other than just squared
loss. They generalized the loss function to minimize $\sum_i G(w_i, d_i)q_i$ for
weights $w_i$ and design-weights $d_i$ where $G()$ is non-negative, strictly
convex function with respect to $w$, defined on an interval containing $d_i$,
with $g(w_i, d_i) = \partial G / \partial w$ continuous.
\footnote{The \cite{deville1992calibration} paper considers regression estimators
for a single phase setup, which we apply to our two-phase example. } This
generalization includes empirical likelihood estimation, and maximum entropy
estimation amoung others. The variance estimation is based on a linearization
that shows that minimizing the generalized loss function subject to the
calibration constraints is asymptotically equivalent to a regression estimator.

While this generalization is useful to an analyst who may want different
properties of their estimator from maximum entropy estimation rather than the
minimal squared loss, unless $\pi_{2i|1}^{-1} \in C(X)$, the estimator is not
design consistent. \footnote{ \textcolor{red}{I do not understand this part.} } 
Furthermore, at a conceptual level, the regression estimator
has a nice feature that its two terms can be thought about as minimizing
variance and bias correction,

$$ \hat Y_{reg, tp} 
= \underbrace{\sum_{i \in A_1} \frac{x_i \hat \beta_2}{\pi_i^{(1)}}}_{Minimizing
the variance} + \underbrace{\sum_{i \in A_2}
\frac{1}{\pi_i^{(1)}\pi_{2i|1}^{(2)}} (y_i - x_i \hat \beta_2)}_{Bias
correction}.$$

The \cite{deville1992calibration} method incorporates the design weights into
the loss function, which is the part minimizing the variance. Instead, there is
a desire to get design consistency from the calibration term. In 
\cite{kwon2024debiased}, the authors show that for a generalized entropy
function $G(w)$, including a term of $g(\pi_{2i|1}^{-1})$ into the calibration
for $g = \partial G / \partial w$ not only creates a design consistent
estimator, but it also has better efficiency than the generalized regression
estimators of \cite{deville1992calibration}.

Unfortunately, the method of \cite{kwon2024debiased} requires known calibration
levels (no uncertainty) for the finite population. It does not handle the
two-phase setup where we need to estimate the finite population total of $x$
from the Phase 1 sample. Hence, we extend this method to have a valid variance 
estimator when including estimated Phase 1 weights.

\subsection*{Methodology}

% TODO: Add detailed method for the setup of the debiased calibration method and
% explain how we estimate the weights in two-phase sampling.
% DUE: Thursday April 11

\subsection*{Theoretical Results}

% TODO: Provide asymptotic results showing the variance estimation of the
% estimated debiased calibration method
% DUE: Thursday April 11 (I have already showed the proof but I might finish the
% simulation study before writing this up.)

\subsection*{Simulation Studies}

% TODO: Conduct a simulation study highlighting the proposed method
% DUE: Friday April 12

\section*{Topic 2: Non-nested Two-Phase Sampling}

\section*{Topic 3: Multi-source Two-Phase Sampling}

\printbibliography

\end{document}
