\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
%\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}
\usepackage{float}

\usepackage{amsmath, amsthm, mathtools,commath}
\usepackage{graphics, color}
\usepackage{latexsym}
\usepackage{amssymb, amsfonts, bm}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{makeidx}
\usepackage{fullpage}
\usepackage{booktabs, arydshln}
\usepackage{comment} 
%\makeindex

\hbadness=10000 \tolerance=10000 \hyphenation{en-vi-ron-ment
in-ven-tory e-num-er-ate char-ac-ter-is-tic}

\usepackage[round]{natbib}
%\bibliographystyle{apalike2}
% \bibliographystyle{jmr}


\newcommand{\biblist}{\begin{list}{}
{\listparindent 0.0cm \leftmargin 0.50cm \itemindent -0.50 cm
\labelwidth 0 cm \labelsep 0.50 cm
\usecounter{list}}\clubpenalty4000\widowpenalty4000}
\newcommand{\ebiblist}{\end{list}}

\newcounter{list}

%\usepackage{setspace}

%\usepackage{hangpar}
\newcommand{\lbl}[1]{\label{#1}{\ensuremath{^{\fbox{\tiny\upshape#1}}}}}
% remove % from next line for final copy
\renewcommand{\lbl}[1]{\label{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{result}{Result}

\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\bf}[1]{\mathbf{#1}}


\begin{document}

\title{Similarities between Nonnested Regression Estimation and Ridge Regression}
\author{Caleb Leedy}
\maketitle 

\baselineskip .3in

\section{Nonnested Regression}

In the case of non-nested two phase sampling, we have $A_1 = (X_i)_{i = 1}^{n_1}$
and $A_2 = (X_1, Y_i)_{i = 1}^{n_2}$ with $A_1$ and $A_2$ being selected
independently from the same sampling frame. The regression estimator is then

\begin{align*}
\hat{\bar{Y}}_{reg} 
  &= \hat{\bar{Y}}_{HT} + (\hat{\bar{X}}_c - \hat{\bar{X}}_2)' \hat \beta_2 
  & \text{for } \hat \beta_2 = \left(\sum_{i \in A_2} x_i x_i'\right)^{-1} 
    \sum_{i \in A2} x_i y_i \text{ with } x_{1i} = 1 \\
  &= \hat{\bar{Y}}_{HT} + (\hat{\bar{X}}_1 - \hat{\bar{X}}_2)'W'\hat \beta_2 
  & \text{ if } \hat{\bar{X}}_c = W \hat{\bar{X}}_1 + (I - W) \hat{\bar{X}}_2\\
  &= \sum_{i \in A_1} x_i' W' \hat \beta_2 + \sum_{i \in A_2} (y_i - x_i' W'
  \hat \beta_2).
\end{align*}

While the matrix $W$ controls the interaction between $A_1$ and $A_2$ it also
plays the role of shrinkage on $\hat \beta_2$.

\section{Ridge Regression}

Given a sample $A$, ridge regression solves the optimization problem,

$$\hat \beta = \argmin_{\beta \in \R^p} \sum_{i \in A} (y_i - x_i'\beta)^2 + \beta^T
(\lambda I_p) \beta.$$

Differentiating with respect to $\beta$ and setting this equal to zero yields a
solution,

$$\hat \beta = \left(\sum_{i \in A} x_i x_i' + \lambda I_p\right)^{-1} \sum_{i
\in A} x_i' y_i = (X'X + \lambda I_p)^{-1} X'Y.$$

Let $\hat \beta_{OLS} = \hat \beta_2 = (X'X)^{-1} X'Y$, then using the
Sherman-Morrison-Woodbury inverse formula,

$$\hat \beta = (I_p - (X'X)^{-1}(\lambda^{-1}I_p + (X'X)^{-1}))^{-1} \hat \beta_{OLS}.$$

\section{Discussion}

The previous two sections suggest that if we let

$$
\begin{aligned}
  W &= (I_p - (X'X)^{-1}(\lambda^{-1}I_p + (X'X)^{-1}))^{-1} \\
  \iff \lambda W &= (X'X)(I_p - W) \\ 
  \iff \lambda &= (X'X)(W^{-1} - I_p)
\end{aligned}
$$

then we would have equivalent results. In this way, non-nested two phase
sampling is like ridge regression.

\end{document}
