\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
%\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}
\usepackage{float}

\usepackage{amsmath, amsthm, mathtools,commath}
\usepackage{graphics, color}
\usepackage{latexsym}
\usepackage{amssymb, amsfonts, bm}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{makeidx}
\usepackage{fullpage}
\usepackage{booktabs, arydshln}
\usepackage{comment} 
%\makeindex

\hbadness=10000 \tolerance=10000 \hyphenation{en-vi-ron-ment
in-ven-tory e-num-er-ate char-ac-ter-is-tic}

\usepackage[round]{natbib}
%\bibliographystyle{apalike2}
% \bibliographystyle{jmr}


\newcommand{\biblist}{\begin{list}{}
{\listparindent 0.0cm \leftmargin 0.50cm \itemindent -0.50 cm
\labelwidth 0 cm \labelsep 0.50 cm
\usecounter{list}}\clubpenalty4000\widowpenalty4000}
\newcommand{\ebiblist}{\end{list}}

\newcounter{list}

%\usepackage{setspace}

%\usepackage{hangpar}
\newcommand{\lbl}[1]{\label{#1}{\ensuremath{^{\fbox{\tiny\upshape#1}}}}}
% remove % from next line for final copy
\renewcommand{\lbl}[1]{\label{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{result}{Result}

\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\bf}[1]{\mathbf{#1}}


\begin{document}

\title{Debiased Calibration for Generalized Two-Phase Sampling}
\author{Caleb Leedy}
\maketitle 

\baselineskip .3in

\section{Introduction}

Combining information from several sources is an important practical problem. (CITEME)
We want to incorporate information from external data sources to reduce the bias
in our estimates or improve the estimator's efficiency. For many problems, the
additional information consists of summary statistics with standard errors. The
goal of this project is to incorporate external information with existing data 
to create more efficient estimators using calibration weighting.

To model this scenaro, we formulate the problem as a generalized two-phase
sample where the first phase sample consists of data from multiple sources. The
second phase sample contains our existing data. To motivate this setup, we
consider the following approach: first, we consider the classical two-phase
sampling setup where the second phase sample is a subset of the first phase
sample; then, we extend this setup to consider non-nested two-phase samples;
and finally, we consider the more general approach of having multiple sources.

%\begin{itemize}
%\item To achieve the goal, we first consider  the  classical two-phase sampling
%setup where the second-phase sample is a subset of the first-phase sample.
%  After that, we extend the setup to more general cases such as non-nested
%  two-phase sampling or multiple independent surveys with some common
%  measurements. 
%\item The proposed method can be called two-step calibration. In the
%first-step, the best linear unbiased estimators of the auxiliary variable
%totals are computed. In the second-step, the final calibration weights are
%constructed to match (benchmark) with the best estimators computed from Step 1. 
%\end{itemize}

\section{Topic 1: Classical Two-Phase Sampling}

\subsection{Background}

Consider a finite population of size $N$ containing elements $(X_i, Y_i)$ where
an initial (Phase 1) sample of size $n_1$ is selected and $X_i$ is observed. Then
from the Phase 1 sample of elements, a (Phase 2) sample of size $n_2 < n_1$ is
selected and $Y_i$ is observed. This is two-phase sampling (See 
\cite{fuller2009sampling}, \cite{kim2024statistics} for general references.) The
goal of two-phase sampling is to construct an estimator of $\bar Y_N$ 
that uses both the observed information in the Phase 2 sample and also the extra
auxiliary information from $X$ in the Phase 1 sample.
The challenge is doing this efficiently.

An easy-to-implement unbiased estimator in the spirit of a Horvitz-Thompson (HT)
estimator (\cite{horvitz1952generalization}, \cite{narain1951sampling}) is the
$\pi^*$-estimator. Let $\pi_i^{(2)}$ be the response probability of element $i$
being observed in the Phase 2 sample. Then, allowing the elements in the Phase 1
sample to be represented by $A_1$ and the elements in the Phase 2 sample to be
denoted as $A_2$,
%\begin{align*}
%  \pi_i &= \sum_{A_2: i \in A_2} \Pr(A_2) \\ 
%        &= \sum_{A_1: A_2 \subseteq A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid
%        A_1) \Pr(A_1) \\
%        &= \sum_{A_1: i \in A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1) \Pr(A_1).
%\end{align*}
%\textcolor{red}{(I am not sure whether this part is necessary.) }
if we define $\pi_{2i | 1} = \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1)$ and
$\pi_{1i} = \sum_{A_1: i \in A_1} \Pr(A_1)$ then,

$$ \pi_i^{(2)}(A_1) = \pi_{2i | 1} \pi_{1i}.$$

This means that we can define the $\pi^*$-estimator as the following design
unbiased estimator:

$$ \hat Y_{\pi^*} = \sum_{i \in A_2} \frac{y_i}{\pi_{2i | 1} \pi_{1i}}.$$

While unbiased (see \cite{kim2024statistics}), the $\pi^*$-estimator
does not account for the additional information contained in the auxiliary Phase
1 variable $X$. The two-phase regression estimator $\hat Y_{reg, tp}$ does incorporate 
information for $X$ by using the estimate $\hat X_1$ from the Phase 1 sample.
This is how we can leverage the external information $\hat X_1$ to improve the
initial $\pi^*$-estimator in the second phase sample.
The two-phase regression estimator has the form,

$$ \hat Y_{reg, tp} 
= \sum_{i \in A_1} \frac{1}{\pi_{1i}} x_i \hat \beta_q+ \sum_{i \in A_2}
\frac{1}{\pi_{1i}\pi_{2i|1}} (y_i - x_i \hat \beta_q)$$

where for $q_i = q(x_i)$ and is a function of $x_i$,
$$
\hat \beta_q = \left(\sum_{i \in A_2} 
  \frac{x_i x_i'}{\pi_{1i} q_i}\right)^{-1} 
\sum_{i \in A_2} \frac{x_i y_i}{\pi_{1i} q_i}.$$ 

The regression estimator is the minimum variance design consistent linear
estimator which is easily shown to be the case because $\hat Y_{reg, tp} =
\sum_{i \in A_2} \hat w_{2i} y_i / \pi_{1i}$ where 

$$\hat w_{2i} = \argmin_{w} \sum_{i \in A_2} (w_{2i} - \pi_{2i|1}^{-1})^2 q_i \text{ such
that } \sum_{i \in A_2} w_{2i} x_i / \pi_{1i} = \sum_{i \in A_1} x_i / \pi_{1i}.$$

This means that $\hat Y_{reg, tp}$ is also a calibration estimator. The idea
that regression estimation is a form of calibration was noted by
\cite{deville1992calibration} and extended by them to consider loss functions
other than just squared loss. Their generalized loss function minimizes
$\sum_i G(w_i, d_i)q_i$ for weights $w_i$ and design-weights $d_i$ where
$G(\cdot)$ is a non-negative, strictly convex function with respect to $w$,
defined on an interval containing $d_i$, with $g(w_i, d_i) = \partial G /
\partial w$ continuous.\footnote{The \cite{deville1992calibration} paper
considers regression estimators for a single phase setup, which we apply to our
two-phase example.} This
generalization includes empirical likelihood estimation, and maximum entropy
estimation among others. The variance estimation is based on a linearization
that shows that minimizing the generalized loss function subject to the
calibration constraints is asymptotically equivalent to a regression estimator.

Furthermore, the regression estimator
has a nice feature that its two terms can be thought about as minimizing the
variance and bias correction,

$$ \hat Y_{reg, tp} 
= \underbrace{\sum_{i \in A_1} \frac{x_i \hat \beta_q}{\pi_{1i}}}_{
  \text{ Minimizing the variance}} + \underbrace{\sum_{i \in A_2}
\frac{1}{\pi_{1i}\pi_{2i|1}} (y_i - x_i \hat \beta_q)}_{
\text{Bias correction}}.$$

The \cite{deville1992calibration} method incorporates the design weights into
the loss function, which is the part minimizing the variance. We would rather
separate have bias calibration separate from the minimizing the variance so that
we can control each in isolation. In
\cite{kwon2024debiased}, the authors show that for a generalized entropy
function $G(w)$, including a term of $g(\pi_{2i|1}^{-1})$ into the calibration
for $g = \partial G / \partial w$ not only creates a design consistent
estimator, but it also has better efficiency than the generalized regression
estimators of \cite{deville1992calibration}.

The method of \cite{kwon2024debiased} requires known finite population 
calibration levels. It does not handle the
two-phase setup where we need to estimate the finite population total of $x$
from the Phase 1 sample. In the rest of the section, we extend this method to 
two phase sampling so that we have a valid 
estimator when including estimated Phase 1 weights with appropriate variance
estimation.

\subsection{Methodology}

We follow the approach of \cite{kwon2024debiased} for the debiased calibration
method. We consider maximizing the generalized entropy \cite{gneiting2007strictly},

\begin{equation}\label{eq:primalloss}
  H(w) = - \sum_{i \in A_2} \frac{1}{\pi_{1i}} G(w_{2i}) q_i
\end{equation}

where $G: \mathcal{V} \to \R$ is strictly convex, differentiable function
subject to the constraints:

\begin{equation}\label{eq:calconst1}
  \sum_{i \in A_2} \frac{x_i w_{2i}q_i}{\pi_{1i}} = 
\sum_{i \in A_1} \frac{x_iq_i}{\pi_{1i}}
\end{equation}

and 

\begin{equation}\label{eq:calconst2}
  \sum_{i \in A_2} \frac{g(\pi_{2i|1}^{-1})w_{2i}q_i}{\pi_{1i}} = 
  \sum_{i \in A_1} \frac{g(\pi_{2i|1}^{-1})q_i}{\pi_{1i}}.
\end{equation}

The first constraint is the existing calibration constraint and the second
ensures that design consistency is achieved. Here, 
$g(w) = \partial G / \partial w$. 
The original method of \cite{kwon2024debiased} only considered having finite
population quantities on the right hand side of \ref{eq:calconst1}.
Let $z_i = (x_i / q_i, g(\pi_{2i|1}^{-1}))$. Letting $w_{1i} = \pi_{1i}^{-1}$,
the the goal is to solve,

\begin{equation}\label{eq:primal}
  \argmin_{w_{2|1}} \sum_{i \in A_2} \frac{1}{\pi_{1i}} G(w_{2i}) q_i 
  \text{ such that}
  \sum_{i \in A_2} w_{1i} w_{2i|1} z_i q_i = \sum_{i \in A_1} w_{1i} z_i q_i.
\end{equation}

Let $\hat w_{2i|1}$ be the solution to Equation~\ref{eq:primal}, then the
estimate of $Y_N$ is $\hat Y_{DCE} = \sum_{i \in A_2} w_{1i} \hat w_{2i|1} y_i$.


\subsection{Theoretical Results}

\begin{theorem}[Design Consistency]\label{thm:dc1}
  Under some regularity conditions,

  $$\hat Y_{DCE} = \hat Y_\ell(\lambda^*, \phi^*) + O_p(N / n_2)$$

  where

  $$\hat Y_{\ell}(\hat \lambda, \phi^*) = \hat Y_{DCE}(\hat \lambda) + 
  \left(\sum_{i \in A_1} w_{1i} z_i q_i - \sum_{i \in A_2} w_{1i} \hat w_{2i|1}(\hat
  \lambda) z_i q_i\right)\phi^*$$

  and

  $$\phi^* = 
  \left[\sum_{i \in U} \frac{\pi_{2i|1}z_i z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
  \sum_{i \in U} \frac{\pi_{2i|1}z_i y_i}{g'(d_{2i|1})}.$$

   \begin{eqnarray*}
   \hat Y_{\ell}( \lambda^*, \phi^*) &=&   \hat{Y}_{\pi^*} + 
  \left(\sum_{i \in A_1} w_{1i} x_i  -  \sum_{i \in A_2} w_{1i} \pi_{2i \mid
  1}^{-1} x_i  \right)^T \phi_1^* + \left(\sum_{i \in A_1} w_{1i} g_i   -
\sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1}g_i  \right)^T \phi_2^*  
  \end{eqnarray*} 
  and  $$\begin{pmatrix}
  \phi_1^* \\
  \phi_2^* 
  \end{pmatrix}
  = \left[ \sum_{i \in U} \frac{\pi_{2i \mid 1} }{ g'(d_{2i|1}) q_i} 
  \begin{pmatrix}
  \bx_i \bx_i^T &   \bx_i g_i   \\
  g_i  \bx_i^T   & g_i^2     \end{pmatrix} \right]^{-1}
  \sum_{i \in U} \frac{\pi_{2i|1}}{ g'(d_{2i | 1}) q_i} \begin{pmatrix}
    \bx_i \\ g_i 
  \end{pmatrix}y_i $$
with $g_i = g( \pi_{2i |1}^{-1}) q_i$.
\end{theorem}


\textcolor{red}{I have tried to improve the expression of the linearization
formula. Please check if my algebra above is correct. }
\textcolor{blue}{This is correct. I have almost the same expression in
Theorem~\ref{thm:dc2}.}
\begin{proof}
  In this proof, we derive the solution to Equation~\ref{eq:primal} and show
  that it is asymptotically equivalent to a regression estimator. Using the
  method of Lagrange multipliers, to solve Equation~\ref{eq:primal} we need to
  minimize,

  $$\mathcal{L}(w_{2|1}) = \sum_{i \in A_2} w_{1i} G(w_{2i|1}) q_i + 
  \lambda \left(\sum_{i \in A_2} w_{1i} w_{2i|1} z_i q_i - \sum_{i \in A_1} w_{1i}
  z_i q_i\right).$$

  The first order conditions show that

  $$\frac{\partial \mathcal{L}}{\partial w_{2i|1}}: g(w_{2i|1}) w_{1i}q_i +
    \lambda w_{1i} z_i q_i = 0.$$

  Hence, $\hat w_{2i}(\lambda) = g^{-1}(\lambda z_i)$ and $\hat \lambda$ is
  determined by the calibration condition. When the sample size gets large, we
  have $\hat w_{2i|1}(\hat \lambda) \to d_{2i|1}$ which means that $\hat \lambda
  \to \lambda^*$ where $\lambda^* = (\bf 0, 1)$. Then using the linearization
  technique of \cite{randles1982asymptotic}, we can construct a regression
  estimator, 

  $$\hat Y_\ell(\hat \lambda, \phi)  = \hat Y_{DCE}(\hat \lambda) + 
  \left(\sum_{i \in A_1} w_{1i} z_i q_i - \sum_{i \in A_2} w_{1i} \hat w_{2i|1}(\hat
  \lambda) z_i q_i\right)\phi.$$

  Notice that $\hat Y_\ell(\hat \lambda, \phi) = \hat Y_{DCE}(\hat \lambda)$ for
  all $\phi \in \mathbb{R}$. We choose $\phi^*$ such that

  $$E\left[\frac{\partial}{\partial \lambda} \hat Y_\ell(\lambda^*, \phi^*)\right]=0.$$

  Using the fact that $g^{-1}(\lambda^* z_i) = g^{-1}(g(d_{2i|1})) = d_{2i|1}$
  and $(g^{-1})'(x) = 1 / g'(g^{-1}(x))$, we have

  \begin{align*}
    \phi^*
    &= E\left[\sum_{i \in A_2} \frac{w_{1i}z_i z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
    E\left[\sum_{i \in A_2} \frac{w_{1i}z_i y_i}{g'(d_{2i|1})}\right]\\
    &= \left[\sum_{i \in U} \frac{\pi_{2i|1}z_i z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
    \left[\sum_{i \in U} \frac{\pi_{2i|1} z_i y_i}{g'(d_{2i|1})}\right]\\
  \end{align*}

  Thus, the linearization estimator is

  $$\hat Y_\ell(\lambda^*, \phi^*) = \sum_{i \in A_1} w_{1i} q_i z_i \phi^* +
  \sum_{i \in A_2} w_{1i} d_{2i|1} (y_i - q_i z_i \phi^*).$$

  By construction using a Taylor expansion yields,

  \begin{align*}
    \hat Y_{DCE}(\hat \lambda) 
    &= \hat Y_\ell(\lambda^*, \phi^*) + 
    E\left[\frac{\partial}{\partial \lambda}\hat Y_\ell(\lambda^*,
    \phi^*)\right](\hat \lambda - \lambda^*) + \frac{1}{2}
    E\left[\frac{\partial}{\partial \lambda^2} \hat Y_{DCE}(\lambda^*)\right] (\hat
    \lambda - \lambda^*)^2\\
    &= \hat Y_\ell(\lambda^*, \phi^*) + O(N)o_p(n_2^{-1}).
  \end{align*}

  The final equality comes from the fact that 
  $E\left[\frac{\partial}{\partial \lambda}\hat Y_\ell(\lambda^*, \phi^*)\right]
  = 0$, $\frac{\partial}{\partial \lambda^2} \hat w_{2i|1}(\lambda^*)$ is
  bounded and $|\hat \lambda - \lambda^*| = O_p(n_2^{-1/2})$, which proves our
  result.
\end{proof}

\subsection{Simulation Studies}

We run a simulation testing the proposed method. In this approach we have the
following simulation setup:

$$
\begin{aligned}
X_{1i} &\stackrel{ind}{\sim} N(2, 1) \\
X_{2i} &\stackrel{ind}{\sim} Unif(0, 4) \\
X_{3i} &\stackrel{ind}{\sim} N(0, 1) \\
X_{4i} &\stackrel{ind}{\sim} Unif(0.1, 0.9) \\
\varepsilon_i &\stackrel{ind}{\sim} N(0, 1) \\
Y_{i} &= 3 X_{1i} + 2 X_{2i} + \varepsilon_i \\
\pi_{1i} &= \Phi_3(-x_{3i} - 2) \\
\pi_{2i|1} &= x_{4i}.
\end{aligned}
$$

where $\Phi_3$ is the CDF of a t-distribution with 3 degrees of freedom.

\textcolor{red}{The simulation setup is somewhat strange. The first order inclusion probability for the second-phase sampling, $\pi_{2i \mid 1}$, is a function of $x_{4i}$ which is completely independent of $(x_{1i}, x_{2i}, y_i)$. In this case, your second-phase sampling is essentially equivalent to the simple random sampling. }
This is a two-phase extension of the setup in \cite{kwon2024debiased}. We
consider a finite population of size $N = 10,000$ with both the Phase 1 and
Phase 2 sampling occuring under Poisson (Bernoulli) sampling. This yields a
Phase 1 sample
size of $E[n_1] \approx 1100$ and a Phase 2 sample size of
$E[n_2] \approx 550$. In the Phase 1 sample, we observe 
$(X_1, X_2)$ while in the Phase 2 sample we observe $(X_1, X_2, Y)$. This
simulation does not deal with model misspecification, and we compare the
proposed method for the parameter $\bar Y_N$ with four approaches:

\begin{itemize}
  \item[1.] $\pi^*$-estimator: $\hat Y_{\pi^*} = N^{-1} \sum_{i \in A_2}
    \frac{y_i}{\pi_{1i} \pi_{2i|1}},$
  \item[2.] Two Phase Regression estimator (TP-Reg): 
    $\hat Y_{reg} = \sum_{i \in A_1} \frac{\bf x_i' \hat \beta}{\pi_{1i}} + 
    \sum_{i \in A_2} \frac{1}{\pi_{1i}\pi_{2i|1}}(y_i - \bf x_i' \hat \beta)$ 
    where $\hat \beta = 
    \left(\sum_{i \in A_2} \bf x_i \bf x_i'\right)^{-1} \sum_{i \in A_2} \bf x_i y_i$
    and $\bf x_i = (x_{1i}, x_{2i})^T$,
    \textcolor{red}{Dr. Kim, should I modify the simulation so that the
    regression estimator also includes $g(\pi_{2i|1}^{-1})$ as a covariate?}
  \item[3.] Debiased Calibration with Population Constraints (DC-Pop): This is
    the method from \cite{kwon2024debiased} with the true population level
    constraints, and 
  \item[4.] Debiased Calibration with Estimated Population Constraints (DC-Est):
    This is the proposed method with the Phase 1 sample being used to estimate
    the population level constraints.
\end{itemize}

In addition to estimating the mean parameter $\bar Y_N$, we also construct
variance estimates $\hat V(\hat Y)$ for each estimate $\hat Y$. For each
approach we have the following variance estimate\footnote{These variance
estimates use the fact that we have Poisson sampling for both phases in the
simulation.}:

\begin{figure}[ht!]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Estimator & Variance & Notes \\
    \midrule
    $\pi^*$ & $N^{-2} \sum_{i \in A_2} \left(\pi_{2i|1}^{-2} - \pi_{2i|1}^{-1}\right)
    y_i^2 $ & \\
    TP-Reg  & 
      {\scriptsize $N^{-2}\left(\sum\limits_{i \in A_1} \left(\pi_{1i}^{-2} -
          \pi_{1i}^{-1}\right) 
      \eta_i^2 
      + \sum\limits_{i \in A_2} \frac{1}{\pi_{1i} \pi_{2i|1}}(\pi_{2i|1}^{-1}
  - 1) (y_i - \bf x_i' \hat \beta)^2 \right)$}
    & 
      {\scriptsize$\eta_i = \bf x_i \hat \beta + \frac{\delta_{2i}}{\pi_{2i|1}}(y_i - 
      \bf x_i \hat \beta)$} \\
      %$\hat \beta = \left(\sum_{i \in A_2} \bf x_i \bf x_i'\right)^{-1} \sum_{i \in A_2} \bf x_i y_i$ \\
      DC-Pop  & $(Y - \bf z^T \hat \gamma)^T \Pi (Y - \bf z^T \hat \gamma)$ & 
      {\scriptsize$\Pi
      = \text{diag}(1 - (\pi_{1}\pi_{2|1})^{-1}) \cdot \frac{\hat w^2}{N^2}$} \\
      DC-Est  & $(Y - \bf z^T \hat \gamma)^T \Pi (Y - \bf z^T \hat \gamma) +
      \hat \gamma_{[1:3]}^T V(\bf x) \hat \gamma_{[1:3]}/N^2$ & \\
    \bottomrule
  \end{tabular}
  \caption{This table gives the formulas for each variance estimator used in
  this simulation.}
  \label{tab:varforms}
\end{figure}

We run this simulation $1000$ times for each of these methods and compute the
Bias ($E[\hat Y] - \bar Y_N$), the RMSE ($\sqrt{\Var(\hat Y - \bar Y_N)}$), a 95\%
empirical confidence interval ($\sum_{b = 1}^{1000} |\hat Y^{(b)} - \bar Y_N| \leq 
\Phi(0.975)\sqrt{\hat V(\hat Y^{(b)})^{(b)}}$), and a T-test that assesses the
unbiasedness of each estimator. The results are in Figure~\ref{fig:tpdc-mean}.

\begin{figure}[ht!]
  \centering
\input{tables/tpdcsim_mean.tex}
% This table was generated from /src/explore/20240411-tpdcsim.qmd
\caption{This table shows the results of the simulation study. It displays the
Bias, RMSE, empirical 95\% confidence interval, and a t-statistic assessing the
unbiasedness of each estimator for the estimators: $\pi^*$, TP-Reg, DC-Pop, and
DC-Est.}
\label{fig:tpdc-mean}
\end{figure}

\section{Topic 2: Non-nested Two-Phase Sampling}

\subsection{Background}

Now we consider the sampling mechanism known as non-nested two phase sampling 
(\cite{kim2024statistics}). In the last section, we considered two phase sampling
in which the Phase 2 sample was a subset of the Phase 1 sample. With non-nested
two phase sampling the Phase 2 sample is independent of the Phase 1 sample. It
is a separate independent sample of the same population. Like traditional two
phase sampling, we consider the Phase 1 sample, $A_1$, to consist of
observations of $(X_i)_{i = 1}^{n_1}$ and the Phase 2 sample, $A_2$, to consist
of observations of $(X_i, Y_i)_{i = 1}^{n_2}$. 

Whereas the classical two phase estimator uses a single Horvitz-Thompson
estimator of the Phase 1 sample to construct estimates for calibration totals,
in the non-nested two phase sample we have two independent Horvitz-Thompson
estimators of the total of $X$,

$$\hat X_1 = \sum_{i \in A_1}^{n_1} \frac{x_i}{\pi_{1i}} \text{ and } 
\hat X_2 = \sum_{i \in A_2}^{n_2} \frac{x_i}{\pi_{2i}}$$

where $\pi_{1i}$ is the probability of $i \in A_1$ and $\pi_{2i} = \Pr(i \in
A_2)$. \textcolor{red}{I don't know if it makes sense to change the Phase 2
selection probability as $\pi_{2i|1}$. Should I keep it at $\pi_{2i}$? But if
I do, how is the two phase setup useful?} We can combine these estimates to get
$\hat X_c = W \hat X_1 + (1 - W)\hat X_2$ for some matrix $W$ (see
\cite{merkouris2004combining} for the optimal choice of $W$), and can then define
a regression estimator as

$$
\hat Y_{NN, reg} = \hat Y_2 + (\hat X_c - \hat X_2)^T \hat \beta_q = 
\hat Y_2 + (\hat X_1 - \hat X_2)^T W\hat \beta_q 
$$

where 

$$\hat \beta_q = \left(\sum_{i \in A_2} \frac{x_i x_i^T}{q_i}\right)^{-1}
\sum_{i \in A_2} \frac{x_i y_i}{q_i} \text{ and }\hat Y_2 = \sum_{i \in A_2}
\frac{y_i}{\pi_{2i}}.$$

Since the samples $A_1$ and $A_2$ are independent, 

$$V(\hat Y_{NN, reg}) = V\left(\sum_{i \in A_2} \frac{1}{\pi_{2i}}(y_i -
x_i^TW\beta^*_q)\right) + (\beta^*_q)^T W^T V(\hat X_1) W \beta_q^*$$

where $\beta_q^*$ is the probability limit of $\hat \beta_q$. Like the two phase
sample this regression estimator can be viewed as the solution to the following
calibration equation where $d_{2i} = \pi_{2i}^{-1}$,

\begin{equation}\label{eq:nncal}
\hat w = \argmin_w Q(w) = \sum_{i \in A_2} (w_{2i} - d_{2i})^2 q_i \text{ such
that } \sum_{i \in A_2} w_{2i} x_i = \hat X_c
\end{equation}

and $\hat Y_{NN, reg} = \sum_{i \in A_2} \hat w_{2i} y_i$ where $\hat
w_{2i}$ is the solution to Equation~\ref{eq:nncal}.

We extend the debiased calibration estimator of \cite{kwon2024debiased} to the
non-nested two phase sampling case where we use a combined estimate $\hat X_c$
as the calibration totals instead of using the true totals from the finite
population.

\subsection{Methodology}

The methodology for the non-nested two phase sample is very similar to the setup
described as part of Topic 1. Given a strictly convex differentiable function,
$G: \mathcal{V} \to \mathbb{R}$, the goal is to solve

\begin{equation}\label{eq:nnopt}
\hat w_2 = \argmin_w \sum_{i \in A_2} G(w_{2i}) q_i \text{ such that } 
\sum_{i \in A_2} w_{2i} x_i = \hat X_{GLS} \text{ and } 
\sum_{i \in A_2} w_{2i} g(d_{2i}) q_i = \sum_{i \in U} g(d_{2i}) q_i
\end{equation}

for $g(x) = G'(x)$ and a known choice of $q_i \in \mathbb{R}$. The difference
between solving Equation~\ref{eq:nnopt} and Equation~\ref{eq:primal} is that the
estimator $\hat X_{GLS}$ is estimated using GLS from other sample(s). Before using
$\hat X_{GLS}$ in the debiased calibration estimator, we need to estimate it
from the non-nested samples.

We can get multiple estimates of $\hat X$, 

$$\hat X_1 = \sum_{i \in A_1} \frac{x_i}{\pi_{1i}} \text{ and }
\hat X_2 = \sum_{i \in A_2} \frac{x_i}{\pi_{2i}}.$$

Let $V_1$ and $V_2$ be known variance matrices for $\hat X_1$ and $\hat X_2$,
then the optimal combined GLS estimates is 

$$\hat X_c = \frac{V_1^{-1} \hat X_1 + V_2^{-1} \hat X_2}{V_1^{-1} + V_2^{-1}}.$$

We can constuct a non-nested two phase estimator $\hat Y_{NNE}$ for $Y_N$ where
$\hat Y_{NNE} = \sum_{i \in A_2} \hat w_{2i} y_i$ and $\hat w_{2i}$ solves
Equation~\ref{eq:nnopt}.

\subsection{Theoretical Results}

\begin{theorem}[Design Consistency]\label{thm:dc2}
  Under some regularity conditions, $\hat Y_{NNE} = \hat Y_{\ell, NNE}(\lambda^*,
  \phi^*) + O_p(Nn_2^{-1})$ where

  $$\hat Y_{\ell, NNE}(\lambda^*, \phi^*) = \sum_{i \in A_2} \hat w_{2i}(
  \lambda^*) + \left(T - \sum_{i \in A_2} \hat w_{2i}(\lambda^*) z_i q_i\right)
  \phi^*$$

  and 
  $$\phi^* =
  \left(\sum_{i \in U} \frac{\pi_{2i} q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    x_i^2 / q_i & x_i g(d_{2i}) / q_i \\
    x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in U} \frac{\pi_{2i} y_i}{g'(d_{2i})} 
  \begin{bmatrix} x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$

\end{theorem}

\begin{proof}
  The proof of this result is very similar to the proof the Theorem~\ref{thm:dc1}.
  The biggest difference is that the total for $X$ is estimated from both samples
  using $\hat X_c$ instead of $\hat X_{HT}$ from the Phase 1 sample.

  Since $\hat Y_{NNE} = \sum_{i \in A_2} \hat w_{2i}(\hat \lambda)y_i$ where
  $\hat \lambda$ solves

  \begin{equation}
    \sum_{i \in A_2} \hat w_{2i}(\lambda) q_i
    \underbrace{
    \begin{bmatrix} 
      x_i / q_i \\ g(d_i)
  \end{bmatrix}}_{z_i} =
  \underbrace{\begin{bmatrix} 
      \hat X_c \\
      \sum_{i \in U} g(d_i) q_i
  \end{bmatrix}}_{T}
  \end{equation}

  we have 

  $$\hat Y_{\ell, NNE} (\hat \lambda, \phi) = \sum_{i \in A_2} \hat w_{2i}(\hat
  \lambda) + \left(T - \sum_{i \in A_2} \hat w_{2i}(\hat \lambda) z_i q_i\right)
  \phi.$$

  If we choose $\phi^*$ such that $E\left[\frac{\partial}{\partial \lambda} 
    \hat Y_{\ell, NNE}(\lambda^*, \phi^*)\right] = 0$, then

  $$\phi^* =
  \begin{bmatrix}
    \phi^*_1 \\ \phi^*_2
  \end{bmatrix} = 
  \left(\sum_{i \in U} \frac{\pi_{2i} q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    x_i^2 / q_i & x_i g(d_{2i}) / q_i \\
    x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in U} \frac{\pi_{2i} y_i}{g'(d_{2i})} 
  \begin{bmatrix} x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$

  Hence, by a Taylor expansion around $\hat \lambda$,
  
  $$\hat Y_{NNE}(\hat \lambda) = \hat Y_{\ell, NNE}(\lambda^*, \phi^*) +
  O_p(Nn_2^{-1}).$$

\end{proof}

\begin{theorem}[Variance Estimation]
  The variance of $\hat Y_{NNE}$ is 

  \begin{align*}
    \Var(\hat Y_{NNE}(\hat \lambda)
    &= (\phi_1^*)^T \Var(\hat X_c) \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} (y_i - z_i \phi^* q_i)(y_j - z_j \phi^* q_j)\\
    &\qquad + (1 - W)\phi_1^* \sum_{i \in U} \sum_{j \in U} \Delta_{2ij} \frac{x_i}{\pi_{2i}}
    \frac{(y_j - z_j \phi^*_1 q_j)}{\pi_{2j}}\\
  \end{align*}

  We can estimate the variance using

  \begin{align*}
    \hat V_{NNE} 
    &= (\hat \phi_1)^T \Var(\hat X_c) \hat \phi_1 + 
    \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}\pi_{2i}\pi_{2j}} 
    (y_i - z_i \hat \phi q_i)(y_j - z_j \hat \phi q_j) \\
    &\qquad + (1 - W)\hat \phi_1 \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}}
    \frac{x_i}{\pi_{2i}} \frac{(y_j - z_j \hat \phi_1 q_j)}{\pi_{2j}}
  \end{align*}

  where 
  
  $$\hat \phi =
  \begin{bmatrix}
    \hat \phi_1 \\ \hat \phi_2
  \end{bmatrix} = 
  \left(\sum_{i \in A_2} \frac{q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    x_i^2 / q_i & x_i g(d_{2i}) / q_i \\
    x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in A_2} \frac{y_i}{g'(d_{2i})} 
  \begin{bmatrix} x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$
\end{theorem}

\begin{proof}
  From Theorem~\ref{thm:dc2}, we know that $\hat Y_{NNE}(\hat \lambda) = 
  \hat Y_{\ell, NNE}(\lambda^*, \phi^*) + O_p(Nn_2^{-1})$. Hence, the variance
  of $\hat Y_{NNE}(\hat \lambda)$ is 

  \begin{align*}
    \Var(\hat Y_{NNE}(\hat \lambda)) 
    &= \Var(\hat Y_{\ell, NNE}(\lambda^*, \phi^*) + O_p(Nn_2^{-1})) \\ 
    &= \Var\left(\sum_{i \in A_2} \hat w_{2i}(\lambda^*) y_i + \left(T - \sum_{i
    \in A_2} \hat w_{2i}(\lambda^*) z_i q_i\right)\phi^*\right)\\
    &= (\phi_1^*)^T \Var(\hat X_c) \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} (y_i - z_i \phi^* q_i)(y_j - z_j \phi^* q_j)\\
    &\qquad+ 2 \Cov\left(\hat X_c \phi_1^*, \sum_{i \in A_2} \frac{(y_i - z_i \phi^*
    q_i)}{\pi_{2i}}\right) \\
    &= (\phi_1^*)^T \Var(\hat X_c) \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} (y_i - z_i \phi^* q_i)(y_j - z_j \phi^* q_j)\\
    &\qquad + (1 - W)\phi_1^* \sum_{i \in U} \sum_{j \in U} \Delta_{2ij} \frac{x_i}{\pi_{2i}}
    \frac{(y_j - z_j \phi^*_1 q_j)}{\pi_{2j}}\\
  \end{align*}

  where the last equality comes from the fact that $\hat X_c = W\hat X_1 + (1 -
  W) \hat X_2$. To have an unbiased estimator of the variance we can use:

  \begin{align*}
    \hat V_{NNE} 
    &= (\hat \phi_1)^T \Var(\hat X_c) \hat \phi_1 + 
    \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}\pi_{2i}\pi_{2j}} 
    (y_i - z_i \hat \phi q_i)(y_j - z_j \hat \phi q_j) \\
    &\qquad + (1 - W)\hat \phi_1 \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}}
    \frac{x_i}{\pi_{2i}} \frac{(y_j - z_j \hat \phi_1 q_j)}{\pi_{2j}}
  \end{align*}

  where 
  
  $$\hat \phi =
  \begin{bmatrix}
    \hat \phi_1 \\ \hat \phi_2
  \end{bmatrix} = 
  \left(\sum_{i \in A_2} \frac{q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    x_i^2 / q_i & x_i g(d_{2i}) / q_i \\
    x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in A_2} \frac{y_i}{g'(d_{2i})} 
  \begin{bmatrix} x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$

\end{proof}

\subsection{Simulation Studies}

\section{Topic 3: Multi-source Two-Phase Sampling}


\bibliographystyle{chicago}
\bibliography{references}


\end{document}

