\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
%\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}
\usepackage{float}

\usepackage{amsmath, amsthm, mathtools,commath}
\usepackage{graphics, color}
\usepackage{latexsym}
\usepackage{amssymb, amsfonts, bm}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{makeidx}
\usepackage{fullpage}
\usepackage{booktabs, arydshln}
\usepackage{comment} 
%\makeindex

\hbadness=10000 \tolerance=10000 \hyphenation{en-vi-ron-ment
in-ven-tory e-num-er-ate char-ac-ter-is-tic}

\usepackage[round]{natbib}
%\bibliographystyle{apalike2}
% \bibliographystyle{jmr}


\newcommand{\biblist}{\begin{list}{}
{\listparindent 0.0cm \leftmargin 0.50cm \itemindent -0.50 cm
\labelwidth 0 cm \labelsep 0.50 cm
\usecounter{list}}\clubpenalty4000\widowpenalty4000}
\newcommand{\ebiblist}{\end{list}}

\newcounter{list}

%\usepackage{setspace}

%\usepackage{hangpar}
\newcommand{\lbl}[1]{\label{#1}{\ensuremath{^{\fbox{\tiny\upshape#1}}}}}
% remove % from next line for final copy
\renewcommand{\lbl}[1]{\label{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{result}{Result}

\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\bf}[1]{\mathbf{#1}}


\begin{document}

\title{Debiased Calibration for Generalized Two-Phase Sampling}
\author{Caleb Leedy}
\maketitle 

\baselineskip .3in

\section{Introduction}

Combining information from several sources is an important practical problem. (CITEME)
We want to incorporate information from external data sources to reduce the bias
in our estimates or improve the estimator's efficiency. For many problems, the
additional information consists of summary statistics with standard errors. The
goal of this project is to incorporate external information with existing data 
to create more efficient estimators using calibration weighting.

To model this scenaro, we formulate the problem as a generalized two-phase
sample where the first phase sample consists of data from multiple sources. The
second phase sample contains our existing data. To motivate this setup, we
consider the following approach: first, we consider the classical two-phase
sampling setup where the second phase sample is a subset of the first phase
sample; then, we extend this setup to consider non-nested two-phase samples;
and finally, we consider the more general approach of having multiple sources.

%\begin{itemize}
%\item To achieve the goal, we first consider  the  classical two-phase sampling
%setup where the second-phase sample is a subset of the first-phase sample.
%  After that, we extend the setup to more general cases such as non-nested
%  two-phase sampling or multiple independent surveys with some common
%  measurements. 
%\item The proposed method can be called two-step calibration. In the
%first-step, the best linear unbiased estimators of the auxiliary variable
%totals are computed. In the second-step, the final calibration weights are
%constructed to match (benchmark) with the best estimators computed from Step 1. 
%\end{itemize}

\section{Topic 1: Classical Two-Phase Sampling}

\subsection{Background}

Consider a finite population of size $N$ containing elements $(\bf X_i, Y_i)$ where
an initial (Phase 1) sample of size $n_1$ is selected and $\bf X_i$ is observed. Then
from the Phase 1 sample of elements, a (Phase 2) sample of size $n_2 < n_1$ is
selected and $Y_i$ is observed. This is two-phase sampling (See 
\cite{fuller2009sampling}, \cite{kim2024statistics} for general references.) The
goal of two-phase sampling is to construct an estimator of $\bar Y_N$ 
that uses both the observed information in the Phase 2 sample and also the extra
auxiliary information from $\bf X$ in the Phase 1 sample.
The challenge is doing this efficiently.

An easy-to-implement unbiased estimator in the spirit of a Horvitz-Thompson (HT)
estimator (\cite{horvitz1952generalization}, \cite{narain1951sampling}) is the
$\pi^*$-estimator. Let $\pi_i^{(2)}$ be the response probability of element $i$
being observed in the Phase 2 sample. Then, allowing the elements in the Phase 1
sample to be represented by $A_1$ and the elements in the Phase 2 sample to be
denoted as $A_2$,
%\begin{align*}
%  \pi_i &= \sum_{A_2: i \in A_2} \Pr(A_2) \\ 
%        &= \sum_{A_1: A_2 \subseteq A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid
%        A_1) \Pr(A_1) \\
%        &= \sum_{A_1: i \in A_1} \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1) \Pr(A_1).
%\end{align*}
%\textcolor{red}{(I am not sure whether this part is necessary.) }
if we define $\pi_{2i | 1} = \sum_{A_2: i \in A_2} \Pr(A_2 \mid A_1)$ and
$\pi_{1i} = \sum_{A_1: i \in A_1} \Pr(A_1)$ then,

$$ \pi_i^{(2)}(A_1) = \pi_{2i | 1} \pi_{1i}.$$

This means that we can define the $\pi^*$-estimator as the following design
unbiased estimator:

$$ \hat Y_{\pi^*} = \sum_{i \in A_2} \frac{y_i}{\pi_{2i | 1} \pi_{1i}}.$$

While unbiased (see \cite{kim2024statistics}), the $\pi^*$-estimator does not
account for the additional information contained in the auxiliary Phase 1
variable $\bf X$. The two-phase regression estimator $\hat Y_{reg, tp}$ does
incorporate information for $\bf X$ by using the estimate $\hat{\bf X_1}$ from
the Phase 1 sample. This is how we can leverage the external information 
$\hat{\bf{X_1}}$ to improve the initial $\pi^*$-estimator in the second phase
sample. The two-phase regression estimator has the form,

$$ \hat Y_{reg, tp} 
= \sum_{i \in A_1} \frac{1}{\pi_{1i}} \bf x_i \hat{\bm \beta_q} + 
\sum_{i \in A_2} \frac{1}{\pi_{1i}\pi_{2i|1}} (y_i - \bf x_i \hat{\bm \beta_q})$$

where for $q_i = q(\bf x_i)$ and is a function of $\bf x_i$,
$$
\hat{\bm \beta_q} = \left(\sum_{i \in A_2} 
  \frac{\bf x_i \bf x_i'}{\pi_{1i} q_i}\right)^{-1} 
\sum_{i \in A_2} \frac{\bf x_i y_i}{\pi_{1i} q_i}.$$ 

The regression estimator is the minimum variance design consistent linear
estimator which is easily shown to be the case because $\hat Y_{reg, tp} =
\sum_{i \in A_2} \hat w_{2i} y_i / \pi_{1i}$ where 

$$\hat w_{2i} = \argmin_{w} \sum_{i \in A_2} (w_{2i} - \pi_{2i|1}^{-1})^2 q_i
\text{ such that } \sum_{i \in A_2} w_{2i} \bf x_i / \pi_{1i} = \sum_{i \in A_1}
\bf x_i / \pi_{1i}.$$

This means that $\hat Y_{reg, tp}$ is also a calibration estimator. The idea
that regression estimation is a form of calibration was noted by
\cite{deville1992calibration} and extended by them to consider loss functions
other than just squared loss. Their generalized loss function minimizes
$\sum_i G(w_i, d_i)q_i$ for weights $w_i$ and design-weights $d_i$ where
$G(\cdot)$ is a non-negative, strictly convex function with respect to $w$,
defined on an interval containing $d_i$, with $g(w_i, d_i) = \partial G /
\partial w$ continuous.\footnote{The \cite{deville1992calibration} paper
considers regression estimators for a single phase setup, which we apply to our
two-phase example.} This
generalization includes empirical likelihood estimation, and maximum entropy
estimation among others. The variance estimation is based on a linearization
that shows that minimizing the generalized loss function subject to the
calibration constraints is asymptotically equivalent to a regression estimator.

Furthermore, the regression estimator
has a nice feature that its two terms can be thought about as minimizing the
variance and bias correction,

$$ \hat Y_{reg, tp} 
= \underbrace{\sum_{i \in A_1} \frac{\bf x_i \hat{\bm \beta_q}}{\pi_{1i}}}_{
  \text{ Minimizing the variance}} + \underbrace{\sum_{i \in A_2}
\frac{1}{\pi_{1i}\pi_{2i|1}} (y_i - \bf x_i \hat{\bm \beta_q})}_{
\text{Bias correction}}.$$

The \cite{deville1992calibration} method incorporates the design weights into
the loss function, which is the part minimizing the variance. We would rather
separate have bias calibration separate from the minimizing the variance so that
we can control each in isolation. In
\cite{kwon2024debiased}, the authors show that for a generalized entropy
function $G(w)$, including a term of $g(\pi_{2i|1}^{-1})$ into the calibration
for $g = \partial G / \partial w$ not only creates a design consistent
estimator, but it also has better efficiency than the generalized regression
estimators of \cite{deville1992calibration}.

The method of \cite{kwon2024debiased} requires known finite population 
calibration levels. It does not handle the
two-phase setup where we need to estimate the finite population total of $\bf x$
from the Phase 1 sample. In the rest of the section, we extend this method to 
two phase sampling so that we have a valid 
estimator when including estimated Phase 1 weights with appropriate variance
estimation.

\subsection{Methodology}

We follow the approach of \cite{kwon2024debiased} for the debiased calibration
method. We consider maximizing the generalized entropy \cite{gneiting2007strictly},

\begin{equation}\label{eq:primalloss}
  H(w) = - \sum_{i \in A_2} \frac{1}{\pi_{1i}} G(w_{2i}) q_i
\end{equation}
where $G: \mathcal{V} \to \R$ is strictly convex, differentiable function
subject to the constraints:

\begin{equation}\label{eq:calconst1}
  \sum_{i \in A_2} \frac{\bf x_i w_{2i}q_i}{\pi_{1i}} = 
\sum_{i \in A_1} \frac{\bf x_iq_i}{\pi_{1i}}
\end{equation}

and 

\begin{equation}\label{eq:calconst2}
  \sum_{i \in A_2} \frac{g(\pi_{2i|1}^{-1})w_{2i}q_i}{\pi_{1i}} = 
  \sum_{i \in A_1} \frac{g(\pi_{2i|1}^{-1})q_i}{\pi_{1i}}, 
\end{equation}
where $g(w) = \partial G / \partial w$. 

The first constraint is the existing calibration constraint and the second
ensures that design consistency is achieved. 
The original method of \cite{kwon2024debiased} only considered having finite
population quantities on the right hand side of \eqref{eq:calconst1}.

Writing $w_{1i} = \pi_{1i}^{-1}$, the the goal is to solve

\begin{equation}\label{eq:primal}
  \argmin_{w_{2|1}} \sum_{i \in A_2} w_{1i} G(w_{2i}) q_i 
  \text{ such that}
  \sum_{i \in A_2} w_{1i} w_{2i|1} \bf z_i q_i = \sum_{i \in A_1} w_{1i} \bf z_i q_i
\end{equation}

where  $\bf z_i = (\bf x_i / q_i, g(\pi_{2i|1}^{-1}))$. 
Let $\hat w_{2i|1}$ be the solution to Equation~\eqref{eq:primal}. The resulting 
estimator of $Y_N$ is $\hat Y_{DCE} = \sum_{i \in A_2} w_{1i} \hat w_{2i|1} y_i$.
To solve this problem we can use the method of Lagrange multipliers. We need to
minimize the Lagrangian function

\begin{equation}\label{eq:legragedc1}
  L(w_{2i|1}, \bm \lambda) = \sum_{i \in A_2} w_{1i} G(w_{2i|1}) q_i 
  + \bm \lambda \left( \sum_{i \in A_1} w_{1i} \bf z_i q_i -
    \sum_{i \in A_2} w_{1i} w_{2i|1} \bf z_i q_i\right).
\end{equation}

Differntiating with respect to $w_{2i|1}$ and setting this expression equal to
zero, yields the fact that $\hat w_{2i|1}$ satisfies 

$$ \hat w_{2i|1}(\hat{\bm \lambda}) = g^{-1}(\hat{\bm \lambda}^T \bf z_i) $$

where $\hat{\bm \lambda}$ is the solution to

\begin{equation}\label{eq:lamdc1}
  \left( \sum_{i \in A_1} w_{1i} \bf z_i q_i -
  \sum_{i \in A_2} w_{1i} w_{2i|1}(\hat{\bm \lambda}) \bf z_i q_i\right) = 0.
\end{equation}

\subsection{Theoretical Results}

% \textcolor{red}{In Theorem 1, you used $\hat{\lambda}$ without defining it.  }
\begin{theorem}[Design Consistency]\label{thm:dc1}
  Let $\bm \lambda^*$ be the probability limit of $\hat{\bm \lambda}$.
  Under some regularity conditions,

  $$\hat Y_{DCE} = \hat Y_\ell(\bm \lambda^*, \bm \phi^*) + O_p(N / n_2)$$

  where

  $$\hat Y_{\ell}(\bm \lambda, \bm \phi^*) = \hat Y_{DCE}(\hat{\bm \lambda}) + 
  \left(\sum_{i \in A_1} w_{1i} \bf z_i q_i - \sum_{i \in A_2} w_{1i} \hat w_{2i|1}(
  \bm \lambda) z_i q_i\right)\bm \phi^*$$

  and

  $$\bm \phi^* = 
  \left[\sum_{i \in U} \frac{\pi_{2i|1}\bf z_i \bf z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
  \sum_{i \in U} \frac{\pi_{2i|1}\bf z_i y_i}{g'(d_{2i|1})}.$$

   \begin{eqnarray*}
   \hat Y_{\ell}(\bm \lambda^*, \bm \phi^*) &=&   \hat{Y}_{\pi^*} + 
  \left(\sum_{i \in A_1} w_{1i} \bf x_i  -  \sum_{i \in A_2} w_{1i} \pi_{2i \mid
  1}^{-1} \bf x_i  \right)^T \bm \phi_1^* + \left(\sum_{i \in A_1} w_{1i} g_i  -
  \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1}g_i  \right)^T \phi_2^*  
  \end{eqnarray*} 
  and  $$\begin{pmatrix}
  \bm \phi_1^* \\
  \phi_2^* 
  \end{pmatrix}
  = \left[ \sum_{i \in U} \frac{\pi_{2i \mid 1} }{ g'(d_{2i|1}) q_i} 
  \begin{pmatrix}
  \bx_i \bx_i^T &   \bx_i g_i   \\
  g_i  \bx_i^T   & g_i^2     \end{pmatrix} \right]^{-1}
  \sum_{i \in U} \frac{\pi_{2i|1}}{ g'(d_{2i | 1}) q_i} \begin{pmatrix}
    \bx_i \\ g_i 
  \end{pmatrix}y_i $$
with $g_i = g( \pi_{2i |1}^{-1}) q_i$.
\end{theorem}


\begin{proof}
  In this proof, we derive the solution to Equation~\ref{eq:primal} and show
  that it is asymptotically equivalent to a regression estimator. Using the
  method of Lagrange multipliers, to solve Equation~\eqref{eq:primal} we need to
  minimize the Lagrangian in Equation~\eqref{eq:legragedc1}. 
  The first order conditions show that

  $$\frac{\partial \mathcal{L}}{\partial w_{2i|1}}: g(w_{2i|1}) w_{1i}q_i -
  w_{1i} \bm \lambda \bf z_i q_i = 0.$$

  Hence, $\hat w_{2i}(\bm \lambda) = g^{-1}(\bm \lambda^T \bf z_i)$ and
  $\hat{\bm \lambda}$ is determined by Equation~\eqref{eq:lamdc1}. When the
  sample size gets large, we have $\hat w_{2i|1}(\hat{\bm \lambda}) \to
  d_{2i|1}$ which means that $\hat{\bm \lambda} \to \bm \lambda^*$ where $\bm
  \lambda^* = (\bf 0, 1)$. Then using the linearization technique of
  \cite{randles1982asymptotic}, we can construct a regression estimator, 

  $$\hat Y_\ell(\hat{\bm \lambda}, \bm \phi)  = \hat Y_{DCE}(\hat{\bm \lambda}) + 
  \left(\sum_{i \in A_1} w_{1i} \bm z_i q_i - \sum_{i \in A_2} w_{1i} 
  \hat w_{2i|1}(\hat{\bm \lambda}) \bf z_i q_i\right)\bm \phi.$$

  Notice that $\hat Y_\ell(\hat{\bm \lambda}, \bm \phi) = \hat Y_{DCE}(\hat{\bm
  \lambda})$ for all $\bm \phi$. We choose $\bm \phi^*$ such that

  $$E\left[\frac{\partial}{\partial \bm \lambda} \hat Y_\ell(\bm \lambda^*,
  \bm \phi^*)\right]=0.$$

  Using the fact that $g^{-1}(\bm \lambda^* \bf z_i) = g^{-1}(g(d_{2i|1})) = d_{2i|1}$
  and $(g^{-1})'(x) = 1 / g'(g^{-1}(x))$, we have

  \begin{align*}
    \bm \phi^*
    &= E\left[\sum_{i \in A_2} \frac{w_{1i}\bf z_i \bf z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
    E\left[\sum_{i \in A_2} \frac{w_{1i}\bf z_i y_i}{g'(d_{2i|1})}\right]\\
    &= \left[\sum_{i \in U} \frac{\pi_{2i|1} \bf z_i \bf z_i^T q_i}{g'(d_{2i|1})}\right]^{-1}
    \left[\sum_{i \in U} \frac{\pi_{2i|1} \bf z_i y_i}{g'(d_{2i|1})}\right]\\
  \end{align*}

  Thus, the linearization estimator is

  $$\hat Y_\ell(\bm \lambda^*, \bm \phi^*) = 
  \sum_{i \in A_1} w_{1i} q_i \bf z_i \bm \phi^* +
  \sum_{i \in A_2} w_{1i} d_{2i|1} (y_i - q_i \bf z_i \bm \phi^*).$$

  By construction using a Taylor expansion yields,

  \begin{align*}
    \hat Y_{DCE}(\hat{\bm \lambda}) 
    &= \hat Y_\ell(\bm \lambda^*, \bm \phi^*) + 
    E\left[\frac{\partial}{\partial \bm \lambda}\hat Y_\ell(\bm \lambda^*,
    \bm \phi^*)\right](\hat{\bm \lambda} - \bm \lambda^*) + \frac{1}{2}
    E\left[\frac{\partial}{\partial \bm \lambda^2} \hat Y_{DCE}(\bm \lambda^*)\right] 
    (\hat{\bm \lambda} - \bm \lambda^*)^2\\
    &= \hat Y_\ell(\bm \lambda^*, \bm \phi^*) + O(N)O_p(n_2^{-1}).
  \end{align*}

  The final equality comes from the fact that 
  $E\left[\frac{\partial}{\partial \bm \lambda}\hat Y_\ell(\bm \lambda^*, \bm \phi^*)\right]
  = 0$, $\frac{\partial}{\partial \bm \lambda^2} \hat w_{2i|1}(\bm \lambda^*)$ is
  bounded and $||\hat{\bm \lambda} - \bm \lambda^*|| = O_p(n_2^{-1/2})$, which proves our
  result.
\end{proof}

\subsection{Simulation Study 1}

We run a simulation testing the proposed method. In this approach we have the
following simulation setup:

$$
\begin{aligned}
X_{1i} &\stackrel{ind}{\sim} N(2, 1) \\
X_{2i} &\stackrel{ind}{\sim} Unif(0, 4) \\
X_{3i} &\stackrel{ind}{\sim} N(0, 1) \\
\varepsilon_i &\stackrel{ind}{\sim} N(0, 1) \\
Y_{i} &= 3 X_{1i} + 2 X_{2i} + \varepsilon_i \\
%\pi_{1i} &= \min(\Phi_3(-x_{3i} - 2), 0.7) \\
\pi_{1i} &= n_1 / N \\
\pi_{2i|1} &= \min(\Phi_3(-x_{1i} + 1), 0.9).
\end{aligned}
$$

where $\Phi_3$ is the CDF of a t-distribution with 3 degrees of freedom.
%\textcolor{red}{The simulation setup is somewhat strange. The first order
%inclusion probability for the second-phase sampling, $\pi_{2i \mid 1}$, is a
%function of $x_{4i}$ which is completely independent of $(x_{1i}, x_{2i}, y_i)$.
%In this case, your second-phase sampling is essentially equivalent to the simple
%random sampling. }
This is a two-phase extension of the setup in \cite{kwon2024debiased}. We
consider a finite population of size $N = 10,000$ with the Phase 1 sampling
being a simple random sample (SRS) and the 
Phase 2 sampling occuring under Poisson sampling. This yields a
Phase 1 sample
size of $n_1 = 1000$ and a Phase 2 sample size of
$E[n_2] \approx 267$. In the Phase 1 sample, we observe 
$(X_1, X_2)$ while in the Phase 2 sample we observe $(X_1, X_2, Y)$. This
simulation does not deal with model misspecification, and we compare the
proposed method for the parameter $\bar Y_N$ with four approaches:

\begin{itemize}
  \item[1.] $\pi^*$-estimator: $\hat Y_{\pi^*} = N^{-1} \sum_{i \in A_2}
    \frac{y_i}{\pi_{1i} \pi_{2i|1}},$
  \item[2.] Two Phase Regression estimator (TP-Reg): 
    $\hat Y_{reg} = \sum_{i \in A_1} \frac{\bf x_i' \hat \beta}{\pi_{1i}} + 
    \sum_{i \in A_2} \frac{1}{\pi_{1i}\pi_{2i|1}}(y_i - \bf x_i' \hat \beta)$ 
    where $\hat \beta = 
    \left(\sum_{i \in A_2} \bf x_i \bf x_i'\right)^{-1} \sum_{i \in A_2} \bf x_i y_i$
    and $\bf x_i = (1, x_{1i}, x_{2i})^T$,
    % \textcolor{red}{Dr. Kim, should I modify the simulation so that the
    % regression estimator also includes $g(\pi_{2i|1}^{-1})$ as a covariate?}

    \textcolor{red}{You should include an intercept in $\bf x_i$. }
    \textcolor{blue}{I have been including an intercept, but I forgot to add it
    into these notes.}
  \item[3.] Debiased Calibration with Population Constraints (DC-Pop): This 
    solves 

  \begin{equation}
    \argmin_{w_{2|1}} \sum_{i \in A_2} w_{1i} G(w_{2i}) \text{ such that}
    \sum_{i \in A_2} w_{1i} w_{2i|1} \bf z_i = \sum_{i \in U} \bf z_i.
  \end{equation}

  \item[4.] Debiased Calibration with Estimated Population Constraints (DC-Est):
    This solves Equation~\eqref{eq:primal} with $q_i = 1$.
\end{itemize}

In addition to estimating the mean parameter $\bar Y_N$, we also construct
variance estimates $\hat V(\hat Y)$ for each estimate $\hat Y$. For each
approach we give the variance estimate in Table~\ref{tab:varforms}.

\begin{table}[ht!]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Estimator & Estimated Variance & Notes \\
    \midrule
    $\pi^*$ 
    & {\scriptsize$\left(\frac{1}{n_1} - \frac{1}{N}\right) \hat s^2_Y + 
      \sum\limits_{i \in A_2} \frac{w_{1i}^2}{N^2} 
        \frac{(1 - \pi_{2i|1})}{\pi_{2i|1}^2}y_i^2$}
    & {\scriptsize$\hat s^2_Y = \frac{1}{n_2 - 1}\sum\limits_{i \in A_2} (y_i -
    \bar y_i)^2$} \\
    TP-Reg  
    & {\scriptsize$\left(\frac{1}{n_1} - \frac{1}{N}\right) \hat s^2_\eta + 
      \sum\limits_{i \in A_2} \frac{w_{1i}}{N^2} 
        \frac{(1 - \pi_{2i|1})}{\pi_{2i|1}^2}\varepsilon_i^2$}
    & {\scriptsize$ \eta_i = \bf x_i'\bm \beta + \delta_{2i} w_{2i|1}\varepsilon_i,\;
        \varepsilon_i = (y_i - \bf x_i' \bm \beta)$} \\
    DC-Pop  
    & {\scriptsize$\left(\frac{1}{n_1} - \frac{1}{N}\right) \hat s^2_\varepsilon + 
      \sum\limits_{i \in A_2} \frac{w_{1i}}{N^2} 
        \frac{(1 - \pi_{2i|1})}{\pi_{2i|1}^2}\varepsilon_i^2$}
    & {\scriptsize$\eta_i = \bf z_i' \bm \phi + \delta_{2i}w_{2i|1}\varepsilon_i,\;
        \varepsilon_i = (y_i - \bf z_i'\bm \phi)$} \\
    DC-Est  
    & {\scriptsize$\left(\frac{1}{n_1} - \frac{1}{N}\right) \hat s^2_\eta + 
      \sum\limits_{i \in A_2} \frac{w_{1i}}{N^2} 
        \frac{(1 - \pi_{2i|1})}{\pi_{2i|1}^2}\varepsilon_i^2$}
    & {\scriptsize$\eta_i = \bf z_i' \bm \phi + \delta_{2i}w_{2i|1}\varepsilon_i,\;
        \varepsilon_i = (y_i - \bf z_i' \bm \phi)$} \\
    \bottomrule
  \end{tabular}
  \caption{This table gives the formulas for each variance estimator used in
  Simulation 1. For the derivation of the estimated variance of the DC-Pop
estimatator see Appendix A.}
  \label{tab:varforms}
\end{table}

\textcolor{red}{I am not sure about the variance estimation formula in Table 1. 
They are different from what we discussed before. I wonder whether you can make
the sampling rate $n/N$ negligible so that the second term becomes
insignificant. How about making $N=50,000$, $n_1=500$, and $n_2=200$?}

\textcolor{blue}{The result with $N = 50,000$, $n_1 = 500$ and $E[n_2] \approx
200$}

\begin{table}[ht!]
  \centering
  {\color{blue}\input{tables/tpdcsim50000_mean.tex}}
% This table was generated from ../../src/explore/proto-dctpmcmc.R
  \caption{\textcolor{blue}{This table shows the results of Simulation Study 1 with
      a different finite population and Phase 2 selection probability.
      It displays the Bias, RMSE, empirical 95\% confidence interval, and a
      t-statistic assessing the unbiasedness of each estimator for the
      estimators: $\pi^*$, TP-Reg, DC-Pop, and DC-Est.}}
\end{table}


We run the simulation $1000$ times for each of these methods and compute the
Bias ($E[\hat Y] - \bar Y_N$), the RMSE ($\sqrt{\Var(\hat Y - \bar Y_N)}$), a 95\%
empirical confidence interval ($\sum_{b = 1}^{1000} |\hat Y^{(b)} - \bar Y_N| \leq 
\Phi(0.975)\sqrt{\hat V(\hat Y^{(b)})^{(b)}}$), and a T-test that assesses the
unbiasedness of each estimator where $\hat Y^{(b)}$ is the result from the $b$th
simulation replicate. The results are in Table~\ref{tab:tpdc-mean}.

\begin{table}[ht!]
  \centering
\input{tables/tpdcsim_mean.tex}
% This table was generated from ../../src/explore/proto-dctpmcmc.R
\caption{This table shows the results of Simulation Study 1. It displays the
Bias, RMSE, empirical 95\% confidence interval, and a t-statistic assessing the
unbiasedness of each estimator for the estimators: $\pi^*$, TP-Reg, DC-Pop, and
DC-Est.}
\label{tab:tpdc-mean}
\end{table}

\textcolor{red}{Dr. Kim is this confidence interval good enough? It seems
slightly high for me.}

\textcolor{red}{I also give the mean variance estimates, the Monte Carlo mean,
as well as a test for bias.}
\begin{table}[ht!]
  \centering
{\color{red} \input{tables/tpdcsim_var.tex} }
% This table was generated from ../../src/explore/proto-dctpmcmc.R
\caption{\textcolor{red}{This table shows the variance estimates from Simulation
    Study 1. It displays the Monte Carlo variance of $\hat Y^{(b)}$, the average
    estimated variance, the variance of the variance estimator, and a
    t-statistic assessing the unbiasedness of each variance estimator.}}
\label{tab:tpdc-var}
\end{table}

\section{Topic 2: Non-nested Two-Phase Sampling}

\subsection{Background}

Now we consider the sampling mechanism known as non-nested two phase sampling 
(\cite{hidiroglou2001double}). In the last section, we considered two phase sampling
in which the Phase 2 sample was a subset of the Phase 1 sample. With non-nested
two phase sampling the Phase 2 sample is independent of the Phase 1 sample. It
is a separate independent sample of the same population. Like traditional two
phase sampling, we consider the Phase 1 sample, $A_1$, to consist of
observations of $(\bf X_i)_{i = 1}^{n_1}$ and the Phase 2 sample, $A_2$, to consist
of observations of $(\bf X_i, Y_i)_{i = 1}^{n_2}$. 

Whereas the classical two phase estimator uses a single Horvitz-Thompson
estimator of the Phase 1 sample to construct estimates for calibration totals,
in the non-nested two phase sample we have two independent Horvitz-Thompson
estimators of the total of $\bf X$,

$$\hat{\bf X_1} = \sum_{i \in A_1}^{n_1} d_{1i} \bf x_i \text{ and } 
\hat{\bf X_2} = \sum_{i \in A_2}^{n_2} d_{2i} \bf x_i $$

where $d_{1i} = \pi_{1i}^{-1}$, $d_{2i} = \pi_{2i}^{-1}$, $\pi_{1i}$ is the
probability of $i \in A_1$ and $\pi_{2i} = \Pr(i \in A_2)$. 
We can combine these estimates using the effective sample size 
(\cite{kish1965survey}) to get
$\hat{\bf X_c} = (n_{1, eff} \hat{\bf X_1} + n_{2, eff}\hat{\bf X_2}) / 
(n_{1, eff} + n_{2, eff})$ where $n_{1, eff}$ and $n_{2, eff}$ are the effective
sample size for $A_1$ and $A_2$ respectively. Then we can define a regression
estimator as

$$
\hat Y_{NN, reg} = \hat Y_2 + (\hat{\bf X_c} - \hat{\bf X_2})^T \hat{\bm \beta_q} = 
\hat Y_2 + (\hat{\bf X_1} - \hat{\bf X_2})^T W\hat{\bm \beta_q}
$$

where, $W = n_{1, eff} / (n_{1, eff} + n_{2, eff})$, and

$$\hat{\bm \beta_q} = \left(\sum_{i \in A_2} 
\frac{\bf x_i \bf x_i^T}{q_i}\right)^{-1} 
\sum_{i \in A_2} \frac{\bf x_i y_i}{q_i} \text{ and
}\hat Y_2 = \sum_{i \in A_2} d_{2i} y_i. $$

Since the samples $A_1$ and $A_2$ are independent, 

$$V(\hat Y_{NN, reg}) = V\left(\sum_{i \in A_2} \frac{1}{\pi_{2i}}(y_i -
\bf x_i^TW\bm \beta^*_q)\right) + (\bm \beta^*_q)^T W^T V(\hat{\bf X_1}) W \bm \beta_q^*$$

where $\bm \beta_q^*$ is the probability limit of $\hat{\bm \beta_q}$. Like the two phase
sample this regression estimator can be viewed as the solution to the following
calibration equation 

\begin{equation}\label{eq:nncal}
  \hat w_2 = \argmin_{w_2} Q(w_2) = \sum_{i \in A_2} (w_{2i} - d_{2i})^2 q_i 
  \text{ such that } \sum_{i \in A_2} w_{2i} \bf x_i = \hat{\bf X_c}
\end{equation}

and $\hat Y_{NN, reg} = \sum_{i \in A_2} \hat w_{2i} y_i$ where $\hat
w_{2i}$ is the solution to Equation~\ref{eq:nncal}.

We extend the debiased calibration estimator of \cite{kwon2024debiased} to the
non-nested two phase sampling case where we use a combined estimate 
$\hat{\bm X_c}$ as the calibration totals instead of using the true totals from
the finite population.

\subsection{Methodology}

The methodology for the non-nested two phase sample is very similar to the setup
described as part of Topic 1. Given a strictly convex differentiable function,
$G: \mathcal{V} \to \mathbb{R}$, the goal is to solve

\begin{equation}\label{eq:nnopt}
\hat w_2 = \argmin_w \sum_{i \in A_2} G\left(w_{2i}\right) q_i 
\text{ with } 
\sum_{i \in A_2} w_{2i} \bf x_i = \hat{\bf X_{c}} \text{ and } 
\sum_{i \in A_2} w_{2i} g(d_{2i}) q_i = \sum_{i \in U} g(d_{2i}) q_i
\end{equation}

for $g(x) = G'(x)$ and a known choice of $q_i \in \mathbb{R}$. 
The difference
between solving Equation~\ref{eq:nnopt} and Equation~\ref{eq:primal} is that the
estimator $\hat{\bf X_c}$ is estimated from the combined sample $A_c= A_1 \cup
A_2$. Before using $\hat{\bf X_c}$ in the debiased calibration estimator, we
need to estimate it from the non-nested samples. We can get multiple estimates
of $\hat{\bf X}$, 

$$\hat{\bf X_1} = \sum_{i \in A_1} d_{1i} \bf x_i \text{ and }
\hat{\bf X_2} = \sum_{i \in A_2} d_{2i} \bf x_i.$$

Let $V_1$ and $V_2$ be known variance matrices for $\hat{\bf X_1}$ and 
$\hat{\bf X_2}$, then the optimal combined estimate is 

$$\hat{\bf X_c} = \frac{V_1^{-1} \hat{\bf X_1} + V_2^{-1} 
\hat{\bf X_2}}{V_1^{-1} + V_2^{-1}}.$$

We can constuct a non-nested two phase estimator $\hat Y_{NNE}$ for $Y_N$ where
$\hat Y_{NNE} = \sum_{i \in A_2} \hat w_{2i} y_i$ and $\hat w_{2i}$ solves
Equation~\ref{eq:nnopt}. Like the classical two phase approach, to solve this
setup we minimize the Lagrangian,

\begin{equation}\label{eq:legragedc2}
  L(w_{2i}, \bm \lambda) = \sum_{i \in A_2} G(w_{2i}) q_i + 
  \bm \lambda \left( \hat{\bf T} - \sum_{i \in A_2} w_{2i} \bf z_i q_i\right).
\end{equation}

with 

$$\hat{\bf T} = 
\begin{bmatrix}
  \hat{\bf X_c} \\ \sum_{i \in U} g(d_{2i}) q_i
\end{bmatrix}.
$$


\textcolor{red}{I am a little worried about the second term of $\hat{T}$. It is
  somewhat weird to assume that $T_g= \sum_{i \in U} g(d_{2i}) q_i$ is known.
  One simple way is to use a HT estimator  of $T_g$ from sample $A_2$. That is,
  use $\hat{T}_{g, {\rm HT}} = \sum_{i \in A_2} d_{2i} g( d_{2i}) q_i$ or even
  use a GREG estimator of $T_g$. 
%I wonder we can assume that an unbiased estimator of $T_g$ is available from
  %$A_c=A_1 \cup A_2$. That is, let's assume that we have $\hat{T}_c = W
  %\hat{T}_1 + (1- W) \hat{T}_2$ as the estimator  for $T$. 
}

Differntiating with respect to $w_{2i}$ and setting this expression equal to
zero, yields the fact that $\hat w_{2i}$ satisfies 

$$ \hat w_{2i}(\hat{\bm \lambda}) = g^{-1}(\hat{\bm \lambda^T} \bf z_i) $$

where $\hat{\bm \lambda}$ is the solution to

\begin{equation}\label{eq:lamdc2}
  \left( \hat{\bf T} - \sum_{i \in A_2} w_{2i}(\hat{\bm \lambda}) \bf z_i
  q_i\right) = 0.
\end{equation}

\subsection{Theoretical Results}

\begin{theorem}[Design Consistency]\label{thm:dc2}
  Allowing $\bm \lambda^*$ to be the probability limit of $\hat{\bm \lambda}$,
  under some regularity conditions, $\hat Y_{NNE} = \hat Y_{\ell, NNE}(\bm \lambda^*,
  \bm \phi^*) + O_p(Nn_2^{-1})$ where

  $$\hat Y_{\ell, NNE}(\bm \lambda^*, \bm \phi^*) = \sum_{i \in A_2} \hat w_{2i}(
  \bm \lambda^*) + \left(\hat{\bf T} - \sum_{i \in A_2} \hat w_{2i}(
  \bm \lambda^*) \bf z_i q_i\right) \bm \phi^*$$

  and 
  $$\bm \phi^* =
  \left(\sum_{i \in U} \frac{\pi_{2i} q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    \bf x_i^2 / q_i & \bf x_i g(d_{2i}) / q_i \\
    \bf x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in U} \frac{\pi_{2i} y_i}{g'(d_{2i})} 
  \begin{bmatrix} \bf x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$
\end{theorem}

\begin{proof}
  The proof of this result is very similar to the proof the
  Theorem~\ref{thm:dc1}. The biggest difference is that the total for $\bf X$ is
  estimated from both samples using $\hat{\bf X_c}$ instead of $\hat{\bf
  X_{HT}}$ from the Phase 1 sample.

  Since $\hat Y_{NNE} = \sum_{i \in A_2} \hat w_{2i}(\hat{\bm \lambda})y_i$ where
  $\hat{\bm \lambda}$ solves

  \begin{equation}
    \sum_{i \in A_2} \hat w_{2i}(\bm \lambda) q_i
    \underbrace{
    \begin{bmatrix} 
      \bf x_i / q_i \\ g(d_i)
  \end{bmatrix}}_{\bf z_i} = \bf T
  \end{equation}

  we have 

  $$\hat Y_{\ell, NNE} (\hat{\bm \lambda}, \bm \phi) = \sum_{i \in A_2} \hat
  w_{2i}(\hat{\bm \lambda}) + \left(\bf T - \sum_{i \in A_2} \hat
  w_{2i}(\hat{\bm \lambda}) \bf z_i q_i\right) \bm \phi.$$

  If we choose $\bm \phi^*$ such that $E\left[\frac{\partial}{\partial \bm \lambda} 
    \hat Y_{\ell, NNE}(\bm \lambda^*, \bm \phi^*)\right] = 0$, then

  $$\bm \phi^* =
  \begin{bmatrix}
    \bm \phi^*_1 \\ \bm \phi^*_2
  \end{bmatrix} = 
  \left(\sum_{i \in U} \frac{\pi_{2i} q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    \bf x_i^2 / q_i & \bf x_i g(d_{2i}) / q_i \\
    \bf x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in U} \frac{\pi_{2i} y_i}{g'(d_{2i})} 
  \begin{bmatrix} \bf x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$

  Hence, by a Taylor expansion around $\hat{\bm \lambda}$,
  
  $$\hat Y_{NNE}(\hat{\bm \lambda}) = \hat Y_{\ell, NNE}(\bm \lambda^*, 
  \bm \phi^*) + O_p(Nn_2^{-1}).$$

\end{proof}

\begin{theorem}[Variance Estimation]
  The variance of $\hat Y_{NNE}$ is 

  \begin{align*}
    \Var(\hat Y_{NNE}(\hat \lambda)
    &= (\bm \phi_1^*)^T \Var(\hat{\bf X_c}) \bm \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} 
      (y_i - \bf z_i \bm \phi^* q_i)(y_j - \bf z_j \bm \phi^* q_j)\\
    &\qquad + (1 - W)\bm \phi_1^* \sum_{i \in U} \sum_{j \in U} \Delta_{2ij}
    d_{2i} \bf x_i d_{2j}(y_j - \bf z_j \bm \phi^*_1 q_j)\\
  \end{align*}

  We can estimate the variance using

  \begin{align*}
    \hat V_{NNE} 
    &= (\hat{\bm \phi_1})^T \Var(\hat{\bf X_c}) \hat{\bm \phi_1} + 
    \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}\pi_{2i}\pi_{2j}} 
    (y_i - \bf z_i \hat{\bm \phi} q_i)(y_j - \bf z_j \hat{\bm \phi} q_j) \\
    &\qquad + (1 - W)\hat{\bm \phi_1} \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}}
    \frac{\bf x_i}{\pi_{2i}} \frac{(y_j - \bf z_j \hat{\bm \phi_1} q_j)}{\pi_{2j}}
  \end{align*}

  where 
  
  $$\hat{\bm \phi} =
  \begin{bmatrix}
    \hat{\bm \phi_1} \\ \hat \phi_2
  \end{bmatrix} = 
  \left(\sum_{i \in A_2} \frac{q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    \bf x_i^2 / q_i & \bf x_i g(d_{2i}) / q_i \\
    \bf x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in A_2} \frac{y_i}{g'(d_{2i})} 
  \begin{bmatrix} \bf x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$
\end{theorem}

\begin{proof}
  From Theorem~\ref{thm:dc2}, we know that $\hat Y_{NNE}(\hat \lambda) = 
  \hat Y_{\ell, NNE}(\bm \lambda^*, \bm \phi^*) + O_p(Nn_2^{-1})$. Hence, the
  variance of $\hat Y_{NNE}(\hat{\bm \lambda})$ is 

  \begin{align*}
    \Var(\hat Y_{NNE}(\hat{\bm \lambda})) 
    &= \Var(\hat Y_{\ell, NNE}(\bm \lambda^*, \bm \phi^*) + O_p(Nn_2^{-1})) \\ 
    &= \Var\left(\sum_{i \in A_2} \hat w_{2i}(\bm \lambda^*) y_i + 
      \left(\bf T - \sum_{i \in A_2} \hat w_{2i}(\bm \lambda^*) \bf z_i
    q_i\right)\bm \phi^*\right)\\
    &= (\bm \phi_1^*)^T \Var(\hat{\bf X_c}) \bm \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} 
    (y_i - \bf z_i \bm \phi^* q_i)(y_j - \bf z_j \bm \phi^* q_j)\\
    &\qquad+ 2 \Cov\left(\hat{\bf X_c} \bm \phi_1^*, \sum_{i \in A_2} 
      \frac{(y_i - \bf z_i \bm \phi^* q_i)}{\pi_{2i}}\right) \\
    &= (\bm \phi_1^*)^T \Var(\hat{\bf X_c}) \bm \phi_1^* + 
    \sum_{i \in U} \sum_{j \in U} \frac{\Delta_{2ij}}{\pi_{2i}\pi_{2j}} 
    (y_i - \bf z_i \bm \phi^* q_i)(y_j - \bf z_j \bm \phi^* q_j)\\
    &\qquad + (1 - W)\bm \phi_1^* \sum_{i \in U} \sum_{j \in U} \Delta_{2ij} 
    \frac{x_i}{\pi_{2i}} \frac{(y_j - \bf z_j \bm \phi^*_1 q_j)}{\pi_{2j}}\\
  \end{align*}

  where the last equality comes from the fact that $\hat{\bf X_c} = W\hat{\bf
  X_1} + (1 - W) \hat{\bf X_2}$. To have an unbiased estimator of the variance
  we can use:

  \begin{align*}
    \hat V_{NNE} 
    &= (\hat{\bm \phi_1})^T \Var(\hat{\bf X_c}) \hat{\bm \phi_1} + 
    \sum_{i \in A_2} \sum_{j \in A_2} \frac{\Delta_{2ij}}{\pi_{2ij}\pi_{2i}\pi_{2j}} 
    (y_i - \bf z_i \hat{\bm \phi} q_i)(y_j - \bf z_j \hat{\bm \phi} q_j) \\
    &\qquad + (1 - W)\hat \phi_1 \sum_{i \in A_2} \sum_{j \in A_2} 
    \frac{\Delta_{2ij}}{\pi_{2ij}} \frac{x_i}{\pi_{2i}} 
    \frac{(y_j - \bf z_j \hat{\bm \phi_1} q_j)}{\pi_{2j}}
  \end{align*}

  where 
  
  $$\hat{\bm \phi} =
  \begin{bmatrix}
    \hat{\bm \phi_1} \\ \hat \phi_2
  \end{bmatrix} = 
  \left(\sum_{i \in A_2} \frac{q_i}{g'(d_{2i})} 
  \begin{bmatrix}
    \bf x_i^2 / q_i & \bf x_i g(d_{2i}) / q_i \\
    \bf x_i g(d_{2i}) / q_i & g(d_{2i})^2
  \end{bmatrix}
  \right)^{-1}
  \sum_{i \in A_2} \frac{y_i}{g'(d_{2i})} 
  \begin{bmatrix} \bf x_i / q_i \\ g(d_i) \end{bmatrix}.
  $$

\end{proof}

\subsection{Simulation Study 2}

We run a simulation testing the proposed method. This is very similar to
Simulation 1. We have the following simulation setup:

$$
\begin{aligned}
X_{1i} &\stackrel{ind}{\sim} N(2, 1) \\
X_{2i} &\stackrel{ind}{\sim} Unif(0, 4) \\
X_{3i} &\stackrel{ind}{\sim} N(0, 1) \\
\varepsilon_i &\stackrel{ind}{\sim} N(0, 1) \\
Y_{i} &= 3 X_{1i} + 2 X_{2i} + \varepsilon_i \\
\pi_{1i} &= n_1 / N \\
\pi_{2i} &= \min(\Phi_3(-x_{1i} + 1), 0.9).
\end{aligned}
$$

where $\Phi_3$ is the CDF of a t-distribution with 3 degrees of freedom.
%\textcolor{red}{The simulation setup is somewhat strange. The first order
%inclusion probability for the second-phase sampling, $\pi_{2i \mid 1}$, is a
%function of $x_{4i}$ which is completely independent of $(x_{1i}, x_{2i}, y_i)$.
%In this case, your second-phase sampling is essentially equivalent to the simple
%random sampling. }
We consider a finite population of size $N = 10,000$ with the Phase 1 
sampling being a simple random sample (SRS) of size $n_1 = 1000$. The Phase 2
sample is a Poisson sample with an expected sample size of 

and
Phase 2 sampling occuring under Poisson (Bernoulli) sampling. This yields a
Phase 1 sample
size of $E[n_1] \approx 1100$ and a Phase 2 sample size of
$E[n_2] \approx 300$. In the Phase 1 sample, we observe 
$(X_1, X_2)$ while in the Phase 2 sample we observe $(X_1, X_2, Y)$. This
simulation does not deal with model misspecification, and we compare the
proposed method for the parameter $\bar Y_N$ with four approaches:

\begin{itemize}
  \item[1.] HT-estimator: $\hat Y_{HT} = N^{-1} \sum_{i \in A_2}
    \frac{y_i}{\pi_{2i}},$
  \item[2.] Regression estimator (Reg): Let $\hat Y_{NN, reg} = \hat Y_{HT} + (
    \hat{\bf X_c} - \hat{\bf X_{2, HT}}) \hat{\bm \beta_2}$ where $\hat Y_{HT} =
    \sum_{i \in A_2} d_{2i} y_i$, $\hat{\bf X_c} = W \hat{\bf X_{1, HT}} + (1 -
    W)\hat{\bf X_{2, HT}}$, $W = n_{1,eff} / (n_{1,eff} + n_{2, eff})$,
    $\hat{\bf X_{1, HT}} = \sum_{i \in A_1} d_{1i} \bf x_{i}$, $\hat{\bf X_{2,
    HT}} = \sum_{i \in A_2} d_{2i} \bf x_{i}$, $\bf x_i = (1, x_{1i}, x_{2i})^T$
    and 

    $$ \hat{\bm \beta_2} = \left(\sum_{i \in A_2} \bf x_i \bf x_i^T\right)^{-1}
    \sum_{i \in A_2} \bf x_i y_i.$$

    Then $\hat{\bar Y}_{NN, reg} = \hat Y_{NN, reg} / N$.
  \item[3.] Debiased Calibration with Population Constraints (DC-Pop): This 
    solves 

  \begin{equation*}
  \hat w_2 = \argmin_w \sum_{i \in A_2} G(w_{2i}) q_i \text{ with } 
  \sum_{i \in A_2} w_{2i} \bf x_i = \sum_{i \in U} \bf x_i \text{ and } 
  \sum_{i \in A_2} w_{2i} g(d_{2i}) q_i = \sum_{i \in U} g(d_{2i}) q_i
  \end{equation*}

  \item[4.] Debiased Calibration with Estimated Population Constraints (DC-Est):
    This solves Equation~\eqref{eq:nnopt} with $q_i = 1$.
\end{itemize}

In addition to estimating the mean parameter $\bar Y_N$, we also construct
variance estimates $\hat V(\hat Y)$ for each estimate $\hat Y$.
% For each approach we give the variance estimate in Table~\ref{tab:varforms2}.

We run the simulation $1000$ times for each of these methods and compute the
Bias ($E[\hat Y] - \bar Y_N$), the RMSE ($\sqrt{\Var(\hat Y - \bar Y_N)}$), a 95\%
empirical confidence interval ($\sum_{b = 1}^{1000} |\hat Y^{(b)} - \bar Y_N| \leq 
\Phi(0.975)\sqrt{\hat V(\hat Y^{(b)})^{(b)}}$), and a T-test that assesses the
unbiasedness of each estimator where $\hat Y^{(b)}$ is the result from the $b$th
simulation replicate. The results are in Table~\ref{tab:nndc-mean}.

\begin{table}[ht!]
  \centering
\input{tables/nndcsim_mean.tex}
% This table was generated from ../../src/explore/proto-dctpmcmc.R
\caption{This table shows the results of Simulation Study 2. It displays the
Bias, RMSE, empirical 95\% confidence interval, and a t-statistic assessing the
unbiasedness of each estimator for the estimators: HT, Reg, DC-Pop, and
DC-Est.}
\label{tab:nndc-mean}
\end{table}


\section{Topic 3: Multi-source Two-Phase Sampling}

\newpage 

\bibliographystyle{chicago}
\bibliography{references}

\newpage

\appendix

\section{Derivation of the Estimated Variance of DC-Pop in Simulation 1}

We know that $\hat Y_{DC-Pop} = \sum_{i \in A_2} w_{1i} \hat w_{2i|1}(\hat
\lambda) y_i$ where $\hat w_{2i|1}$ solves

  \begin{equation}
    \argmin_{w_{2|1}} \sum_{i \in A_2} \frac{1}{\pi_{1i}} G(w_{2i})
    \text{ such that}
    \sum_{i \in A_2} w_{1i} w_{2i|1} z_i = \sum_{i \in U} z_i.
  \end{equation}

This means that using the technique of \cite{randles1982asymptotic},
the linearization of $\hat Y_{DC-Pop}$ is 

$$\hat Y_{DC-Pop, \ell} = \sum_{i \in A_2} w_{1i} \hat w_{2i|1}(\hat \lambda) y_i + 
\left( \sum_{i \in U} z_i - 
\sum_{i \in A_2} w_{1i} w_{2i|1}(\hat \lambda) z_i \right) \phi.$$

Since $\phi^*$ satisfies $E\left[\frac{\partial}{\partial \lambda} \hat Y_{DC-Pop,
\ell}(\lambda^*, \phi^*)\right] = 0$, $\phi^*$ is the same as it is for $\hat Y_{DC-Est,
\ell}$ with $q_i = 1$,

  \begin{align*}
    \phi^*
    &= E\left[\sum_{i \in A_2} \frac{w_{1i}z_i z_i^T }{g'(d_{2i|1})}\right]^{-1}
    E\left[\sum_{i \in A_2} \frac{w_{1i}z_i y_i}{g'(d_{2i|1})}\right]\\
    &= \left[\sum_{i \in U} \frac{\pi_{2i|1}z_i z_i^T}{g'(d_{2i|1})}\right]^{-1}
    \left[\sum_{i \in U} \frac{\pi_{2i|1} z_i y_i}{g'(d_{2i|1})}\right].\\
  \end{align*}

Hence,

$$
\hat Y_{DC-Pop, \ell} = \sum_{i \in U} z_i \phi^* + \sum_{i \in A_2} w_{1i} \hat
w_{2i|1}(\lambda^*) (y_i - z_i \phi^*).
$$

Unlike the two-phase regression estimator and $\hat Y_{DC-Est}$ this
linearization only has uncertainty in the second term. The first term is a
population estimate. Let $\varepsilon_i = (y_i - z_i \phi^*)$. This means that

\begin{align*}
  V(\hat Y_{DC-Pop, \ell}) 
  &= V(E[\hat Y_{DC-Pop, \ell} \mid A_1]) + E[V(\hat Y_{DC-Pop, \ell} \mid A_1)] \\
  &= V\left(\sum_{i \in A_1} w_{1i} \pi_{2i|1} \hat w_{2i|1}(\lambda^*)
  \varepsilon_i\right) + 
  E\left[\sum_{i \in A_1} \pi_{2i|1}(1 - \pi_{2i|1}) w_{1i}^2 \hat w_{2i|1}(\lambda^*)^2
    \varepsilon_i^2 \right]\\
  &= \sum_{i \in U} \pi_{1i}(1 - \pi_{1i}) w_{1i}^2 \pi_{2i|1}^2 
  \hat w_{2i|1}(\lambda^*)^2 \varepsilon_i^2 +
  \sum_{i \in U} \pi_{1i}\pi_{2i|1}(1 - \pi_{2i|1}) w_{1i}^2 \hat w_{2i|1}(\lambda^*)^2
    \varepsilon_i^2\\
  &= \sum_{i \in U} w_{1i} w_{2i|1}(\lambda^*)^2 \pi_{2i|1}
  ((1 - \pi_{1i})\pi_{2i|1} + (1 - \pi_{2i|1})) \varepsilon_i^2.
\end{align*}

Since $w_{2i|1}(\lambda^*) \pi_{2i|1} = 1$, there are two potential estimators
of the variance,

\begin{align*}
  \hat V_1 &= \sum_{i \in A_2} w_{1i}^2 w_{2i|1}(\hat \lambda)^2 
  ((1 - \pi_{1i})\pi_{2i|1} + (1 - \pi_{2i|1})) \varepsilon_i^2\\
  \hat V_2 &= \sum_{i \in A_2}  w_{1i}^2 d_{2i|1}^2
  ((1 - \pi_{1i})\pi_{2i|1} + (1 - \pi_{2i|1})) \varepsilon_i^2.
\end{align*}

In the current simulation, we use $\hat V_1$, but I have tried $\hat V_2$ and
the results are similar.
\end{document}
