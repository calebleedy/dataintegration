\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, mathrsfs, fancyhdr}
\usepackage{syntonly, lastpage, hyperref, enumitem, graphicx}
%\usepackage[style=authoryear]{biblatex}
\usepackage{booktabs}
\usepackage{float}

\usepackage{amsmath, amsthm, mathtools,commath}
\usepackage{graphics, color}
\usepackage{latexsym}
\usepackage{amssymb, amsfonts, bm}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{makeidx}
\usepackage{fullpage}
\usepackage{booktabs, arydshln}
\usepackage{comment} 
%\makeindex

\hbadness=10000 \tolerance=10000 \hyphenation{en-vi-ron-ment
in-ven-tory e-num-er-ate char-ac-ter-is-tic}

\usepackage[round]{natbib}
%\bibliographystyle{apalike2}
% \bibliographystyle{jmr}


\newcommand{\biblist}{\begin{list}{}
{\listparindent 0.0cm \leftmargin 0.50cm \itemindent -0.50 cm
\labelwidth 0 cm \labelsep 0.50 cm
\usecounter{list}}\clubpenalty4000\widowpenalty4000}
\newcommand{\ebiblist}{\end{list}}

\newcounter{list}

%\usepackage{setspace}

%\usepackage{hangpar}
\newcommand{\lbl}[1]{\label{#1}{\ensuremath{^{\fbox{\tiny\upshape#1}}}}}
% remove % from next line for final copy
\renewcommand{\lbl}[1]{\label{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{result}{Result}

\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\MAP}{{\text{MAP}}}
\newcommand{\Cov}{{\text{Cov}}}
\newcommand{\Var}{{\text{Var}}}
\newcommand{\logistic}{{\text{logistic}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\bf}[1]{\mathbf{#1}}


\begin{document}

\title{Debiased two-phase regression estimation}
\author{Caleb Leedy \and Jae Kwang Kim}
\maketitle 

\baselineskip .3in


\section{Introduction}

\begin{itemize}
\item Introduce two-phase sampling
\item Introduce classical two-phase regression estimation under parametric regression model 
$$ \hat{Y}_{\rm reg} = \sum_{i \in A_1} w_{1i} m(\bx_i ; \hat{\beta}) + \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1} \left( y_i - m(\bx_i; \hat{\beta}) \right) $$
 Properties: If $\mbox{dim} (\beta)=p$ is fixed in the asymptotic sense, then $\hat{Y}_{\rm reg}$ is asymptotically equivalent to 
$$ \hat{Y}_{\rm reg, \ell} = \sum_{i \in A_1} w_{1i} m(\bx_i ; {\beta}^*) + \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1} \left( y_i - m(\bx_i; {\beta}^*) \right). $$
Note that $\hat{Y}_{\rm reg, \ell}$ is design unbiased and achieves the optimality if $E(Y \mid \bx_i)= m(\bx_i; \beta_0)$ for some $\beta_0$ and $\hat{\beta}$ is consistent for $\beta_0$. 

\item Two-phase regression estimation under non-parametric regression 
$$ \hat{Y}_{\rm np, reg} = \sum_{i \in A_1} w_{1i} \hat{m}(\bx_i ) + \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1} \left( y_i - \hat{m}(\bx_i) \right) $$
where $\hat{m}(\bx)$ is a consistent estimator of $m(\bx)= E(Y \mid \bx)$, but the convergence rate is slow. 
\item Generally speaking, $\hat{Y}_{\rm rep, np}$ is not asymptotically equivalent to the oracle regression estimator given by 
\begin{equation}
\hat{Y}_{\rm oracle} = \sum_{i \in A_1} w_{1i} {m}(\bx_i ) + \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1} \left( y_i - {m}(\bx_i) \right). 
\label{oracle}
\end{equation}
Note that 
\begin{eqnarray*}
 \hat{Y}_{\rm np, reg} - \hat{Y}_{\rm  oracle} &=&\sum_{i \in A_1} w_{1i} \left\{ \hat{m} (\bx_i) - m(\bx_i) \right\} -\sum_{i \in A_2} w_{1i} \pi_{2i|1}^{-1} \left\{ \hat{m} (\bx_i) - m(\bx_i) \right\} \\
 &=& \sum_{i \in A_1} w_{1i} \left\{ \hat{m} (\bx_i) - m(\bx_i) \right\} \left( 1- \delta_i \pi_{2i|1}^{-1} \right) \\
 &:=& D_n .\end{eqnarray*} 
\end{itemize}
In general, the estimation error term $D_n$ is not asymptotically negligible. 
\section{Proposed estimator}


One possible idea is to develop a debiased estimation under two-phase sampling. If the covariates are high dimensional or the regression employs nonparametric regression (such as spline or random forest, etc), then the resulting two-phase regression estimator can be biased. To correct for the bias, we can use the following approach.

\begin{enumerate}
\item Split the sample $A_2$ into two parts: $A_2= A_2^{(a)} \cup A_2^{(b)}$.
We can use SRS to split the sample, but other sampling designs can be used. 
\item Use the observations in $A_2^{(a)}$ only to obtain a predictor of $y_i$, $\hat{m}^{(a)} (\bx_i)$. Also, use the observations in $A_2^{(b)}$ only to obtain a predictor of $y_i$, $\hat{m}^{(b)} (\bx_i)$. 
\item Let 
$$ \hat{m}(\bx_i) = \left( \hat{m}^{(a)} (\bx_i) + \hat{m}^{(b)} (\bx_i) \right)/ 2$$
be the predictor combining two samples. 
\item The final debiased two-phase regression estimator is given by 
\begin{equation}
\hat{Y}_{\rm d, reg} = \sum_{i \in A_1} w_{1i}  \hat{m}(\bx_i) +  \sum_{i \in A_2^{(a)} } w_{1i} \pi_{2i \mid 1}^{-1}   \left\{y_i - \hat{m}^{(b)} (\bx_i) \right\} +\sum_{i \in A_2^{(b)} } w_{1i} \pi_{2i \mid 1}^{-1}   \left\{y_i - \hat{m}^{(a)} (\bx_i) \right\}
\label{debiased}
\end{equation}
\end{enumerate}


Note that $\hat{Y}_{\rm d, reg}$ can be expresssed as 
$$ \hat{Y}_{\rm d, reg}= \left( \hat{Y}_{\rm d, reg}^{(a)} + \hat{Y}_{\rm d, reg}^{(b)} \right)/ 2 $$
where 
$$\hat{Y}_{\rm d, reg}^{(a)} =\sum_{i \in A_1} w_{1i} \hat{m}^{(b)}(\bx_i ) + \sum_{i \in A_2^{(a)}} w_{1i} 2\pi_{2i \mid 1}^{-1} \left( y_i - \hat{m}^{(b)}(\bx_i) \right)$$
and $$\hat{Y}_{\rm d, reg}^{(b)} =\sum_{i \in A_1} w_{1i} \hat{m}^{(a)}(\bx_i ) + \sum_{i \in A_2^{(b)}} w_{1i} 2\pi_{2i \mid 1}^{-1} \left( y_i - \hat{m}^{(a)}(\bx_i) \right).$$

Now, we can establish the following result. 

\begin{theorem}
Under some regularity conditions (to be explained later), we obtain 
\begin{equation}
N^{-1} \left( \hat{Y}_{\rm d, reg} - \hat{Y}_{\rm oracle} \right) =o_p( n^{-1/2} ),
\label{result}
\end{equation}
where $\hat{Y}_{\rm oracle}$ is defined in (\ref{oracle}). 
\end{theorem}
\begin{proof}
Define 
$$ \hat{Y}_{\rm oracle}^{(a)} = \sum_{i \in A_1} w_{1i} m(\bx_i) + \sum_{i \in A_2^{(a)} }w_{1i} 2 \pi_{2i \mid 1}^{-1} \left( y_i - m(\bx_i) \right) $$
and
$$ \hat{Y}_{\rm oracle}^{(b)} = \sum_{i \in A_1} w_{1i} m(\bx_i) + \sum_{i \in A_2^{(b)} }w_{1i} 2 \pi_{2i \mid 1}^{-1} \left( y_i - m(\bx_i) \right) .$$
Note that 
$$ \hat{Y}_{\rm oracle} = \left( \hat{Y}_{\rm oracle}^{(a)} + \hat{Y}_{\rm oracle}^{(b)} \right)/ 2$$
where $\hat{Y}_{\rm oracle}$ is defined in (\ref{oracle}). 

Note that  
\begin{eqnarray*}
 \hat{Y}_{\rm d, reg}^{(a)} - \hat{Y}_{\rm  oracle}^{(a)} &=&\sum_{i \in A_1} w_{1i} \left\{ \hat{m}^{(b)} (\bx_i) - m(\bx_i) \right\} -\sum_{i \in A_2^{(a)}} w_{1i} 2\pi_{2i|1}^{-1} \left\{ \hat{m}^{(b)}  (\bx_i) - m(\bx_i) \right\} \\
 &=& \sum_{i \in A_1} w_{1i} \left\{ \hat{m}^{(b)}  (\bx_i) - m(\bx_i) \right\} \left( 1- 2 \delta_i I_i^{(a)} \pi_{2i|1}^{-1} \right) \\
 &:=& D_n^{(a)} .\end{eqnarray*} 
It can be shown that, under some regularity conditions, we obtain 
\begin{equation}
 N^{-1} D_n^{(a)} = o_p(n^{-1/2} ).
 \label{res1}
 \end{equation}
%( We may present the above result as a lemma.) 

Also, writing 
$$ D_n^{(b)} = \hat{Y}_{\rm d, reg}^{(b)} - \hat{Y}_{\rm  oracle}^{(b)},  $$
we can establish 
\begin{equation}
 N^{-1} D_n^{(b)} = o_p(n^{-1/2} ).
 \label{res2}
 \end{equation}
Combining (\ref{res1}) and (\ref{res2}), we can establish that 
\begin{equation}
N^{-1} \left( \hat{Y}_{\rm d, reg} - \hat{Y}_{\rm oracle} \right) =o_p( n^{-1/2} ).
\label{result}
\end{equation}
Therefore, asymptotic unbiasedness of the $\hat{Y}_{\rm d, reg}$ in (\ref{debiased}) is established. 
 \end{proof}

Theorem 1 means that the estimation error of $\hat{m}(\bx)$ can be safely ignored in the asymptotic sense. There are several advantages of the debiased two-phase regression estimator in (\ref{debiased}). 
\begin{enumerate}
\item Unlike the classical two-phase regression estimator using nonparametric regression, we can establish asymptotic unbiasedness and  $\sqrt{n}$-consistency. 
\item Even if we use the sample split, there is no efficiency loss. That is, the asymptotic variance is equal to 
$$ V \left( \hat{Y}_{\rm d, reg} \right) = V \left( \hat{Y}_1 \right) + E\left[ V \left\{ \sum_{i \in A_2} w_{1i} \pi_{2i \mid 1}^{-1} \left( y_i - m(\bx_i) \right) \mid A_1 \right\} \right] 
$$
where $m(\bx_i)$ is the probability limit of $\hat{m}(\bx_i)$. 
\item Variance estimation is also straightforward. We can compute 
$$ \hat{\eta}_i = \hat{m}(\bx_i) +\delta_i  \pi_{2i \mid 1}^{-1} I_i^{(a)} \left\{ y_i - \hat{m}^{(b)}(\bx_i) \right\} +\delta_i \pi_{2i \mid 1}^{-1} I_i^{(b)} \left\{ y_i - \hat{m}^{(a)} (\bx_i) \right\}$$
and apply to the usual variance estimation formula for the first-phase sample, 
where $I_i^{(a)}$ is the indicator function for $A_2^{(a)}$ such that $I_i^{(a)} = 1$ if $i \in A_2^{(a)}$ and $I_i^{(a)}=0$ otherwise. Also, $I_i^{(b)}= 1- I_i^{(a)}$. 
\end{enumerate}


My variance estimator is 
$$ \hat{V} = \frac{1}{n_1 (n_1-1) } \sum_{i \in A_1} \left( \hat{\eta}_i - \bar{\eta}_n \right)^2 $$

I also have an idea on how to implement the above debiased regression estimator using calibration. I will give more details once we are confident in the proposed idea. 
\end{document}
